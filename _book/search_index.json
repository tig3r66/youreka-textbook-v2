[["index.html", "A Gentle Introduction to Data Science with R Explore, analyze, and visualize data Welcome to Youreka Canada How to use this textbook Contributors Credit License", " A Gentle Introduction to Data Science with R Explore, analyze, and visualize data Eddie Guo Youreka Canada Programs Team Welcome to Youreka Canada Welcome to the Youreka Canada program! In this course, you will learn how to wrangle data, perform statistical tests, and visualize data with R. The purpose of this textbook is to provide a companion to the Youreka Canada program, which teaches introductory statistics, data science, and research methods. Here, we offer an intuitive approach to data science rather than a rigorous, proof-based course. As such, this textbook does not assume you have any prior knowledge other than basic arithmetic, and it should be accessible to both high school and undergraduate students. How to use this textbook Please note that this text goes into additional detail not covered in session. All optional material is marked as OPTIONAL in the headings. Again, note that this text complements the Youreka program. These notes are not a substitute for attending the Youreka sessions! This text focuses on how the material taught in-session can be applied using the R programming language. As with all things programming, the best way to learn is to actively code. That is, when you read this textbook, open RStudio and play around with the presented code—try to find alternative solutions, or even break the code. Don’t be afraid to make mistakes, and soon enough, you will be confident to code on your own! Contributors This textbook was written by Eddie Guo, Pouria Torabi, Shuce Zhang, and Devin Aggarwal, who are part of the Youreka Canada Programs Team. A special thanks goes to Matthew Pietrosanu for his critical statistical review of the Youreka program. Credit This material was adapted from: Jennifer Bryan STAT 545 at UBC https://stat545.com/ Jennifer Bryan (2017). gapminder: Data from Gapminder. R package version 0.3.0. https://CRAN.R-project.org/package=gapminder License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike—If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["install.html", "1 Install R and RStudio 1.1 Installing R 1.2 Installing RStudio 1.3 Add-on packages", " 1 Install R and RStudio Why do we need to install both R and RStudio? Well, the answer is the following: R is the programming language whereas RStudio is the integrated development environment (IDE) for R. You can think of R as the thing that turns your code into commands that your computer runs and RStudio as a fancy text editor (although it is so much more than that!). 1.1 Installing R Go to the R Project website. On the left side bar, click on “CRAN” under “Download.” Choose the mirror you wish to download from (e.g., https://mirror.rcg.sfu.ca/mirror/CRAN/) Download the correct version for your OS. Ensure to download the latest release of R. 1.2 Installing RStudio Go to the RStudio website. Navigate to the RStudio page and download RStudio Desktop. If you have a pre-existing installation of R and/or RStudio, we highly recommend that you update both. If you upgrade R, you’ll need to update any packages you have installed. Type the following command into the Console in RStudio: update.packages(ask = FALSE, checkBuilt = TRUE) Once you’ve installed an updated version of R and RStudio, open RStudio. You should get a window similar to this screenshot, but yours will be more boring because you haven’t written any code or made any figures yet! Place your cursor in the pane called “Console,” which is where you interact with R. Type print('Hello World!') in the console and hit the enter or return key. You should see “Hello World!” print to the screen. If you do, you’ve succeeded in installing R and RStudio. 1.3 Add-on packages R contains a huge number of packages that enhances its functionality. People often share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN (e.g., the tidyverse), type this into the R console: install.packages(&#39;tidyverse&#39;, dependencies = TRUE) By including dependencies = TRUE, we are including any additional packages our target package requires. Please also install gapminder, ggsignif, and ggpubr. Without further ado, let’s jump into our adventure with R! "],["intro-to-r.html", "2 Intro to R 2.1 Objects 2.2 Functions 2.3 Math operators 2.4 Conditionals 2.5 Working directory", " 2 Intro to R We will begin our adventure by opening RStudio. If this is your first time opening RStudio, you should see the following panes: Console (entire left) Environment/History (upper right) Files/Plots/Packages/Help (lower right) You can change the default location of the panes, among many other things: Customizing RStudio. For now, place your cursor in the console so we can start coding with R! 2.1 Objects R is an object-oriented programming language. This means R creates different types of objects that we can manipulate with functions and operators. To create an object in R, we can assign a value to an object using an assignment operator using either a left arrow &lt;- or an equal sign =. Click the “Run Code” button to get started and play around with the code! In plain English, the above snippet tells us that “five times ten is assigned to my_object.” By convention, we use &lt;- to assign variables. Don’t be lazy and use = to assign variables. Although it will work, it will just sow confusion later. Code is miserable to read on a good day. Give your eyes a break and use &lt;-. Although object names are flexible, we need to follow some rules: Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. As a general rule of thumb, object names should be short and meaningful. Misleading or overly long object names will make it a pain to debug your code. Below are examples of various object name conventions. My best advice would be to pick one and stick with it. this_is_snake_case other.people.use.periods evenOthersUseCamelCase Let’s make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect the object we’ve just created, try out RStudio’s auto-complete feature: type the first few characters, press TAB, add characters until you get what you want, then press return/enter. 2.2 Functions We will use functions in most of our work with R, either pre-written or ones we write ourselves. Functions help us easily repeat instructions and carry out multiple tasks in a single step, saving us a lot of space in our code. You can call functions like this: functionName(arg1 = val1, arg2 = val2, ...) Notice that we use = instead of &lt;- within a function. Here, arg1 and arg2 are the arguments of the function. Likewise, val1 and val2 are the parameters of arg1 and arg2. Let’s try using seq() which makes regular sequences of numbers: seq(1, 10) ## [1] 1 2 3 4 5 6 7 8 9 10 The above snippet also demonstrates something about how R resolves function arguments. You can always specify name = value if you’re unsure. If you don’t, R attempts to resolve by position. In the above snippet, R assumed we wanted a sequence from = 1 that goes to = 10. As an exercise, try creating a sequence of numbers from 1 to 10 by increments of 2: If you just make an assignment, you don’t see the assigned value. To show the assigned value, just call the variable. one_to_ten &lt;- seq(1, 10) one_to_ten ## [1] 1 2 3 4 5 6 7 8 9 10 You can shorten this common action by surrounding the assignment with parentheses. (one_to_ten &lt;- seq(1, 10)) ## [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() ## [1] &quot;Sun May 9 15:18:38 2021&quot; If you’ve been following along in RStudio, look at your workspace (in the upper right pane.) The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() ls() If you want to remove the object named one_to_ten, you can do this: rm(one_to_ten) To remove everything: rm(list = ls()) or click the broom icon in RStudio’s Environment pane. 2.3 Math operators Here are some basic math operations you can perform in R. Try playing around with them in the interactive window. 2.4 Conditionals Conditional statements check if a condition is true or false using logical operators (operators that return either TRUE or FALSE). For example: 20 == 10*2 ## [1] TRUE &quot;hello&quot; == &quot;goodbye&quot; ## [1] FALSE These statements return a value is of type \"logical\", which is either TRUE if the condition is satisfied, or FALSE if the condition is not satisfied. One important note is that TRUE and FALSE are objects on their own, rather than the strings “true” and “false.” Conditional statements are made with a range of logical operators. Here are some examples: Operator Plain English == is equal to != is not equal to &lt; or &gt; is less than OR is greater than &lt;= or &gt;= is less than or equal to OR is greater than or equal to is.na() is an NA value There are other logical operators, including %in%, which checks if a value is present in a vector of possible values. Try playing around with the following statements and checking their output by running the code. If you’re keen, you’ll notice in the last line that we use c() to group objects together. This data structure is called a vector. As a brief introduction, vectors combine objects of the same type. Don’t worry too much about the specifics of vectors, as we will cover it in much greater depth in the next chapter. We can also combine conditions using the logical and (&amp;) along with the logical or (|). The logical &amp; returns TRUE if and only if both conditions are true, and it returns FALSE otherwise. Let’s look at the following examples: # is (5 greater than 2) AND (6 greater than 10)? (5 &gt; 2) &amp; (6 &gt;= 10) ## [1] FALSE # is (5 greater than 2) OR (6 greater than 10)? (5 &gt; 2) | (6 &gt;= 10) ## [1] TRUE 2.4.1 If statements Conditional statements generate logical values to filter inputs. if statements use conditional statements to control flow of a program. Below is the general form of an if statement: if (the conditional statement is TRUE) { do something } Let’s look at an example: Try assigning 6 to x and predict the output. Although an if statement alone is handy, we often want to check multiple conditions. We can add more conditions and associated actions with else if statements. Suppose we want to send an automated message to our friends. Here’s how we can do it: friend &lt;- &quot;Jasmine&quot; if (friend == &quot;Jason&quot;) { msg &lt;- &quot;Hi, Jason!&quot; } else if (friend == &quot;Jasmine&quot;) { msg &lt;- &quot;How are you, Jasmine?&quot; } msg ## [1] &quot;How are you, Jasmine?&quot; We can specify what to do if none of the conditions are TRUE by using else on its own. Try modifying the code below to print “Stranger danger!” if our friend’s name isn’t “Jason” or “Jasmine.” 2.5 Working directory Any process running on your computer has a notion of its “working directory.” By default in R, a working directory is where R will look for files you ask it to load. It is also where any files you write to disk will go. You can explicitly get your working directory with the getwd() function: getwd() The working directory is also displayed at the top of the RStudio console. You can set your working directory at the command line like so: setwd(&quot;path-to-my-directory/&quot;) The setwd() function is extremely useful for times you want to read in external data, such as a .csv file. 2.5.1 Other important things Below is a collection of important miscellaneous items to consider. R scripts are usually saved with a .R or .r suffix. Comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). Clean out the workspace. You can do so by clicking the broom icon or by typing rm(list = ls()) into the console. This workflow will serve you well in the future: Create an RStudio project for an analytical project. Keep inputs there (we’ll soon talk about importing). Keep scripts there; edit them, run them in bits or as a whole from there. Keep outputs there. Avoid using your mouse for your workflow. Firstly, using the keyboard is faster. Secondly, writing code instead of clicking helps with reproducibility. That is, it will be much easier to retrospectively determine how a numerical table or PDF was actually produced. Many experienced users never save the workspace, never save .RData files (I’m one of them), and never save or consult the history. Once/if you get to that point, there are options available in RStudio to disable the loading of .RData and permanently suppress the prompt on exit to save the workspace (go to Tools &gt; Options &gt; General). "],["data-structures.html", "3 Data Structures 3.1 Vectors 3.2 Lists 3.3 Data frames", " 3 Data Structures In this chapter, we will learn about data structures that will greatly aid our data science workflow. 3.1 Vectors Vectors are a sequence of values with the same type. We can create vectors using c(), which stands for “combine.” (my_nums &lt;- c(2.8, 3.2, 1.5, 3.8)) ## [1] 2.8 3.2 1.5 3.8 To access the elements inside a vector, we can do something called “slicing.” To access a single item or multiple items, use the square bracket operator []. In general [] in R means, “give me a piece of something.” For example: my_nums[4] ## [1] 3.8 my_nums[1:3] ## [1] 2.8 3.2 1.5 my_nums[c(1, 2, 3)] == my_nums[1:3] ## [1] TRUE TRUE TRUE In my_nums[1:3], the 1:3 creates a vector from 1 to 3, which is then used to subset multiple items in a vector. Here are some additional useful functions: length(my_nums) mean(my_nums) max(my_nums) min(my_nums) sum(my_nums) Given the data in the interactive block, consider the following exercises: Select “Pouria” and “Ana” from the names vector. Select all individuals who have ages greater than 20. Assume the order of names and ages correlates by index. Select all individuals whose age is not 21. Find the average age of all individuals. 3.1.1 Missing values So far we’ve worked with data with no missing values. In real life, however, we often have missing values (NA values). Unfortunately for us, R does not get along with NA values. density_ha &lt;- c(2.8, 3.2, 1.5, NA) mean(density_ha) ## [1] NA Why did we get NA? Well, it’s hard to say what a calculation including NA should be, so most calculations return NA when NA is in the data. One way to resolve this issue is to tell our function to remove the NA before executing: mean(density_ha, na.rm = TRUE) ## [1] 2.5 3.2 Lists Lists are a vector-like structure that can store other objects/data structures. You can think of it like a vector that holds other vectors. sites &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) notes &lt;- &quot;It was a good day in the field today. Warm, sunny, lots of gators.&quot; helpers &lt;- 4 field_notes &lt;- list(sites, notes, helpers) You can index lists in the following ways: field_notes[1] ## [[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; field_notes[[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; We can also give the values names and access them using the $ symbol–which is the preferred method–or via [\"variable_name\"] with subsetting. Try getting the my_sets vector from field_notes. 3.3 Data frames This is where things get really exciting! We will use these data structures extensively in the upcoming labs, so it’s important to pay attention here. A data frame is a list of equal length vectors grouped together. More importantly, a data frame can contain both categorical and numerical values, whereas a vector can only contain variables of the same type (i.e., all numerical, all categorical, etc.). sites &lt;- c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;) area_ha &lt;- c(1, 2, 3, 4) density_ha &lt;- c(2.8, 3.2, 1.5, NA) # creating the data frame surveys &lt;- data.frame(sites, density_ha, area_ha) surveys ## sites density_ha area_ha ## 1 a 2.8 1 ## 2 a 3.2 2 ## 3 b 1.5 3 ## 4 c NA 4 Here are some useful commands to investigate a data frame: str() returns the structure of a data frame. length() returns the length of a data frame. ncol() returns the number of columns of a data frame (same as length()) nrow() returns the number of rows of a data frame. str(surveys) ## &#39;data.frame&#39;: 4 obs. of 3 variables: ## $ sites : chr &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; ## $ density_ha: num 2.8 3.2 1.5 NA ## $ area_ha : num 1 2 3 4 ncol(surveys) ## [1] 3 nrow(surveys) ## [1] 4 Subsetting data frames is extremely similar to that for vectors. This time, however, we need to consider both rows and columns. We can access a specific member like this: my_data_frame[row, column]. Try playing around with the code below :) 3.3.1 External data We can read in external data using theread.csv() function. The main argument is the location of the data, which is either a url or a path on your computer. shrub_data &lt;- read.csv(&#39;https://datacarpentry.org/semester-biology/data/shrub-dimensions-labeled.csv&#39;) 3.3.2 Factors Let’s use the str() function to get more information about our variable shrub_data. str(shrub_data) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ shrubID: chr &quot;a1&quot; &quot;a2&quot; &quot;b1&quot; &quot;b2&quot; ... ## $ length : num 2.2 2.1 2.7 3 3.1 2.5 1.9 1.1 3.5 2.9 ## $ width : num 1.3 2.2 1.5 4.5 3.1 2.8 1.8 0.5 2 2.7 ## $ height : num 9.6 7.6 2.2 1.5 4 3 4.5 2.3 7.5 3.2 Notice that the shrubID column has type Factor. A factor is a special data type in R for categorical data. Factors are useful for statistics, but can mess up some aspects of computation as we’ll see in future chapters. shrub_data &lt;- read.csv(&#39;https://datacarpentry.org/semester-biology/data/shrub-dimensions-labeled.csv&#39;, stringsAsFactors = FALSE) str(shrub_data) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ shrubID: chr &quot;a1&quot; &quot;a2&quot; &quot;b1&quot; &quot;b2&quot; ... ## $ length : num 2.2 2.1 2.7 3 3.1 2.5 1.9 1.1 3.5 2.9 ## $ width : num 1.3 2.2 1.5 4.5 3.1 2.8 1.8 0.5 2 2.7 ## $ height : num 9.6 7.6 2.2 1.5 4 3 4.5 2.3 7.5 3.2 "],["loops-and-functions.html", "4 Loops and Functions 4.1 For loops 4.2 Functions", " 4 Loops and Functions Loops are fundamental a programming concept as they get a lot of repetitive stuff done in very few lines of code. Paired with custom functions, we can begin to tackle complex programming problems. 4.1 For loops Here’s what the syntax of a for loop looks like: for (item in list_of_items) { do_something(item) } And here is an example: for (i in 1:3) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 In the previous example, we used the dummy variable i to take on some range of values. Notice that i can be called anything you want. Try creating a for loop that prints the square of a number plus one for numbers ranging from 2 to 6. 4.1.1 Looping over multiple files We turn our attention now to a (slightly more) useful example: how do we analyze multiple files with similar contents? In this hypothetical example, we have 5 datasets with satellite coodinates at specific points orbiting the Earth. Suppose the files are similarly named (click on the files to download them): locations-2016-01-01.txt locations-2016-01-02.txt locations-2016-01-03.txt locations-2016-01-04.txt locations-2016-01-05.txt Our goal is to determine the number of satellite coordinates per file. First, retrieve the name of each file. my_dir &lt;- &quot;data/04_intro-to-r/&quot; # files are located in this location (on my computer) my_files &lt;- &quot;locations-.*.txt&quot; # file names follow this pattern data_files &lt;- list.files(path = my_dir, pattern = my_files, full.names = TRUE) Note that the asterisk in \"*.txt\" refers to “any name in this directory” whereas the \".txt\" part ensures we are only selecting .txt files. Next, determine the number of observations in each file. We will assume that each row corresponds to a single coordinate. results &lt;- vector(mode = &quot;integer&quot;, length = length(data_files)) for (i in 1:length(data_files)) { data &lt;- read.csv(data_files[i]) count &lt;- nrow(data) results[i] &lt;- count } Now, store the output in a data frame and associate the file name with the count. # initializing the data frame with empty columns results &lt;- data.frame(file_name = character(length(data_files)), count = integer(length(data_files)), stringsAsFactors = FALSE) # reading the data into the data frame for (i in 1:length(data_files)) { data &lt;- read.csv(data_files[i]) count &lt;- nrow(data) results$file_name[i] &lt;- data_files[i] results$count[i] &lt;- count } # voila! results ## file_name count ## 1 data/04_intro-to-r//locations-2016-01-01.txt 4 ## 2 data/04_intro-to-r//locations-2016-01-02.txt 8 ## 3 data/04_intro-to-r//locations-2016-01-03.txt 10 ## 4 data/04_intro-to-r//locations-2016-01-04.txt 10 ## 5 data/04_intro-to-r//locations-2016-01-05.txt 12 4.1.2 Nested loops Sometimes, we need to loop over more than a single range of numbers. For example, what if we want to select all pixels on a 2x3 rectangular screen? Here, we need to cover both the “x” and “y” pixel coodinates: for (i in 1:2) { for (j in 1:3) { print(paste(&quot;i = &quot; , i, &quot;; j = &quot;, j, sep=&quot;&quot;)) } } ## [1] &quot;i = 1; j = 1&quot; ## [1] &quot;i = 1; j = 2&quot; ## [1] &quot;i = 1; j = 3&quot; ## [1] &quot;i = 2; j = 1&quot; ## [1] &quot;i = 2; j = 2&quot; ## [1] &quot;i = 2; j = 3&quot; 4.2 Functions Sometimes, we will need to create custom functions. Luckily, we can define our own functions! This is the general syntax for a function: function_name &lt;- function(arguments) { output_value &lt;- do_something(inputs) return(output_value) } Remark: every function returns a value. Recall from your grade-school math class that functions take an input and return an output. In R, however, a function may or may not take user-defined input. This brings us to an extremely important point: creating a function does NOT run it. You must call the function to run it. As an exercise, create a function called calc_vol that takes three parameters length, width, and height, and use those values to calculate the volume of the object. Then, call the function to calculate the volume of a 1x1x1 object and a 3x2x5 object. Since R treats functions like a black box, you can’t access a variable that was created in a function. You must save the output of a function (to a variable) to use it later. 4.2.1 Conditionals within functions We can use conditionals in a function for more complex tasks. As an exercise, create a function called pred_c19_cases to predict the number of COVID-19 cases in a population (note that these numbers are fictional): The function will have two parameters pop_size (population size) and vac_brand (vaccine brand). If the vaccine is Moderna, multiply pop_size by 0.941. If the vaccine is Pfizer, multiply pop_size by 0.950. If the vaccine is Astrazeneca, multiply pop_size by 0.870. Return the predicted cases by subtracting the number of healthy individuals from pop_size. Now that we’ve got the basics of R under our belts, we can jump into the delightful world of data science 😄. "],["gapminder-and-dplyr.html", "5 Gapminder and dplyr 5.1 Get the gapminder data 5.2 Explore gapminder 5.3 Data frames with dplyr", " 5 Gapminder and dplyr Whenever you have rectangular, “spreadsheetey” data, your default data structure in R should be the data frame. Do not depart from this without good reason. Data frames are awesome because They neatly package related variables and maintain a “row-ordering” like that in a spreadsheet. This makes it easy to apply filters to rows and columns of interest. Most functions for inference, modelling, and graphing will happily take a data frame object. The set of packages known as the tidyverse takes data frames one step further and explicitly prioritizes the processing of data frames. Data frames, unlike general arrays or matrices in R, can hold variables of different flavours. For example, data frames can simultaneously hold character data (e.g., subject ID or name), quantitative data (e.g., white blood cell count), and categorical information (e.g., treated vs. untreated). If you use data structures that can only hold 1 type of data (e.g., matrices) for data analysis, you are likely to make the terrible mistake of spreading a dataset out over multiple, unlinked objects. Why? Because you can’t put character data, such as subject name, into the numeric matrix that holds white blood cell count. This fragmentation is a bad idea. 5.1 Get the gapminder data We will work with some of the data from the Gapminder project. The Gapminder project contains the gapminder dataset, which summarises the progression of countries over time for statistics like life expectancy and GDP. If you haven’t installed gapminder yet, you can do so like this: install.packages(&quot;gapminder&quot;) Now load the package. library(gapminder) 5.2 Explore gapminder By loading the gapminder package, we now have access to a data frame by the same name. Get an overview of the data frame with str(), which displays the structure of an object. str(gapminder) ## tibble [1,704 × 6] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... ## $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... ## $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... ## $ gdpPercap: num [1:1704] 779 821 853 836 740 ... str() will provide a sensible description of almost anything and, worst case, nothing bad can actually happen. When in doubt, just use str() on your recently created objects to get ideas about what to do next. We could print the gapminder object itself to screen. However, if you’ve used R before, you might be reluctant to do this, because large datasets fill your Console and provide very little insight. If you have not already done so, install the tidyverse meta-package now: install.packages(&quot;tidyverse&quot;) Now load it: library(tidyverse) class(gapminder) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; We can now print gapminder! Notice that the class (type of data structure) of the gapminder object is a tibble, the tidyverse’s version of R’s data frame. A tibble is also a data frame. gapminder ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows Although this seems like a lot of output, notice that tibbles provide a nice print method that shows the most important stuff and doesn’t fill up your console. Let’s make sense of the output: The first line refers to what we’re printing—a tibble with 1704 rows and 6 columns. Below each column heading, we see &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;. These refer to the variable type of that column. fct is a factor (kind of like a categorical variable), int stands for integer, and dbl stands for double (a number with decimal places). If you’re only interested in the first or last couple of rows, use head() or tail(). head() displays the first 6 rows of your data frame by default, and tail() shows the last 6 rows. head(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. tail(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Zimbabwe Africa 1982 60.4 7636524 789. ## 2 Zimbabwe Africa 1987 62.4 9216418 706. ## 3 Zimbabwe Africa 1992 60.4 10704340 693. ## 4 Zimbabwe Africa 1997 46.8 11404948 792. ## 5 Zimbabwe Africa 2002 40.0 11926563 672. ## 6 Zimbabwe Africa 2007 43.5 12311143 470. You can also specify the number of rows displayed by passing in a number. head(gapminder, n = 3) ## # A tibble: 3 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. Just for your reference, if you want to change a data frame into a tibble for nicer printing, use as_tibble()! as_tibble(my_data_frame) # my_data_frame is the thing we want to make a tibble Here are more ways to query basic info on a data frame: Function Description names() returns column names ncol() returns number of columns nrow() returns number of rows dim() returns # of rows by # of columns names(gapminder) ## [1] &quot;country&quot; &quot;continent&quot; &quot;year&quot; &quot;lifeExp&quot; &quot;pop&quot; &quot;gdpPercap&quot; ncol(gapminder) ## [1] 6 nrow(gapminder) ## [1] 1704 dim(gapminder) ## [1] 1704 6 A statistical summary of the data can be obtained with summary(). That is, each column’s statistics are shown separately. summary(gapminder) ## country continent year lifeExp ## Afghanistan: 12 Africa :624 Min. :1952 Min. :23.60 ## Albania : 12 Americas:300 1st Qu.:1966 1st Qu.:48.20 ## Algeria : 12 Asia :396 Median :1980 Median :60.71 ## Angola : 12 Europe :360 Mean :1980 Mean :59.47 ## Argentina : 12 Oceania : 24 3rd Qu.:1993 3rd Qu.:70.85 ## Australia : 12 Max. :2007 Max. :82.60 ## (Other) :1632 ## pop gdpPercap ## Min. :6.001e+04 Min. : 241.2 ## 1st Qu.:2.794e+06 1st Qu.: 1202.1 ## Median :7.024e+06 Median : 3531.8 ## Mean :2.960e+07 Mean : 7215.3 ## 3rd Qu.:1.959e+07 3rd Qu.: 9325.5 ## Max. :1.319e+09 Max. :113523.1 ## 5.2.1 Importing and exporting data We can export the data frame to a comma-separated values (.csv) file. write.csv(gapminder, file = &quot;data/03_data-frames/gapminder.csv&quot;) The “.csv” extension stands for comma-separated values. This is the preferred way of importing and exporting data as it contains no formatting. You can also import a .csv file to Excel. On top of writing to a .csv file, we can also read .csv files into R. It’s as simple as read.csv()! gapminder2 &lt;- read.csv(&quot;data/03_data-frames/gapminder.csv&quot;, header = TRUE) class(gapminder2) ## [1] &quot;data.frame&quot; As you can see,read.csv() returns a data frame object by default. 5.2.2 Exploring variables in a data frame To specify a single variable from a data frame, use the dollar sign $. Let’s explore the numeric variable for life expectancy. head(gapminder$lifeExp) ## [1] 28.801 30.332 31.997 34.020 36.088 38.438 summary(gapminder$lifeExp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 23.60 48.20 60.71 59.47 70.85 82.60 hist(gapminder$lifeExp) Don’t worry too much about the code to make the figures right now—we will learn how to visualize data in future lectures. For now, let’s continue to explore gapminder. Take a look at the year variable: class(gapminder$year) ## [1] &quot;integer&quot; Notice that year holds integers. On the other hand, continent holds categorical information, which is called a factor in R. class(gapminder$continent) ## [1] &quot;factor&quot; Now, I want to illustrate something important: summary(gapminder$year) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1952 1966 1980 1980 1993 2007 summary(gapminder$continent) ## Africa Americas Asia Europe Oceania ## 624 300 396 360 24 Notice that the same function returned different outputs for different variable types—forgetting this observation can lead to confusion in the future, so make sure to check your data before analysis! Let’s check out a couple more useful functions and highlight important ideas in the meantime. Within a given column/variable, table() returns the number of observations, levels() returns unique values, and nlevels() returns the number of unique values. table(gapminder$continent) ## ## Africa Americas Asia Europe Oceania ## 624 300 396 360 24 levels(gapminder$continent) ## [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; nlevels(gapminder$continent) ## [1] 5 The levels of the factor continent are “Africa,” “Americas,” etc.—this is what’s usually presented to your eyeballs by R. Behind the scenes, R assigns integer values (i.e., 1, 2, 3, …) to each level. Never ever ever forget this fact. Look at the result from str(gapminder$continent) if you are skeptical: str(gapminder$continent) ## Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... Specifically in modelling and figure-making, factors are anticipated and accommodated by the functions and packages you will want to exploit. Note that factors do NOT contain integers. Factors are a numerical way that R uses to represent categorical data. Tl;dr, factors are categorical variables whereas levels are unique values within a factor. 5.2.3 Data frame summary Use data frames and the tidyverse!! The tidyverse provides a special type of data frame called a “tibble” that has nice default printing behavior, among other benefits. When in doubt, str() something or print something. Always understand the basic extent of your data frames: number of rows and columns. Understand what your variable types are. Use factors!! (but with intention and care) Do basic statistical and visual sanity checking of each variable. Refer to variables by name (ex: gapminder$lifeExp) and NOT by column number. Your code will be more robust and readable. 5.3 Data frames with dplyr dplyr is a package for data manipulation developed by Hadley Wickham and Romain Francois. It is built to be fast, highly expressive, and open-minded about how your data is stored. It is installed as part of the the tidyverse meta-package and it is among the packages loaded via library(tidyverse). Here’s a bit of fun trivia: dplyr stands for “data frame pliers.” 5.3.1 Subsetting data If you feel the urge to store a little snippet of your data: canada &lt;- gapminder[241:252, ] Stop and ask yourself … Do I want to create a separate subset of my original data? If “YES,” use proper data aggregation techniques or don’t subset the data. Alternatively, only subset the data as a temporary measure while you develop your elegant code. If “NO,” then don’t subset! Copies and excerpts of your data clutter your workspace, invite mistakes, and sow general confusion. Avoid whenever possible. Reality can also lie somewhere in between. You will find the workflows presented below can help you accomplish your goals with minimal creation of temporary, intermediate objects. Recall therm() function, which removes unwanted variable(s). x &lt;- &#39;thing to not keep&#39; print(x) rm(x) # print(x) # gives an error because x is deleted 5.3.2 Filter rows with filter() filter() takes logical expressions and returns the rows for which all are TRUE. Use this when you want to subset observations based on values. The first argument is the name of the data frame. The subsequent arguments are the expressions that filter the dataframe. For example, let’s filter all rows from gapminder where life expectancy is less than 29 years. filter(gapminder, lifeExp &lt; 29) ## # A tibble: 2 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Rwanda Africa 1992 23.6 7290203 737. When you run this line of code, dplyr filters the data and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you need to use the assignment operator, &lt;-. Let’s try this out! Here we filter based on country and year: rwanda_gthan_1979 &lt;- filter(gapminder, country == &quot;Rwanda&quot;, year &gt; 1979) Compare with some base R code to accomplish the same things: gapminder[gapminder$lifeExp &lt; 29, ] # indexing is distracting subset(gapminder, country == &quot;Rwanda&quot;) # almost same as filter; quite nice actually What if you want to filter rows based on multiple values in a variable? For example, what if we want to filter all rows with either Rwanda or Afghanistan as countries? filter(gapminder, country == &quot;Rwanda&quot; | country == &quot;Afghanistan&quot;) Here we use a Boolean operator, |, which means “or.” Boolean operators always return either TRUE or FALSE. Some other common ones are &amp; (and) and ! (not). What if we want to keep more than just 2 countries? One way would be to string Boolean operators together like so: country == \"Canada\" | country == \"Rwanda\" | country == \"Afghanistan | ... This, however, is very wordy. A useful shortcut is to use x %in% y. This selects every row where x is one of the values in y: filter(gapminder, country %in% c(&quot;Rwanda&quot;, &quot;Afghanistan&quot;)) filter(gapminder, country %in% c(&quot;Canada&quot;, &quot;Rwanda&quot;, &quot;Afghanistan&quot;)) Under no circumstances should you subset your data the way I did at first: excerpt &lt;- gapminder[241:252, ] Why is this a terrible idea? It is not self-documenting. What is so special about rows 241 through 252? It is fragile. This line of code will produce different results if someone changes the row order of gapminder, e.g. sorts the data earlier in the script. filter(gapminder, country == &quot;Canada&quot;) This call explains itself and is fairly robust. 5.3.3 Pipe operator %&gt;% Before we go any further, we should exploit the new pipe operator that the tidyverse imports from the magrittr package by Stefan Bache. Here’s what it looks like: %&gt;%. The RStudio keyboard shortcut: Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac). Let’s demo then I’ll explain: gapminder %&gt;% head() ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. The above code is equivalent to head(gapminder). The pipe operator takes the thing on the left-hand-side and pipes it into the function call on the right-hand-side. It literally drops it in as the first argument. You can think of an argument as your input to a function. If you remember your grade school math, functions in R do exactly what you’ve learned in school – it takes inputs (arguments/parameters) and spits an output, or a return value. Never fear, you can still specify other arguments to this function! To see the first 3 rows of Gapminder, we could say head(gapminder, 3) or this: gapminder %&gt;% head(3) ## # A tibble: 3 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. You are probably not impressed yet, but the magic will happen soon. 5.3.4 Select Columns with select() Use select() to subset the data on variables or columns. Here’s a conventional call: select(gapminder, year, lifeExp) ## # A tibble: 1,704 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 ## 2 1957 30.3 ## 3 1962 32.0 ## 4 1967 34.0 ## 5 1972 36.1 ## 6 1977 38.4 ## 7 1982 39.9 ## 8 1987 40.8 ## 9 1992 41.7 ## 10 1997 41.8 ## # … with 1,694 more rows And here’s the same operation, but written with the pipe operator and piped through head(): gapminder %&gt;% select(year, lifeExp) %&gt;% head(4) ## # A tibble: 4 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 ## 2 1957 30.3 ## 3 1962 32.0 ## 4 1967 34.0 Think: “Take gapminder, then select the variables year and lifeExp, then show the first 4 rows.” If we didn’t have the pipe operator, this is what the above function would look like: head(select(gapminder, year, lifeExp), 4) ## # A tibble: 4 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 ## 2 1957 30.3 ## 3 1962 32.0 ## 4 1967 34.0 As you can see, this is way harder to read. That’s why the pipe operator is so useful. An important note is that select does not actually filter any rows. It simply selects columns. select() used alongisde everything() is also quite handy if you want to move variables within your data frame. The everything() function selects all variables not explicitly mentioned in select(). For example, let’s move year and continent to the front of the gapminder tibble: select(gapminder, year, continent, everything()) ## # A tibble: 1,704 x 6 ## year continent country lifeExp pop gdpPercap ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1952 Asia Afghanistan 28.8 8425333 779. ## 2 1957 Asia Afghanistan 30.3 9240934 821. ## 3 1962 Asia Afghanistan 32.0 10267083 853. ## 4 1967 Asia Afghanistan 34.0 11537966 836. ## 5 1972 Asia Afghanistan 36.1 13079460 740. ## 6 1977 Asia Afghanistan 38.4 14880372 786. ## 7 1982 Asia Afghanistan 39.9 12881816 978. ## 8 1987 Asia Afghanistan 40.8 13867957 852. ## 9 1992 Asia Afghanistan 41.7 16317921 649. ## 10 1997 Asia Afghanistan 41.8 22227415 635. ## # … with 1,694 more rows Here’s the data for Cambodia, but only certain variables… gapminder %&gt;% filter(country == &quot;Cambodia&quot;) %&gt;% select(year, lifeExp) ## # A tibble: 12 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 39.4 ## 2 1957 41.4 ## 3 1962 43.4 ## 4 1967 45.4 ## 5 1972 40.3 ## 6 1977 31.2 ## 7 1982 51.0 ## 8 1987 53.9 ## 9 1992 55.8 ## 10 1997 56.5 ## 11 2002 56.8 ## 12 2007 59.7 … and what a typical base R call would look like: gapminder[gapminder$country == &quot;Cambodia&quot;, c(&quot;year&quot;, &quot;lifeExp&quot;)] ## # A tibble: 12 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 39.4 ## 2 1957 41.4 ## 3 1962 43.4 ## 4 1967 45.4 ## 5 1972 40.3 ## 6 1977 31.2 ## 7 1982 51.0 ## 8 1987 53.9 ## 9 1992 55.8 ## 10 1997 56.5 ## 11 2002 56.8 ## 12 2007 59.7 5.3.5 Pure, predictable, pipeable (OPTIONAL) We’ve barely scratched the surface of dplyr but I want to point out key principles you may start to appreciate. If you’re new to R or “programming with data,” feel free skip this section. dplyr’s verbs, such as filter() and select(), are what’s called pure functions. To quote from Wickham’s Advanced R Programming book: The functions that are the easiest to understand and reason about are pure functions: functions that always map the same input to the same output and have no other impact on the workspace. In other words, pure functions have no side effects: they don’t affect the state of the world in any way apart from the value they return. And finally, the data is always the very first argument of every dplyr function. 5.3.6 Additional resources dplyr official stuff package home on CRAN note there are several vignettes, with the introduction being the most relevant right now the one on window functions will also be interesting to you now development home on GitHub tutorial HW delivered (note this links to a DropBox folder) at useR! 2014 conference RStudio Data Wrangling cheatsheet, covering dplyr and tidyr. Remember you can get to these via Help &gt; Cheatsheets. Excellent slides on pipelines and dplyr by TJ Mahr, talk given to the Madison R Users Group. Blog post Hands-on dplyr tutorial for faster data manipulation in R by Data School, that includes a link to an R Markdown document and links to videos Cheatsheet from R Studio for dplyr. "],["more-dplyr.html", "6 More dplyr 6.1 Review and preparation 6.2 Use mutate() to add new variables 6.3 Use arrange() to row-order data 6.4 Use rename() to rename variables 6.5 Perform tasks on subsets with group_by() 6.6 Introduction to visualization (OPTIONAL) 6.7 Comprehensive practice 6.8 Data wrangling summary", " 6 More dplyr 6.1 Review and preparation In the previous chapter, we introduced three important data wrangling concepts: filter() for subsetting rows select() for subsetting columns (i.e., variables) The pipe operator %&gt;%, which feeds the left-hand side as the first argument to the expression on the right-hand side We also discussed dplyr’s role inside the tidyverse and tibbles: dplyr is a core package in the tidyverse meta-package. Since we often make incidental usage of the others, we will load dplyr and the others via library(tidyverse). The tidyverse embraces a special flavor of data frame, called a tibble. The gapminder dataset is stored as a tibble. Let’s load the tidyverse and gapminder. library(tidyverse) library(gapminder) We’re going to make changes to the gapminder tibble. To eliminate any fear that you’re damaging the data that comes with the package, let’s create an explicit copy of gapminder for our experiments. Don’t worry if you modify the gapminder package, since your changes are temporary (i.e., you can reload the gapminder to get a fresh dataset). my_gap &lt;- gapminder Pay close attention when we evaluate statements but let the output just print to screen… ## let output print to screen, but do not store my_gap %&gt;% filter(country == &quot;Canada&quot;) ## # A tibble: 12 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Canada Americas 1952 68.8 14785584 11367. ## 2 Canada Americas 1957 70.0 17010154 12490. ## 3 Canada Americas 1962 71.3 18985849 13462. ## 4 Canada Americas 1967 72.1 20819767 16077. ## 5 Canada Americas 1972 72.9 22284500 18971. ## 6 Canada Americas 1977 74.2 23796400 22091. ## 7 Canada Americas 1982 75.8 25201900 22899. ## 8 Canada Americas 1987 76.9 26549700 26627. ## 9 Canada Americas 1992 78.0 28523502 26343. ## 10 Canada Americas 1997 78.6 30305843 28955. ## 11 Canada Americas 2002 79.8 31902268 33329. ## 12 Canada Americas 2007 80.7 33390141 36319. … versus when we assign the output to a new variable, or overwritting one that already exists. ## store the output as an R object my_precious &lt;- my_gap %&gt;% filter(country == &quot;Canada&quot;) 6.2 Use mutate() to add new variables Imagine we wanted to recover each country’s GDP. After all, the Gapminder data has a variable for population and GDP per capita. Let’s multiply them together to get the GDP of the whole country. The mutate() function defines and inserts new variables into a data frame/tibble. You can refer to existing variables by name. my_gap %&gt;% mutate(gdp = pop * gdpPercap) ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap gdp ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. ## 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. ## 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. ## 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. ## 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. ## 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. ## 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. ## # … with 1,694 more rows If you don’t want to add a new column to your tibble, you can use transmute(). It works just like mutate() except it only keeps the column(s) you specify. Let’s save our output in a new tibble called gap_gdp. Recall that saving the return of functions generally suppresses printing to the console. If you want to see the output, either type or print the variable. gap_gdp &lt;- my_gap %&gt;% transmute(country, gdp = pop * gdpPercap) gap_gdp # or use print(gap_gdp) ## # A tibble: 1,704 x 2 ## country gdp ## &lt;fct&gt; &lt;dbl&gt; ## 1 Afghanistan 6567086330. ## 2 Afghanistan 7585448670. ## 3 Afghanistan 8758855797. ## 4 Afghanistan 9648014150. ## 5 Afghanistan 9678553274. ## 6 Afghanistan 11697659231. ## 7 Afghanistan 12598563401. ## 8 Afghanistan 11820990309. ## 9 Afghanistan 10595901589. ## 10 Afghanistan 14121995875. ## # … with 1,694 more rows Hmmm… those GDP numbers are almost uselessly large and abstract. Consider the advice of Randall Munroe of xkcd: One thing that bothers me is large numbers presented without context… ‘If I added a zero to this number, would the sentence containing it mean something different to me?’ If the answer is ‘no,’ maybe the number has no business being in the sentence in the first place.\" Maybe it would be more meaningful to consumers of my tables and figures to stick with GDP per capita. But what if I reported GDP per capita, relative to some benchmark country. Since Canada is my home country, I’ll go with that. I need to create a new variable that is gdpPercap divided by Canadian gdpPercap, taking care that I always divide two numbers that pertain to the same year. Here is what we need to do: Filter down to the rows for Canada. Create a new temporary variable in my_gap: Extract the gdpPercap variable from the Canadian data. Replicate it once per country in the dataset, so it has the right length. Divide raw gdpPercap by this Canadian figure. Discard the temporary variable of replicated Canadian gdpPercap. ctib &lt;- my_gap %&gt;% filter(country == &quot;Canada&quot;) ## this is a semi-dangerous way to add this variable ## I&#39;d prefer to join on year, but we haven&#39;t covered joins yet my_gap &lt;- my_gap %&gt;% mutate(tmp = rep(ctib$gdpPercap, nlevels(country)), gdpPercapRel = gdpPercap / tmp, tmp = NULL) Note that, mutate() builds new variables sequentially so you can reference earlier ones (like tmp) when defining later ones (like gdpPercapRel). Also, you can get rid of a variable by setting it to NULL. How could we sanity check that this worked? The Canadian values for gdpPercapRel better all be 1! my_gap %&gt;% filter(country == &quot;Canada&quot;) %&gt;% select(country, year, gdpPercapRel) ## # A tibble: 12 x 3 ## country year gdpPercapRel ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Canada 1952 1 ## 2 Canada 1957 1 ## 3 Canada 1962 1 ## 4 Canada 1967 1 ## 5 Canada 1972 1 ## 6 Canada 1977 1 ## 7 Canada 1982 1 ## 8 Canada 1987 1 ## 9 Canada 1992 1 ## 10 Canada 1997 1 ## 11 Canada 2002 1 ## 12 Canada 2007 1 I perceive Canada to be a “high GDP” country, so I predict that the distribution of gdpPercapRel is located below 1, possibly even well below. Check your intuition! summary(my_gap$gdpPercapRel) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.007236 0.061648 0.171521 0.326659 0.446564 9.534690 The relative GDP per capita numbers are, in general, well below 1. We see that most of the countries covered by this dataset have substantially lower GDP per capita, relative to Canada, across the entire time period. Remember: Trust No One. Including (especially?) yourself. Always try to find a way to check that you’ve done what meant to. Prepare to be horrified. 6.3 Use arrange() to row-order data The arrange() function reorders rows in a data frame/tibble. Imagine you wanted this data ordered by year then country, as opposed to by country then year. Remember, to save the output, you must assign it to a variable. my_gap %&gt;% arrange(year, country) ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap gdpPercapRel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 0.0686 ## 2 Albania Europe 1952 55.2 1282697 1601. 0.141 ## 3 Algeria Africa 1952 43.1 9279525 2449. 0.215 ## 4 Angola Africa 1952 30.0 4232095 3521. 0.310 ## 5 Argentina Americas 1952 62.5 17876956 5911. 0.520 ## 6 Australia Oceania 1952 69.1 8691212 10040. 0.883 ## 7 Austria Europe 1952 66.8 6927772 6137. 0.540 ## 8 Bahrain Asia 1952 50.9 120447 9867. 0.868 ## 9 Bangladesh Asia 1952 37.5 46886859 684. 0.0602 ## 10 Belgium Europe 1952 68 8730405 8343. 0.734 ## # … with 1,694 more rows Or maybe you want just the data from 2007, sorted on life expectancy? my_gap %&gt;% filter(year == 2007) %&gt;% arrange(lifeExp) ## # A tibble: 142 x 7 ## country continent year lifeExp pop gdpPercap gdpPercapRel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Swaziland Africa 2007 39.6 1.13e6 4513. 0.124 ## 2 Mozambique Africa 2007 42.1 2.00e7 824. 0.0227 ## 3 Zambia Africa 2007 42.4 1.17e7 1271. 0.0350 ## 4 Sierra Leone Africa 2007 42.6 6.14e6 863. 0.0237 ## 5 Lesotho Africa 2007 42.6 2.01e6 1569. 0.0432 ## 6 Angola Africa 2007 42.7 1.24e7 4797. 0.132 ## 7 Zimbabwe Africa 2007 43.5 1.23e7 470. 0.0129 ## 8 Afghanistan Asia 2007 43.8 3.19e7 975. 0.0268 ## 9 Central African Repub… Africa 2007 44.7 4.37e6 706. 0.0194 ## 10 Liberia Africa 2007 45.7 3.19e6 415. 0.0114 ## # … with 132 more rows Oh, you’d like to sort on life expectancy in descending order? Then use desc(). my_gap %&gt;% filter(year == 2007) %&gt;% arrange(desc(lifeExp)) ## # A tibble: 142 x 7 ## country continent year lifeExp pop gdpPercap gdpPercapRel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. 0.872 ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. 1.09 ## 3 Iceland Europe 2007 81.8 301931 36181. 0.996 ## 4 Switzerland Europe 2007 81.7 7554661 37506. 1.03 ## 5 Australia Oceania 2007 81.2 20434176 34435. 0.948 ## 6 Spain Europe 2007 80.9 40448191 28821. 0.794 ## 7 Sweden Europe 2007 80.9 9031088 33860. 0.932 ## 8 Israel Asia 2007 80.7 6426679 25523. 0.703 ## 9 France Europe 2007 80.7 61083916 30470. 0.839 ## 10 Canada Americas 2007 80.7 33390141 36319. 1 ## # … with 132 more rows I advise that your analyses NEVER rely on rows or variables being in a specific order. But it’s still true that human beings write the code and the interactive development process can be much nicer if you reorder the rows of your data as you go along. Also, once you are preparing tables for human eyeballs, it is imperative that you step up and take control of row order. 6.4 Use rename() to rename variables When I started programming, I was a camelCase person, but now I’m all about snake_case. Let’s rename some variables! my_gap %&gt;% rename(life_exp = lifeExp, gdp_percap = gdpPercap, gdp_percap_rel = gdpPercapRel) ## # A tibble: 1,704 x 7 ## country continent year life_exp pop gdp_percap gdp_percap_rel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 0.0686 ## 2 Afghanistan Asia 1957 30.3 9240934 821. 0.0657 ## 3 Afghanistan Asia 1962 32.0 10267083 853. 0.0634 ## 4 Afghanistan Asia 1967 34.0 11537966 836. 0.0520 ## 5 Afghanistan Asia 1972 36.1 13079460 740. 0.0390 ## 6 Afghanistan Asia 1977 38.4 14880372 786. 0.0356 ## 7 Afghanistan Asia 1982 39.9 12881816 978. 0.0427 ## 8 Afghanistan Asia 1987 40.8 13867957 852. 0.0320 ## 9 Afghanistan Asia 1992 41.7 16317921 649. 0.0246 ## 10 Afghanistan Asia 1997 41.8 22227415 635. 0.0219 ## # … with 1,694 more rows I did NOT assign the post-rename object back to my_gap because that would make the chunks in this tutorial harder to copy/paste and run out of order. In real life, I would probably assign this back to my_gap, in a data preparation script, and proceed with the new variable names. 6.4.1 Use select() to rename and reposition variables You’ve seen simple uses of select(). There are two tricks you might enjoy: select() can rename the variables you request to keep. select() can be used with everything() to hoist a variable up to the front of the tibble. my_gap %&gt;% filter(country == &quot;Burundi&quot;, year &gt; 1996) %&gt;% select(yr = year, lifeExp, gdpPercap) %&gt;% select(gdpPercap, everything()) ## # A tibble: 3 x 3 ## gdpPercap yr lifeExp ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 463. 1997 45.3 ## 2 446. 2002 47.4 ## 3 430. 2007 49.6 everything() is one of several helpers for variable selection. Read the documentation to see the rest. 6.5 Perform tasks on subsets with group_by() I have found collaborators love to ask seemingly innocuous questions like, “which country experienced the sharpest 5-year drop in life expectancy?” In fact, that is a totally natural question to ask. But if you are using a language that doesn’t know about data, it’s an incredibly annoying question to answer. dplyr offers powerful tools to solve this class of problem. group_by() adds extra structure to your dataset – grouping information – which lays the groundwork for computations within the groups. summarize() takes a dataset with \\(n\\) observations, computes requested summaries, and returns a dataset with 1 observation. Window functions take a dataset with \\(n\\) observations and return a dataset with \\(n\\) observations. You can also do very general computations on your groups with do(). Combined with the verbs you already know, these new tools allow you to solve an extremely diverse set of problems with relative ease. 6.5.1 Counting Let’s start with simple counting. How many observations do we have per continent? The n() function counts the number of observations in a particular group. my_gap %&gt;% group_by(continent) %&gt;% summarize(n = n()) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 Let us pause here to think about the tidyverse. You could get these same frequencies using table() from base R. table(gapminder$continent) ## ## Africa Americas Asia Europe Oceania ## 624 300 396 360 24 str(table(gapminder$continent)) ## &#39;table&#39; int [1:5(1d)] 624 300 396 360 24 ## - attr(*, &quot;dimnames&quot;)=List of 1 ## ..$ : chr [1:5] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; ... But the object of class table that is returned makes downstream computation a bit fiddlier than we would like. For example, it’s too bad the continent levels come back only as names and not as a proper factor, with the original set of levels. The tally() function is a convenient function that counts rows. my_gap %&gt;% group_by(continent) %&gt;% tally() ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 The count() function is an even more convenient function that does both grouping and counting. my_gap %&gt;% count(continent) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 What if we wanted to add the number of unique countries for each continent? You can compute multiple summaries inside summarize(). Use the n_distinct() function to count the number of distinct countries within each continent. my_gap %&gt;% group_by(continent) %&gt;% summarize(n = n(), n_countries = n_distinct(country)) ## # A tibble: 5 x 3 ## continent n n_countries ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 Africa 624 52 ## 2 Americas 300 25 ## 3 Asia 396 33 ## 4 Europe 360 30 ## 5 Oceania 24 2 6.5.2 General summarization The functions you’ll apply within summarize() include classical statistical summaries, like mean(), median(), var(), sd(), mad(), IQR(), min(), and max(). Remember they are functions that take \\(n\\) inputs and distill them down into 1 output. Although this may be statistically ill-advised, let’s compute the average life expectancy by continent. my_gap %&gt;% group_by(continent) %&gt;% summarize(avg_lifeExp = mean(lifeExp)) ## # A tibble: 5 x 2 ## continent avg_lifeExp ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 48.9 ## 2 Americas 64.7 ## 3 Asia 60.1 ## 4 Europe 71.9 ## 5 Oceania 74.3 summarize_at() applies the same summary function(s) to multiple variables. Let’s compute average and median life expectancy and GDP per capita by continent by year … but only for 1952 and 2007. my_gap %&gt;% filter(year %in% c(1952, 2007)) %&gt;% group_by(continent, year) %&gt;% summarize_at(vars(lifeExp, gdpPercap), funs(mean, median)) ## # A tibble: 10 x 6 ## # Groups: continent [5] ## continent year lifeExp_mean gdpPercap_mean lifeExp_median gdpPercap_median ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 1952 39.1 1253. 38.8 987. ## 2 Africa 2007 54.8 3089. 52.9 1452. ## 3 Americas 1952 53.3 4079. 54.7 3048. ## 4 Americas 2007 73.6 11003. 72.9 8948. ## 5 Asia 1952 46.3 5195. 44.9 1207. ## 6 Asia 2007 70.7 12473. 72.4 4471. ## 7 Europe 1952 64.4 5661. 65.9 5142. ## 8 Europe 2007 77.6 25054. 78.6 28054. ## 9 Oceania 1952 69.3 10298. 69.3 10298. ## 10 Oceania 2007 80.7 29810. 80.7 29810. Let’s focus just on Asia. What are the minimum and maximum life expectancies seen by year? my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% group_by(year) %&gt;% summarize(min_lifeExp = min(lifeExp), max_lifeExp = max(lifeExp)) ## # A tibble: 12 x 3 ## year min_lifeExp max_lifeExp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1952 28.8 65.4 ## 2 1957 30.3 67.8 ## 3 1962 32.0 69.4 ## 4 1967 34.0 71.4 ## 5 1972 36.1 73.4 ## 6 1977 31.2 75.4 ## 7 1982 39.9 77.1 ## 8 1987 40.8 78.7 ## 9 1992 41.7 79.4 ## 10 1997 41.8 80.7 ## 11 2002 42.1 82 ## 12 2007 43.8 82.6 Of course it would be much more interesting to see which country contributed these extreme observations. Is the minimum (maximum) always coming from the same country?We will tackle this with window functions shortly. 6.5.3 Computing with group-wise summaries Don’t worry too much about this section if all the data wrangling is starting to become overwhelming – it’s mainly here for the curious. Let’s make a new variable that is the years of life expectancy gained (lost) relative to 1952, for each individual country. We group by country and use mutate() to make a new variable. The first() function extracts the first value from a vector. Notice that first() is operating on the vector of life expectancies within each country group. new_var &lt;- my_gap %&gt;% group_by(country) %&gt;% select(country, year, lifeExp) %&gt;% mutate(lifeExp_gain = lifeExp - first(lifeExp)) %&gt;% filter(year &lt; 1963) new_var ## # A tibble: 426 x 4 ## # Groups: country [142] ## country year lifeExp lifeExp_gain ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 28.8 0 ## 2 Afghanistan 1957 30.3 1.53 ## 3 Afghanistan 1962 32.0 3.20 ## 4 Albania 1952 55.2 0 ## 5 Albania 1957 59.3 4.05 ## 6 Albania 1962 64.8 9.59 ## 7 Algeria 1952 43.1 0 ## 8 Algeria 1957 45.7 2.61 ## 9 Algeria 1962 48.3 5.23 ## 10 Angola 1952 30.0 0 ## # … with 416 more rows Within country, we take the difference between life expectancy in year \\(i\\) and life expectancy in 1952. Therefore we always see zeroes for 1952 and, for most countries, a sequence of positive and increasing numbers. 6.5.4 Window functions (OPTIONAL) Window functions take \\(n\\) inputs and give back \\(n\\) outputs. Furthermore, the output depends on all the values. So rank() is a window function but sum() is not. Here we use window functions based on ranks and offsets. Let’s revisit the worst and best life expectancies in Asia over time, but retaining info about which country contributes these extreme values. my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% group_by(year) %&gt;% filter(min_rank(desc(lifeExp)) &lt; 2 | min_rank(lifeExp) &lt; 2) %&gt;% arrange(year) ## # A tibble: 24 x 3 ## # Groups: year [12] ## year country lifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Afghanistan 28.8 ## 2 1952 Israel 65.4 ## 3 1957 Afghanistan 30.3 ## 4 1957 Israel 67.8 ## 5 1962 Afghanistan 32.0 ## 6 1962 Israel 69.4 ## 7 1967 Afghanistan 34.0 ## 8 1967 Japan 71.4 ## 9 1972 Afghanistan 36.1 ## 10 1972 Japan 73.4 ## # … with 14 more rows We see that (min = Afghanistan, max = Japan) is the most frequent result, but Cambodia and Israel pop up at least once each as the min or max, respectively. That table should make you impatient for our upcoming work on tidying and reshaping data! Wouldn’t it be nice to have one row per year? How did that actually work? First, I store and view a partial that leaves off the filter() statement. All of these operations should be familiar. asia &lt;- my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% group_by(year) asia ## # A tibble: 396 x 3 ## # Groups: year [12] ## year country lifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Afghanistan 28.8 ## 2 1957 Afghanistan 30.3 ## 3 1962 Afghanistan 32.0 ## 4 1967 Afghanistan 34.0 ## 5 1972 Afghanistan 36.1 ## 6 1977 Afghanistan 38.4 ## 7 1982 Afghanistan 39.9 ## 8 1987 Afghanistan 40.8 ## 9 1992 Afghanistan 41.7 ## 10 1997 Afghanistan 41.8 ## # … with 386 more rows Now we apply a window function: min_rank(). Since asia is grouped by year, min_rank() operates within mini-datasets, each for a specific year. Applied to the variable lifeExp, min_rank() returns the rank of each country’s observed life expectancy. FYI, the min part just specifies how ties are broken. Here is an explicit peek at these within-year life expectancy ranks, in both the (default) ascending and descending order. If you specify rank(), ties will be denoted by .5. For instance: x &lt;- c(1, 2, 3, 3, 4) min_rank(x) ## [1] 1 2 3 3 5 rank(x) ## [1] 1.0 2.0 3.5 3.5 5.0 For concreteness, I use mutate() to actually create these variables, even though I dropped this in the solution above. Let’s look at a bit of that. asia %&gt;% mutate(le_rank = min_rank(lifeExp), le_desc_rank = min_rank(desc(lifeExp))) %&gt;% filter(country %in% c(&quot;Afghanistan&quot;, &quot;Japan&quot;, &quot;Thailand&quot;), year &gt; 1995) ## # A tibble: 9 x 5 ## # Groups: year [3] ## year country lifeExp le_rank le_desc_rank ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1997 Afghanistan 41.8 1 33 ## 2 2002 Afghanistan 42.1 1 33 ## 3 2007 Afghanistan 43.8 1 33 ## 4 1997 Japan 80.7 33 1 ## 5 2002 Japan 82 33 1 ## 6 2007 Japan 82.6 33 1 ## 7 1997 Thailand 67.5 12 22 ## 8 2002 Thailand 68.6 12 22 ## 9 2007 Thailand 70.6 12 22 Afghanistan tends to present 1’s in the le_rank variable, Japan tends to present 1’s in the le_desc_rank variable and other countries, like Thailand, present less extreme ranks. You can understand the original filter() statement now: filter(min_rank(desc(lifeExp)) &lt; 2 | min_rank(lifeExp) &lt; 2) These two sets of ranks are formed on-the-fly, within year group, and filter() retains rows with rank less than 2, which means … the row with rank = 1. Since we do for ascending and descending ranks, we get both the min and the max. If we had wanted just the min OR the max, an alternative approach using top_n() would have worked. my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% arrange(year) %&gt;% group_by(year) %&gt;% #top_n(1, wt = lifeExp) ## gets the max top_n(1, wt = desc(lifeExp)) ## gets the min ## # A tibble: 12 x 3 ## # Groups: year [12] ## year country lifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Afghanistan 28.8 ## 2 1957 Afghanistan 30.3 ## 3 1962 Afghanistan 32.0 ## 4 1967 Afghanistan 34.0 ## 5 1972 Afghanistan 36.1 ## 6 1977 Cambodia 31.2 ## 7 1982 Afghanistan 39.9 ## 8 1987 Afghanistan 40.8 ## 9 1992 Afghanistan 41.7 ## 10 1997 Afghanistan 41.8 ## 11 2002 Afghanistan 42.1 ## 12 2007 Afghanistan 43.8 6.6 Introduction to visualization (OPTIONAL) Although we will get into more serious plotting in future chapters, I want to give you a taste of the excitement to come. Here, we will get sampling of the almighty ggplot2 package. Let’s look at a few basic examples. If you want to compare continuous data with a few categories, either a bar plot or box plot would be a good bet. Let’s look at the 1952 gapminder data. dat.1952 &lt;- my_gap %&gt;% filter(year == 1952) ggplot(data = dat.1952, aes(x=continent, y=lifeExp)) + geom_dotplot(binaxis = &quot;y&quot;, stackdir = &quot;center&quot;, dotsize = 0.5) + geom_boxplot(alpha=0.3) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. Look at this figure, what would you comment on the mean and variance of the data? Have you identified any outliers? Now suppose we had no idea about what our data looks like, but we want to check the relationship between 2 continuous variables. A great place to start would be a scatter plot: ggplot(data = dat.1952, aes(x = gdpPercap, y = lifeExp)) + geom_point() The scatter plot shows an upwards relationship—we will quantify this correlation in a future chapter. To make gdpPercap look more like a straight line, we can plot it in a base 10 log scale using the function scale_x_log10(). While we’re at it, let’s also add colours to label different continents. ggplot(data = dat.1952, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color = continent)) + scale_x_log10() We can also remove the grey background by setting the theme: ggplot(data = dat.1952, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color = continent)) + scale_x_log10() + theme_classic() Don’t worry too much about figures right now. We will cover data visualization in much more depth in future lessons. 6.7 Comprehensive practice So let’s answer a “simple” question: which country experienced the sharpest 5-year drop in life expectancy (le)? Recall that this excerpt of the gapminder data only has data every five years, e.g. for 1952, 1957, etc. So this really means looking at life expectancy changes between adjacent timepoints. At this point, the question is just too easy to answer, so find life expectancy by continent while we’re at it. my_gap %&gt;% select(country, year, continent, lifeExp) %&gt;% group_by(continent, country) %&gt;% # within country, take (lifeExp in year i) - (lifeExp in year i - 1) # positive means lifeExp went up, negative means it went down mutate(le_delta = lifeExp - lag(lifeExp)) %&gt;% # within country, retain the worst lifeExp change = smallest or most negative summarize(worst_le_delta = min(le_delta, na.rm = TRUE)) %&gt;% # within continent, retain the row with the lowest worst_le_delta top_n(-1, wt = worst_le_delta) %&gt;% arrange(worst_le_delta) ## `summarise()` has grouped output by &#39;continent&#39;. You can override using the `.groups` argument. ## # A tibble: 5 x 3 ## # Groups: continent [5] ## continent country worst_le_delta ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Africa Rwanda -20.4 ## 2 Asia Cambodia -9.10 ## 3 Americas El Salvador -1.51 ## 4 Europe Montenegro -1.46 ## 5 Oceania Australia 0.170 Now this data is interesting. Take a look at the life expectancy in Rwanda in 1987 and in 1992. gapminder %&gt;% select(country, year, lifeExp) %&gt;% filter(year == 1987 | year == 1992, country == &#39;Rwanda&#39;) ## # A tibble: 2 x 3 ## country year lifeExp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Rwanda 1987 44.0 ## 2 Rwanda 1992 23.6 Ponder the real-life implications of this output for a while. What you’re seeing here is genocide in dry statistics on average life expectancy. 6.8 Data wrangling summary Wow, we covered a lot of data wrangling! Don’t wory if you don’t understand everything the first time around. Programming takes practice, and practice makes perfect. Here are some general remarks: Break your code into pieces starting at the top, and inspect the intermediate results. That’s certainly how I was able to write such a thing. The commands presented in this lab do not leap fully formed out of anyone’s forehead—they are built up gradually, with lots of errors and refinements along the way. If your statements are difficult to read, by all means break it into pieces and make some intermediate objects. Your code should be easy to read and write when you’re done. The functions presented here should cover most of your basic data wrangling needs. If you ever need to do something more complicated, search it up! Although I have programmed for many years, I still need to do a quick Google search for documentation and StackOverflow solutions. "],["central-limit-theorem.html", "7 Central limit theorem 7.1 CLT for means 7.2 CLT for proportions", " 7 Central limit theorem In this chapter, we will investigate the intuition behind the Central Limit Theorem (CLT). In short, the CLT states that if you have a bunch of samples and plotted the distribution of means (not individual observations!), the distribution would look normal regardless of the population distribution. This theorem is incredibly powerful, and we will explore how it applies to data analysis in future chapters. 7.1 CLT for means 7.1.1 CLT Part 1 As always, let’s load the tidyverse. library(tidyverse) Many observed quantities follow normal distribution. Imagine we have a population following normal distribution that has a mean of 10 and standard deviation of 2. If we draw samples from it, are we able to estimate its mean? In this example, we will use rnorm(), a random number generator for this simulation. Here, rnorm() returns a vector of random numbers from a normal distribution. (samp10 &lt;- rnorm(n=10, mean=10, sd=2)) ## [1] 9.775265 14.021729 7.811684 10.518009 9.083140 9.196666 8.725705 ## [8] 9.583818 11.288335 8.763757 mean(samp10) ## [1] 9.876811 Apparently the mean is not 10, but it is pretty close. This should make sense; when we draw a sample to estimate the mean, we may get very close to the desired “true mean,” but we also expect some error. What if I repeat the estimation 10000 times with 10 samples? means10 &lt;- as.vector(NA) # doing experiments 10,000 times for (i in 1:10000) { # each iteration, I draw a sample size of 2 from a normal distribution samp10 &lt;- rnorm(10, mean=10, sd=2) # I calculate the mean of these 10 numbers and record it means10[i] &lt;- mean(samp10) } # let&#39;s look at the result ggplot(as_tibble(means10), aes(value)) + geom_density(fill=&quot;#C5EBCB&quot;) + theme_classic() What does this figure mean? Of the 10000 estimations that we did, most estimations were very close to 10. The probability to overestimate and underestimate decreases as the estimation deviates from 10, our “true mean.” We can now conclude the following: if our population is normal, our sampling distribution is also normal. This first observation demonstrates a key part of the CLT. 7.1.2 CLT Part 2 Now, we are going to explore another aspect of the CLT: given enough sample means, our sample distribution will look normal regardless of the original population distribution. This time, let’s try sampling from a population that is uniformly distributed. For example, let’s create a population with 10,000 completely random numbers between 0 and 20: uniform_popn &lt;- runif(n=10000, min=0, max=20) ggplot(as_tibble(uniform_popn), aes(value)) + geom_density(fill=&quot;#C5EBCB&quot;) + theme_classic() Now, we are going to repeatedly sample from uniform_popn and use the mean of that particular sample as our entry. means10 &lt;- as.vector(NA) means100 &lt;- as.vector(NA) means1000 &lt;- as.vector(NA) # doing experiments 10,000 times for (i in 1:10000) { # each iteration, I draw 10, 100, and 1000 samples from uniform_popn samp10 &lt;- sample(uniform_popn, size = 10, replace = TRUE) samp100 &lt;- sample(uniform_popn, size = 100, replace = TRUE) samp1000 &lt;- sample(uniform_popn, size = 1000, replace = TRUE) # getting means of each sample means10[i] &lt;- mean(samp10) means100[i] &lt;- mean(samp100) means1000[i] &lt;- mean(samp1000) } df &lt;- rbind( data.frame(means = means10, sample_size = &quot;10&quot;), data.frame(means = means100, sample_size = &quot;100&quot;), data.frame(means = means1000, sample_size = &quot;1000&quot;) ) ggplot(df, aes(x = means, fill = sample_size)) + geom_density(alpha = 0.3) + labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, fill = &quot;Sample size&quot;) + theme_classic() Notice two things: With a larger sample size, your estimation for the mean will have a smaller variance. Even though our original population was uniform (i.e., NOT normal), our sampling distribution looks normal. In fact, any population distribution will look normal given enough sample means. This observation is captured by the CLT. Pretty cool, right? 7.2 CLT for proportions Now I would like to estimate the probability of getting a “head” of when I flip a coin. Each time I flip a coin, if I end up with a “head,” I record it as a 1. If I get a “tail,” I will record it as a 0. If I flip the coin for 10 times where I have 6 “heads” and 4 “tails,” the proportion of getting “heads” would be 6/10 = 0.6. If I flip the coin for a sufficiently large amount of times, we would like to expect the proportion to approach the theoretical 0.5. Is this the case? means100 &lt;- as.vector(NA) means1000 &lt;- as.vector(NA) means10000 &lt;- as.vector(NA) for (i in 1:10000) { sample100 &lt;- sample(c(1,0), prob = c(0.5, 0.5), replace = TRUE, size = 100) means100[i] &lt;- mean(sample100) sample1000 &lt;- sample(c(1,0), prob = c(0.5, 0.5), replace = TRUE, size = 1000) means1000[i] &lt;- mean(sample1000) sample10000 &lt;- sample(c(1,0), prob = c(0.5, 0.5), replace = TRUE, size = 10000) means10000[i] &lt;- mean(sample10000) } df &lt;- rbind( data.frame(means = means100, sample_size = &quot;100 flips&quot;), data.frame(means = means1000, sample_size = &quot;1000 flips&quot;), data.frame(means = means10000, sample_size = &quot;10000 flips&quot;) ) ggplot(df, aes(x = means, fill = sample_size)) + geom_density(alpha = 0.3) + labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, fill = &quot;Sample size&quot;) + theme_classic() Can you explain the pattern that we observe with 100 flips? How about the height and width of other curves? What conclusions can we draw? The answers to these questions may appear intuitive: when you flip the coin many times, you will likely get “heads” 50% of the time. The more you flip the coin, the more likely you’ll get 50% heads. How about an “uneven” coin that preferably lands with a “head” with 75% chance? In this case we change the prob in the sample() function with this new probability. means100 &lt;- as.vector(NA) means1000 &lt;- as.vector(NA) means10000 &lt;- as.vector(NA) for (i in 1:10000) { sample100 &lt;- sample(c(1,0), prob = c(0.75, 0.25), replace = TRUE, size = 100) means100[i] &lt;- mean(sample100) sample1000 &lt;- sample(c(1,0), prob = c(0.75, 0.25), replace = TRUE, size = 1000) means1000[i] &lt;- mean(sample1000) sample10000 &lt;- sample(c(1,0), prob = c(0.75, 0.25), replace = TRUE, size = 10000) means10000[i] &lt;- mean(sample10000) } df &lt;- rbind( data.frame(means = means100, sample_size = &quot;100 flips&quot;), data.frame(means = means1000, sample_size = &quot;1000 flips&quot;), data.frame(means = means10000, sample_size = &quot;10000 flips&quot;) ) library(ggplot2) ggplot(df, aes(x = means, fill = sample_size)) + geom_density(alpha = 0.3) + labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, fill = &quot;Sample size&quot;) + theme_classic() Indeed, the peaks converged again and shifted to the new position of 0.75. As you can see, the bigger your sample size, the less variability there is, and the more the distribution looks like a normal distribution. More precisely, the bigger your sample size, the distribution of the sample means will be normally distributed, even if the population is not normally distributed. A good rule of “sufficiently large sample size” is n ≥ 30. This example shows the power of the CLT—it allows us to predict a sampling distribution regardless of the original population. NOTE: The CLT says NOTHING about the individual sample points themselves. Remember our original data data points are either 0 or 1. However, the mean is a continuous variable. "],["basic-statistical-tests.html", "8 Basic statistical tests 8.1 Getting ready 8.2 Student’s t-test 8.3 Chi-squared test 8.4 Visualizing data distributions (OPTIONAL)", " 8 Basic statistical tests R contains extremely powerful tools for data science. These tools are either built-in or available from packages. Thoughout this section we hope to demonstrate best practices organizing, analyzing, and visualizing data in R. 8.1 Getting ready We will again work with the gapminder dataset. Let’s load the usual packages. library(gapminder) library(tidyverse) Now that we’ve loaded our packages, let’s briefly re-explore gapminder. When you get a new dataset, your first action as a good data scientist should be to explore it. str(gapminder) ## tibble [1,704 × 6] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... ## $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... ## $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... ## $ gdpPercap: num [1:1704] 779 821 853 836 740 ... names(gapminder) ## [1] &quot;country&quot; &quot;continent&quot; &quot;year&quot; &quot;lifeExp&quot; &quot;pop&quot; &quot;gdpPercap&quot; head(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. Note that we have 1,704 observations (rows). The variables country and continent are considered as “factor,” which is a catagorical data. “factor” is useful in that you can deal with a finite number of discrete values. We can use levels() to ask what catagories there are. # there are 142 countries, but for the sake of space, we&#39;re only checking the first 5 head(levels(gapminder$country)) ## [1] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;Angola&quot; &quot;Argentina&quot; ## [6] &quot;Australia&quot; levels(gapminder$continent) ## [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; 8.2 Student’s t-test Let’s start by asking the mean life expectancy of the continents in 1952. gapminder %&gt;% filter(year == 1952) %&gt;% group_by(continent) %&gt;% summarise(mean(lifeExp)) ## # A tibble: 5 x 2 ## continent `mean(lifeExp)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 39.1 ## 2 Americas 53.3 ## 3 Asia 46.3 ## 4 Europe 64.4 ## 5 Oceania 69.3 The life expectancy of Europe is about 64.4 years. It seems close to 65, the standard age often associated with retirement in Canada (and when full pension benefits become available!). Is this statistically significantly different from 65 years? To answer this question, we can use a one-sample t-test. We will test the sample (life expectancy measured in Europe in 1952) against our null hypothesis that there is no significant difference between the life expectancy in Europe (64.4 years) and 65 years. # selection method 1: base R method1 &lt;- gapminder$lifeExp[gapminder$continent==&#39;Europe&#39; &amp; gapminder$year==1952] # selection method 2: use dplyr method2 &lt;- gapminder %&gt;% filter(continent == &#39;Europe&#39;, year == 1952) %&gt;% select(lifeExp) # checking if these two methods give identical outputs identical(method1, method2$lifeExp) ## [1] TRUE CAUTION: method2 is a data frame whereas method1 is a vector. To use method2 with the t.test() function, you need to specify the variable inside the data frame (e.g., method2$lifeExp). # let&#39;s rename the variable for interpretability Euro.life.1952 &lt;- method1 t.test(Euro.life.1952, mu = 65, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: Euro.life.1952 ## t = -0.50931, df = 29, p-value = 0.6144 ## alternative hypothesis: true mean is not equal to 65 ## 95 percent confidence interval: ## 62.03323 66.78377 ## sample estimates: ## mean of x ## 64.4085 Notice that p-value is 0.6144. Usually, we choose the alpha (\\(\\alpha\\)) to be 0.05. Since p &lt; \\(\\alpha\\), we conclude that the life expectancy of Europeans in 1952 doesn’t give us evidence indicating a difference in life expectancy from 65. We can also plot this: ggplot() + geom_density(aes(Euro.life.1952)) + geom_vline(xintercept = 65) Note 1: If you used the dplyr method, you would need to plot the specific vector in the data frame: ggplot() + geom_density(aes(Euro.life.1952$lifeExp)) + geom_vline(xintercept = 65) Note 2: This is NOT a figure you would include in an academic paper as the quality is quite low. We’re visualizing this just so we have a better idea of what’s going on with the data. The non-parametric test alternative to one-sample t-test is Wilcoxon signed-rank test. wilcox.test(Euro.life.1952, mu=65) ## ## Wilcoxon signed rank exact test ## ## data: Euro.life.1952 ## V = 238, p-value = 0.9193 ## alternative hypothesis: true location is not equal to 65 The non-parametric test gave us the same conclusion. CAUTION: although we obtained the same results with both the parametric t-test and non-parametric signed-rank test, their use cases are VERY different. We prefer to use parametric tests because they give us more statistical power. Only use non-parametric tests with sample sizes less than 30 and if the data is not normally distributed. Does Asia and Africa differ in life expectancy in 1952? To compare two groups of data, we need a two-sample t-test. As.Af &lt;- gapminder %&gt;% filter(year==1952) %&gt;% filter(continent %in% c(&quot;Africa&quot;, &quot;Asia&quot;)) As.Af ## # A tibble: 85 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Algeria Africa 1952 43.1 9279525 2449. ## 3 Angola Africa 1952 30.0 4232095 3521. ## 4 Bahrain Asia 1952 50.9 120447 9867. ## 5 Bangladesh Asia 1952 37.5 46886859 684. ## 6 Benin Africa 1952 38.2 1738315 1063. ## 7 Botswana Africa 1952 47.6 442308 851. ## 8 Burkina Faso Africa 1952 32.0 4469979 543. ## 9 Burundi Africa 1952 39.0 2445618 339. ## 10 Cambodia Asia 1952 39.4 4693836 368. ## # … with 75 more rows We can plot this: # you don&#39;t need to explicitly declare data = As.Af and aes(x=continent, y=lifeExp) # just make sure your variables are in the correct order ggplot(As.Af, aes(continent, lifeExp)) + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, dotsize=0.65) + geom_boxplot(alpha=0.3) + labs(x=&#39;Continent&#39;, y=&#39;Life expectancy (yrs)&#39;) + theme_classic() ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. Here we want to run a two-sample t-test. Before we do that, we’ll need to check if the 2 samples have the same variance. Recall that different t-tests assume different variances: If you assume equal variance, you would use Student’s t-test. If variances are unequal, use Welch’s t-test. A good rule of thumb is if the larger standard deviation (SD) divded by the smaller SD is less than 2 (SD(larger)/SD(smaller) &lt; 2), then you can assume equal variance. Alternatively, you can test for equal variances: library(car) # for Levene&#39;s test leveneTest(y = As.Af$lifeExp, group = As.Af$continent) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 12.514 0.0006644 *** ## 83 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since \\(p = 0.0006644 &lt; 0.05\\), the two samples have significantly different variances. Indeed, the width of the boxplots in the figure above suggested this difference. Because we have different variances, we need to use Welch’s t-test. By default, t.test() assumes unequal variance. If this wasn’t the case, we would add an additional argument called var.equal = FALSE to t.test(). t.test(lifeExp ~ continent, As.Af, alternative = &quot;two.sided&quot;) ## ## Welch Two Sample t-test ## ## data: lifeExp by continent ## t = -4.0599, df = 44.637, p-value = 0.0001952 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.741084 -3.616704 ## sample estimates: ## mean in group Africa mean in group Asia ## 39.13550 46.31439 The non-parametric test in this case would be the independent 2-group Mann-Whitney U Test. wilcox.test(lifeExp ~ continent, As.Af) ## ## Wilcoxon rank sum test with continuity correction ## ## data: lifeExp by continent ## W = 443, p-value = 0.0001857 ## alternative hypothesis: true location shift is not equal to 0 Next, let’s take a look at life expectancy in 2007: gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarise(meanlife = mean(lifeExp)) ## # A tibble: 5 x 2 ## continent meanlife ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 54.8 ## 2 Americas 73.6 ## 3 Asia 70.7 ## 4 Europe 77.6 ## 5 Oceania 80.7 Has the life expectancy in Africa changed to that in 1952? We can answer this question with a two-sample t-test. This time, we would like to match the countries. First, let’s generate a long data frame. Africa &lt;- gapminder %&gt;% filter(continent==&quot;Africa&quot;) %&gt;% select(country, year, lifeExp) head(Africa) ## # A tibble: 6 x 3 ## country year lifeExp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Algeria 1952 43.1 ## 2 Algeria 1957 45.7 ## 3 Algeria 1962 48.3 ## 4 Algeria 1967 51.4 ## 5 Algeria 1972 54.5 ## 6 Algeria 1977 58.0 Second, let’s look at how the life expectancy changed over the years. p &lt;- ggplot(data = Africa, aes(x = year, y = lifeExp)) + geom_point(aes(color = country)) + geom_line(aes(group = country, color=country)) show(p) Since the life expectancy in 1952 and 2007 look interesting, let’s visualize it: # selecting rows with years 1952 and 2007 Africa.1952.2007 &lt;- Africa %&gt;% filter(year %in% c(1952, 2007)) # plotting p &lt;- ggplot(data = Africa.1952.2007, aes(x=as.factor(year), y=lifeExp)) + geom_point(aes(color=country)) + geom_line(aes(group = country, color=country)) show(p) Most of the countries have improved, while a few have decreased life expectancy. Before testing this observation, we should reorganize our data into a nice (wide) shape. Africa.wide &lt;- spread(Africa, year, lifeExp) Africa.wide ## # A tibble: 52 x 13 ## country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Algeria 43.1 45.7 48.3 51.4 54.5 58.0 61.4 65.8 67.7 69.2 ## 2 Angola 30.0 32.0 34 36.0 37.9 39.5 39.9 39.9 40.6 41.0 ## 3 Benin 38.2 40.4 42.6 44.9 47.0 49.2 50.9 52.3 53.9 54.8 ## 4 Botswa… 47.6 49.6 51.5 53.3 56.0 59.3 61.5 63.6 62.7 52.6 ## 5 Burkin… 32.0 34.9 37.8 40.7 43.6 46.1 48.1 49.6 50.3 50.3 ## 6 Burundi 39.0 40.5 42.0 43.5 44.1 45.9 47.5 48.2 44.7 45.3 ## 7 Camero… 38.5 40.4 42.6 44.8 47.0 49.4 53.0 55.0 54.3 52.2 ## 8 Centra… 35.5 37.5 39.5 41.5 43.5 46.8 48.3 50.5 49.4 46.1 ## 9 Chad 38.1 39.9 41.7 43.6 45.6 47.4 49.5 51.1 51.7 51.6 ## 10 Comoros 40.7 42.5 44.5 46.5 48.9 50.9 52.9 54.9 57.9 60.7 ## # … with 42 more rows, and 2 more variables: 2002 &lt;dbl&gt;, 2007 &lt;dbl&gt; The wide data frame is to align the data from the same country to the same row, so that they have the same index when we call different columns. Now, we can run a paired t-test. Think about what we are testing in the code below. t.test(Africa.wide$&#39;2007&#39;, Africa.wide$&#39;1952&#39;, alternative=&quot;two.sided&quot;, paired=T) ## ## Paired t-test ## ## data: Africa.wide$&quot;2007&quot; and Africa.wide$&quot;1952&quot; ## t = 13.042, df = 51, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 13.25841 18.08267 ## sample estimates: ## mean of the differences ## 15.67054 Note in this process we didn’t check the variance. Is this a problem? Why or why not? Recall that the paired t-test is actually a one-sample t-test on paired differences. Similarly, we could again call wilcox.test to run the paired version. wilcox.test(Africa.wide$&#39;2007&#39;, Africa.wide$&#39;1952&#39;, paired=T) ## ## Wilcoxon signed rank test with continuity correction ## ## data: Africa.wide$&quot;2007&quot; and Africa.wide$&quot;1952&quot; ## V = 1369, p-value = 6.087e-10 ## alternative hypothesis: true location shift is not equal to 0 8.3 Chi-squared test 8.3.1 \\(\\chi^2\\) test for goodness-of-fit This section requires basic of knowledge of Mendelian genetics regarding dominant and recessive alleles. Recall that crossing two heterozygotes (Aa x Aa) produces offspring with dominant and recessive phenotypes with an expected ratio of 3:1. \\[\\begin{array}{c|cc} &amp; \\mathbf{A} &amp; \\mathbf{a} \\\\ \\hline \\mathbf{A} &amp; AA &amp; Aa \\\\ \\mathbf{a} &amp; Aa &amp; aa \\end{array}\\] Also recall that a dihybrid cross (AaBb x AaBb) produces offspring of 4 phenotypes with an expected ratio of 9:3:3:1. \\[\\begin{array}{c|cccc} &amp; \\mathbf{AB} &amp; \\mathbf{Ab} &amp; \\mathbf{aB} &amp; \\mathbf{ab} \\\\ \\hline \\mathbf{AB} &amp; AABB &amp; AABb &amp; AaBB &amp; AaBb \\\\ \\mathbf{Ab} &amp; AABb &amp; AAbb &amp; AaBb &amp; Aabb \\\\ \\mathbf{aB} &amp; AaBb &amp; AaBb &amp; aaBB &amp; aaBb \\\\ \\mathbf{ab} &amp; AaBb &amp; Aabb &amp; aaBb &amp; aabb \\end{array}\\] Now, let’s focus on Mendel’s data from his original paper: Mendel, Gregor. 1866. Versuche über Plflanzenhybriden. Verhandlungen des naturforschenden Vereines in Brünn, Bd. IV für das Jahr 1865, Abhandlungen, 3–47. In his experiment for seed color, the F2 generation produced 6022 yellow, and 2001 green seeds. Thus, the ratio of yellow:green was 3.01:1. Obviously, this ratio is not the exact theoretical ratio of 3:1. A meaningful question would be this: Is the discrepancy appeared because of random fluctuation, or is the observed ratio significantly different from 3:1? To examine whether the observed count fits a theoretical ratio, we will uses the \\(\\chi^2\\) test for goodness-of-fit. chisq.test(x = c(6022, 2001), # the observed data p = c(0.75, 0.25)) # the theoretical probability ## ## Chi-squared test for given probabilities ## ## data: c(6022, 2001) ## X-squared = 0.014999, df = 1, p-value = 0.9025 A p-value of 0.9025 suggested a good match of observed data with the theoretical values. That is, the differences are not significant. Let’s assume Mendel had observed a 1000 times larger number of seeds, with the same proportion. That is, 6,022,000 yellow and 2,001,000 green. Obviously this ratio is still 3.01:1. Would it still be a good fit for the theoretical value? chisq.test(x = c(6022000, 2001000), # The observed data, 1000 times larger p = c(0.75, 0.25)) # The theoretical probability ## ## Chi-squared test for given probabilities ## ## data: c(6022000, 2001000) ## X-squared = 14.999, df = 1, p-value = 0.0001076 This time, p = 0.0001076, suggesting a significant deviation from the theoretical ratio. As an extension of CLT, when you sample a large enough sample, the ratio of the categories should approach the true value. In other words, \\(\\chi^2\\) test should be increasingly sensitive to small deviations when the sample size increases. 8.3.2 \\(\\chi^2\\) test for independence In another experiment, Mendel looked at two pairs of phenotypes of the F2 generation of a double-heterozygote. Below is what he saw: 315 round and yellow, 101 wrinkled and yellow, 108 round and green, 32 wrinkled and green. Before we examine the 9:3:3:1 ratio, we want to ask if the two loci are independent of each other. That is, will being yellow increase or decrease the chance of being round (and vice versa)? To run the \\(\\chi^2\\) test for independence, we will first need a contingency table. This time we will manually build a data frame for this purpose. Mendel2loci &lt;- data.frame( yellow = c(315, 101), green = c(108, 32) ) # adding rownames rownames(Mendel2loci) &lt;- c(&quot;round&quot;, &quot;wrinkled&quot;) # printing the data frame Mendel2loci ## yellow green ## round 315 108 ## wrinkled 101 32 Next we will run the \\(\\chi^2\\) test. The null hypothesis is that the distribution is independent of the groups. chisq.test(Mendel2loci) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: Mendel2loci ## X-squared = 0.051332, df = 1, p-value = 0.8208 The p-value of 0.8208, so we cannot reject the null hypothesis. Therefore, we should consider the two traits as independent. Again, we could try to test for its goodness-of-fit. This time we will not need a contingency table. chisq.test(x = c(315, 101, 108, 32), p = c(9/16, 3/16, 3/16, 1/16)) ## ## Chi-squared test for given probabilities ## ## data: c(315, 101, 108, 32) ## X-squared = 0.47002, df = 3, p-value = 0.9254 Thus, the data fits the 9:3:3:1 ratio well. 8.3.3 \\(\\chi^2\\) test for homogeneity The homogeneity test works the same way as an independence test – the only difference lies in the experimentally design. A test for independence draws samples from the same population, and look at two or more categorical variables. A test for homogeneity draws sample from 2 or more subgroups of the population, and looks at another categorical variable. The subgroup itself serves as a variable. Recall the hypotheses for the test for homogeneity: H\\(_0\\): the distribution of a categorical response variable is the same in each subgroup. H\\(_a\\): the distribution is not the same in each subgroup. Let’s work through a real-life example. div.blue { background-color:#e6f0ff; border-radius: 10px; padding: 20px; } Remdesivir and COVID-19 Remdesivir is an antiviral drug previously tested in animal models infected with coronaviruses like SARS and MERS. As of May 2020, remdesivir had temporary approval from the FDA for use in severely ill COVID-19 patients, and it was the subject of numerous ongoing studies. A randomized controlled trial conducted in China enrolled 236 patients with severe COVID-19 symptoms; 158 were assigned to receive remdesivir and 78 to receive a placebo. In the remdesivir group, 103 patients showed clinical improvement; in the placebo group, 45 patients showed clinical improvement. A placebo is a “fake” treatment. That is, placebos do not contain any active substances that affect health. Reference Wang, Y., Zhang, D., Du, G., Du, R., Zhao, J., Jin, Y., … Wang, C. (2020). Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial. The Lancet. https://doi.org/10.1016/S0140-6736(20)31022-9 If we consider the treatment and the placebo group as two subgroups of the population, we would expect the ratios of clinical improvement to be different. Let’s do a \\(\\chi^2\\) test for homogeneity. We will start with a contingency table. rem_cont &lt;- data.frame(treatment = c(103, 158-103), placebo = c(45, 78-45)) rownames(rem_cont) &lt;- c(&quot;improvement&quot;, &quot;no improvement&quot;) rem_cont ## treatment placebo ## improvement 103 45 ## no improvement 55 33 Next we will run the test. Before we run the test, answer the following questions: What is our null hypothesis? What is our alternative hypothesis? chisq.test(rem_cont) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: rem_cont ## X-squared = 0.95518, df = 1, p-value = 0.3284 What does this result mean? 8.3.4 Fisher’s exact test (OPTIONAL) If the count in any cell of our contigency table is less than 5, the \\(\\chi^2\\) test will not be useful because of its probability distribution assumption. In this case, we will use Fisher’s exact test. The hypotheses of Fisher’s exact same as that of the \\(\\chi^2\\) test. Fisher’s exact test can be used for either homogeneity or independence, depending on your experimental design. Suppose we have a sample 10 times smaller for the Remdesivir trial: small_rem &lt;- data.frame(treatment = c(10, 16-10), placebo = c(4, 8-4)) rownames(rem_cont) &lt;- c(&quot;improvement&quot;, &quot;no improvement&quot;) small_rem ## treatment placebo ## 1 10 4 ## 2 6 4 We have many cells with &lt;5 observations. In this case let’s run Fisher’s exact test. fisher.test(small_rem) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: small_rem ## p-value = 0.6734 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.2140763 12.7643113 ## sample estimates: ## odds ratio ## 1.630755 The p-value is 0.6734. What does this result mean? The odds ratio is yet another useful measurement you will often see in medical science articles. For the sake of time, I will leave it to you if you wish to read up on it. 8.3.5 Comparison of proportions (OPTIONAL) In the Remdesivir study, the participants were randomly assigned to each group. Thus, the groups can be treated as independent. It is also reasonable to assume independence of patients within each group. Suppose we have two proportions \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\). Then, the normal model can be applied to the difference of the two proportions, \\(\\hat{p}_1 - \\hat{p}_2\\), if the following assumptions are fulfilled: The sampling distribution for each sample proportion is nearly normal. The samples are independent random samples from the relevant populations and are independent of each other. Each sample proportion approximately follows a normal model when \\(n_1p_1\\), \\(n_1(1 - p_1)\\), \\(n_2p_2\\), and \\(n_2(1-p_2)\\) are all are \\(\\geq 10\\). To check success-failure in the context of a confidence interval, use \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\). The standard error of the difference in sample proportions is \\[\\sqrt{\\dfrac{p_1(1-p_1)}{n_1} + \\dfrac{p_2(1-p_2)}{n_2}}. \\] For hypothesis testing, an estimate of \\(p\\) is used to compute the standard error of \\(\\hat{p}_1 - \\hat{p}_2\\): \\(\\hat{p}\\), the weighted average of the sample proportions \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\), \\[\\hat{p} = \\dfrac{n_1\\hat{p}_1 + n_2\\hat{p}_2}{n_1 + n_2} = \\dfrac{x_1 + x_2}{n_1 + n_2}. \\] To check success-failure in the context of hypothesis testing, check that \\(\\hat{p}n_1\\) and \\(\\hat{p}n_2\\) are both \\(\\geq 10\\). In this case, let’s calculate the The pooled proportion \\(\\hat{p}\\): \\[\\hat{p} = \\dfrac{x_1 + x_2}{n_1 + n_2} = 0.627\\] x = c(103, 45) n = c(158, 78) p.hat.vector = x/n p.hat.vector ## [1] 0.6518987 0.5769231 #use r as a calculator p.hat.pooled = sum(x)/sum(n) p.hat.pooled ## [1] 0.6271186 Next we will check the success-failure condition, which is, \\(\\hat{p}n_1\\) and \\(\\hat{p}n_2\\) are both \\(\\geq 10\\). #check success-failure n*p.hat.pooled ## [1] 99.08475 48.91525 n*(1 - p.hat.pooled) ## [1] 58.91525 29.08475 The success-failure condition is met; the expected number of successes and failures are all larger than 10. #conduct inference prop.test(x = x, n = n) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: x out of n ## X-squared = 0.95518, df = 1, p-value = 0.3284 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.06703113 0.21698245 ## sample estimates: ## prop 1 prop 2 ## 0.6518987 0.5769231 In this example, we tested \\(H_0: p_1 = p_2\\) against \\(H_a: p_1 \\neq p_2\\) Here, \\(p_1\\) represents the population proportion of clinical improvement in COVID-19 patients treated with remdesivir, and \\(p_2\\) represents the population proportion of clinical improvement in COVID-19 patients treated with a placebo. By convention, \\(\\alpha = 0.05\\). The \\(p\\)-value is 0.3284, which is greater than \\(\\alpha\\). We conclude that there is insufficient evidence to reject the null hypothesis. Although the proportion of patients who experienced clinical improvement about 7% higher in the remdesivir group, this difference is not big enough to show that remdesivir is more effective than a placebo. 8.3.6 Contingency tables Let’s come back to the data of Asia and Africa in 1952. Take a look at the distribution of the life expectancy for all countries in both continents. summary(As.Af) ## country continent year lifeExp ## Afghanistan: 1 Africa :52 Min. :1952 Min. :28.80 ## Algeria : 1 Americas: 0 1st Qu.:1952 1st Qu.:37.00 ## Angola : 1 Asia :33 Median :1952 Median :40.54 ## Bahrain : 1 Europe : 0 Mean :1952 Mean :41.92 ## Bangladesh : 1 Oceania : 0 3rd Qu.:1952 3rd Qu.:45.01 ## Benin : 1 Max. :1952 Max. :65.39 ## (Other) :79 ## pop gdpPercap ## Min. : 60011 Min. : 298.9 ## 1st Qu.: 1022556 1st Qu.: 684.2 ## Median : 3379468 Median : 1077.3 ## Mean : 19211739 Mean : 2783.3 ## 3rd Qu.: 8550362 3rd Qu.: 1828.2 ## Max. :556263527 Max. :108382.4 ## Notice that the median of life expectancy is 40.54. That is, half of the countries had life expectancy greater than 40.54, and the other half less than 40.54. Let’s define a catagorical variable: the countries with life expectancy &gt; 40.54 years are “longer_lived,” and the others are “shorter_lived.” As.Af[&quot;long_short&quot;] &lt;- NA As.Af$long_short[As.Af$lifeExp &gt; 40.54] &lt;- &quot;longer_lived&quot; As.Af$long_short[is.na(As.Af$long_short)] &lt;- &quot;shorter_lived&quot; Now let’s see if the longer lived or shorter lived variable is independent of the continent variable. We realize that both variables are categorical. In this case, we will use chi-squared test. First, we will make a contingency table of the two variables. As.Af &lt;- droplevels(As.Af) cont &lt;- table(As.Af$continent, As.Af$long_short) cont ## ## longer_lived shorter_lived ## Africa 21 31 ## Asia 22 11 Here, our null hypothesis is that countries are independent of the continent it’s a part of. Likewise, our alternative hypothesis is that countries are dependent (not independent) of the continent it’s a part of. chisq.test(cont) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: cont ## X-squared = 4.5769, df = 1, p-value = 0.03241 Given that \\(p &lt; 0.05\\), our null hypothesis that the two variables are independent is rejected. Whether a country is longer lived or shorter lived is dependent on the continent it is located in. N.B., this is only a comparison between Africa and Asia, and does not hold true for all continents. 8.4 Visualizing data distributions (OPTIONAL) Knowing the properties of the normal distribution is essential in understanding the normal distribution. The position of the peak indicates the mean, whereas the spread of the curve indicates the variance. Although you might not think your data follows a bell curve, let’s take a look at this example for our exercise. Let’s first install a package that helps us create ridgeline plots. install.packages(&quot;ggridges&quot;) Here we will plot the distribution of the life expectancy of African countries in different years. For each year, distributions are sectioned into quartiles. What could you say about the trend over the years? Please discuss both the mean and variance. What does it mean? library(ggridges) # getting all rows with Africa as the continent Africa.all &lt;- gapminder %&gt;% filter(continent == &quot;Africa&quot;, year &gt; 1990) # plotting p &lt;- ggplot(Africa.all, aes(lifeExp, as.factor(year), fill=factor(stat(quantile)))) + stat_density_ridges(quantiles=4, quantile_lines=T, geom = &#39;density_ridges_gradient&#39;) + scale_fill_viridis_d(name=&#39;Quartile&#39;) + labs(x=&#39;Life expectancy (yrs)&#39;, y=&#39;Year&#39;) + theme_classic() show(p) ## Picking joint bandwidth of 3.6 Let’s take a look at the mean and standard deviation to see if your guess is correct. Africa.all %&gt;% select(c(year, lifeExp)) %&gt;% group_by(as.factor(year)) %&gt;% summarize(mean_life = mean(lifeExp), sd_life = sd(lifeExp)) ## # A tibble: 4 x 3 ## `as.factor(year)` mean_life sd_life ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1992 53.6 9.46 ## 2 1997 53.6 9.10 ## 3 2002 53.3 9.59 ## 4 2007 54.8 9.63 This visualization shows the same information as that in with the density plots, but in a more digestible manner. p &lt;- ggplot(data = Africa.all, aes(x=as.factor(year), y=lifeExp)) + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, binwidth = 1) + geom_boxplot(alpha=0.3) show(p) You may want to remove the gray background and decrease dot size. This is as easy as specifying the dotsize parameter and adding theme_classic(). There a lot more themes out there! Check them out here. p &lt;- ggplot(data = Africa.all, aes(x=as.factor(year), y=lifeExp)) + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, binwidth = 1, dotsize = 0.65) + geom_boxplot(alpha=0.3) + theme_classic() show(p) While we’re at it, let’s also rename the x- and y-axis. Since we’ve saved the plot already, let’s add a label layer to the plot. p &lt;- p + labs(x = &#39;Year&#39;, y = &#39;Life expectancy (yrs)&#39;) show(p) Now this figure is publication-ready. "],["comparing-multiple-means.html", "9 Comparing multiple means 9.1 Loading packages 9.2 Merging datasets 9.3 One-way ANOVA 9.4 Linear regression", " 9 Comparing multiple means Throughout this lab, we will provide a pipeline to help you wrangle data, perform statistical analyses, and (perhaps most importantly) visualize data in R. Here, we will learn how to compare the means using parametric tests and medians using non-parametric tests of multiple groups. 9.1 Loading packages Let’s load the usual packages. library(gapminder) library(car) # car stands for Companion to Applied Regression library(tidyverse) 9.2 Merging datasets In this section, we will learn how to merge datasets. We will use something called democracy index (democracy score) and convert it into categorical data. As the name suggests, democracy index measures the degree of democracy of a country on a scale from 0 to 10, with higher scores being correlated with greater democracy. In our dataset, however, the scale is from -10 to 10. This data set has been pre-cleaned and made available on gapminder. Alternatively, download the file by clicking here. Let’s load our dataset. democracy.raw &lt;- read.csv(file = &quot;data/07_multi-compare/democracy_score_use_as_color.csv&quot;, header = TRUE) The first thing you should do with new data is explore it. Since the output is quite large, we’ll only show the first row, but you should definitely take a deeper look. head(democracy.raw, n=1) ## country X1800 X1801 X1802 X1803 X1804 X1805 X1806 X1807 X1808 X1809 X1810 ## 1 Afghanistan -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1811 X1812 X1813 X1814 X1815 X1816 X1817 X1818 X1819 X1820 X1821 X1822 X1823 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1824 X1825 X1826 X1827 X1828 X1829 X1830 X1831 X1832 X1833 X1834 X1835 X1836 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1837 X1838 X1839 X1840 X1841 X1842 X1843 X1844 X1845 X1846 X1847 X1848 X1849 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1850 X1851 X1852 X1853 X1854 X1855 X1856 X1857 X1858 X1859 X1860 X1861 X1862 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1863 X1864 X1865 X1866 X1867 X1868 X1869 X1870 X1871 X1872 X1873 X1874 X1875 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1876 X1877 X1878 X1879 X1880 X1881 X1882 X1883 X1884 X1885 X1886 X1887 X1888 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1889 X1890 X1891 X1892 X1893 X1894 X1895 X1896 X1897 X1898 X1899 X1900 X1901 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1902 X1903 X1904 X1905 X1906 X1907 X1908 X1909 X1910 X1911 X1912 X1913 X1914 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1915 X1916 X1917 X1918 X1919 X1920 X1921 X1922 X1923 X1924 X1925 X1926 X1927 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1928 X1929 X1930 X1931 X1932 X1933 X1934 X1935 X1936 X1937 X1938 X1939 X1940 ## 1 -6 -6 -6 -6 -6 -6 -6 -8 -8 -8 -8 -8 -8 ## X1941 X1942 X1943 X1944 X1945 X1946 X1947 X1948 X1949 X1950 X1951 X1952 X1953 ## 1 -8 -8 -8 -8 -10 -10 -10 -10 -10 -10 -10 -10 -10 ## X1954 X1955 X1956 X1957 X1958 X1959 X1960 X1961 X1962 X1963 X1964 X1965 X1966 ## 1 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -7 -7 -7 ## X1967 X1968 X1969 X1970 X1971 X1972 X1973 X1974 X1975 X1976 X1977 X1978 X1979 ## 1 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 0 -10 ## X1980 X1981 X1982 X1983 X1984 X1985 X1986 X1987 X1988 X1989 X1990 X1991 X1992 ## 1 -10 -10 -10 -10 -10 -10 -10 -10 -10 -8 -8 -8 0 ## X1993 X1994 X1995 X1996 X1997 X1998 X1999 X2000 X2001 X2002 X2003 X2004 X2005 ## 1 0 0 0 -7 -7 -7 -7 -7 NA NA NA NA NA ## X2006 X2007 X2008 X2009 X2010 X2011 ## 1 NA NA NA NA NA NA Don’t forget about str() and summary()! str(democracy.raw) summary(democracy.raw) As you can see, there is a lot of missing data (denoted by NA). NA values are often problematic for analyses, so we would like to either remove them or impute (estimate) them. In our case, let’s get rid of the rws with missing data for the year 2007 (the X2007 column). dem07 &lt;- democracy.raw %&gt;% select(country, X2007) %&gt;% # choose filter(!is.na(X2007)) # selecting all non-NA rows Here, is.na() will return TRUE for missing data. Recall that ! is the NOT logical operator (i.e., !TRUE is equivalent to FALSE and vice versa. It follows that !is.na() returns true for non-empty data. Let’s take a looks at how the democracy score is distributed. Here, I’d like to treat each democracy score as a factor. ggplot(dem07, aes(as.factor(X2007))) + geom_bar() Before we do anything, let’s look at some potentially interesting counts. First, we’ll look at two ways to count “low-level” countries. To do so, we will arbitrarily define any democracy score \\(\\leq\\) -3 as low-level. Now, we will count the number of countries in each group. nrow(dem07[dem07$X2007 &lt;= -3,]) # base R ## [1] 39 dem07 %&gt;% filter(X2007 &lt;= -3) %&gt;% nrow() # with dplyr pipe operator ## [1] 39 Let’s define medium-level countries as having a democracy score betwen -2 and 5 inclusive. nrow(dem07[dem07$X2007 &gt;= -2 &amp; dem07$X2007 &lt;= 5,]) # base R ## [1] 32 dem07 %&gt;% filter(X2007 &gt;= -2 &amp; X2007 &lt;= 5) %&gt;% nrow() # with dplyr pipe operator ## [1] 32 Exercise: count the number of high-level countries using both base R and dplyr. High-level countries will be defined as those with democracy score greater or equal to 6. Now let’s actually assign a new categorical variable to each country (row) using the cut() function. Let’s call the new row demLev (our shorthand for democracy level). tempDemLev &lt;- cut(dem07$X2007, c(-Inf, -2.5, 4.5, Inf), c(&quot;LowDem&quot;, &quot;MidDem&quot;, &quot;HighDem&quot;)) dem07$demLev &lt;- tempDemLev # base R method # dem07 &lt;- dem07 %&gt;% mutate(demLev = tempDemLev) # dplyr method head(dem07) ## country X2007 demLev ## 1 Albania 9 HighDem ## 2 Algeria 2 MidDem ## 3 Angola -2 MidDem ## 4 Argentina 8 HighDem ## 5 Armenia 5 HighDem ## 6 Australia 10 HighDem Note: The first argument for cut() takes a vector, the second takes the vector for cutoff thresholds, and the third are names of the bins defined by the cutoffs. We can now merge this new data with gapminder. The main idea of merging is to add the new variables as columns. The identifier of our observations will be country. Since we are taking data from different sources, a given country might exist in one data frame but not the other. Furthermore, the two data sets might use different names for the countries. Before merging, let’s check the data we want to merge. Note that str_detect() finds all instances where a particular string is in a column. # let&#39;s check how they name Korea dem07 %&gt;% filter(str_detect(country, &#39;Korea&#39;)) ## country X2007 demLev ## 1 North Korea -10 LowDem ## 2 South Korea 8 HighDem gapminder %&gt;% filter(str_detect(country, &#39;Korea&#39;)) ## # A tibble: 24 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Korea, Dem. Rep. Asia 1952 50.1 8865488 1088. ## 2 Korea, Dem. Rep. Asia 1957 54.1 9411381 1571. ## 3 Korea, Dem. Rep. Asia 1962 56.7 10917494 1622. ## 4 Korea, Dem. Rep. Asia 1967 59.9 12617009 2144. ## 5 Korea, Dem. Rep. Asia 1972 64.0 14781241 3702. ## 6 Korea, Dem. Rep. Asia 1977 67.2 16325320 4106. ## 7 Korea, Dem. Rep. Asia 1982 69.1 17647518 4107. ## 8 Korea, Dem. Rep. Asia 1987 70.6 19067554 4106. ## 9 Korea, Dem. Rep. Asia 1992 70.0 20711375 3726. ## 10 Korea, Dem. Rep. Asia 1997 67.7 21585105 1691. ## # … with 14 more rows Now that we have a clear idea of which each looks like, we need to determine the potential differences between them. For example, you can’t merge 'Korea, Dem. Rep.' with 'Korea' since the two strings are not exactly equal. # setdiff() finds the differences between values in each dataset # unique() ensures that there are no duplicate values setdiff(unique(dem07$country), unique(gapminder$country)) ## [1] &quot;Armenia&quot; &quot;Azerbaijan&quot; &quot;Belarus&quot; ## [4] &quot;Bhutan&quot; &quot;Cape Verde&quot; &quot;Cyprus&quot; ## [7] &quot;Estonia&quot; &quot;Fiji&quot; &quot;Georgia&quot; ## [10] &quot;Guyana&quot; &quot;Kazakhstan&quot; &quot;Kyrgyz Republic&quot; ## [13] &quot;Lao&quot; &quot;Latvia&quot; &quot;Lithuania&quot; ## [16] &quot;Moldova&quot; &quot;North Korea&quot; &quot;North Macedonia&quot; ## [19] &quot;Papua New Guinea&quot; &quot;Qatar&quot; &quot;Russia&quot; ## [22] &quot;Solomon Islands&quot; &quot;South Korea&quot; &quot;Suriname&quot; ## [25] &quot;Tajikistan&quot; &quot;Timor-Leste&quot; &quot;Turkmenistan&quot; ## [28] &quot;Ukraine&quot; &quot;United Arab Emirates&quot; &quot;Uzbekistan&quot; ## [31] &quot;Yemen&quot; setdiff(unique(gapminder$country), unique(dem07$country)) ## [1] &quot;Afghanistan&quot; &quot;Bosnia and Herzegovina&quot; &quot;Hong Kong, China&quot; ## [4] &quot;Iceland&quot; &quot;Korea, Dem. Rep.&quot; &quot;Korea, Rep.&quot; ## [7] &quot;Puerto Rico&quot; &quot;Reunion&quot; &quot;Sao Tome and Principe&quot; ## [10] &quot;Taiwan&quot; &quot;West Bank and Gaza&quot; &quot;Yemen, Rep.&quot; It looks like we need to change “South Korea” to “Korea, Rep.” and “Yemen” to “Yemen, Rep.” We can do this using the factor recode function: fct_recode() dem07 &lt;- dem07 %&gt;% mutate(country = fct_recode(country, &#39;Korea, Rep.&#39; = &#39;South Korea&#39;, &#39;Yemen, Rep.&#39; = &#39;Yemen&#39;)) Finally, let’s can merge the two data frames using a left join. There are many types of joins (right join, inner join, etc.), and you can check them out here. # need to filter out missing data! my_gap &lt;- gapminder %&gt;% left_join(dem07, by = &quot;country&quot;) %&gt;% filter(!is.na(demLev)) %&gt;% filter(!is.na(lifeExp)) # let&#39;s see what the data looks like now head(my_gap) ## # A tibble: 6 x 8 ## country continent year lifeExp pop gdpPercap X2007 demLev ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; ## 1 Albania Europe 1952 55.2 1282697 1601. 9 HighDem ## 2 Albania Europe 1957 59.3 1476505 1942. 9 HighDem ## 3 Albania Europe 1962 64.8 1728137 2313. 9 HighDem ## 4 Albania Europe 1967 66.2 1984060 2760. 9 HighDem ## 5 Albania Europe 1972 67.7 2263554 3313. 9 HighDem ## 6 Albania Europe 1977 68.9 2509048 3533. 9 HighDem Please note that demLev was based on the score of 2007. We don’t do that here, but you could also include the levels based on the scores from different years. 9.3 One-way ANOVA 9.3.1 The iris dataset The iris dataset contains information about three species of flowers: setosa, veriscolor, and virginia. Iris is a built-in dataset, meaning we can call it without reading it in. iris$Species refers to one column in iris. That is, the column with the name of the species (setosa, versicolor, or virginica). We can see how many rows and columns are in a data.frame with the dim command. dim(iris) prints out the number of rows (150) and the number of columns (5): head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Analysis of Variance (ANOVA) allows us to test whether there are differences in the mean between multiple samples. The question we will address is: Are there differences in average sepal width among the three species? To run an ANOVA, we need to check if The variance is is equal for each group, and The data distributes normally within each group. Let’s address the first point. leveneTest(Sepal.Width ~ Species, data = iris) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.5902 0.5555 ## 147 A p-value of 0.5555 suggested that the variances are not significantly different. This means we should proceed with a parametric test like ANOVA (otherwise, use the Kruskal-Wallis test). Keep in mind we haven’t yet checked the normality. We will do it after running ANOVA. We start by building an analysis of variance model with the aov() function: In this case, we pass two arguments to the aov() function: For the formula parameter, we pass Sepal.Width ~ Species. This format is used throughout R for describing relationships we are testing. The format is y ~ x, where the response variables (e.g. y) are to the left of the tilde (~) and the predictor variables (e.g. x) are to the right of the tilde. In this example, we are asking if petal length is significantly different among the three species. We also need to tell R where to find the Sepal.Width and Species data, so we pass the variable name of the iris data.frame to the data parameter. But we want to store the model, not just print it to the screen, so we use the assignment operator &lt;- to store the product of the aov function in a variable of our choice Sepal.Width.aov &lt;- aov(formula = Sepal.Width ~ Species, data = iris) Notice how when we execute this command, nothing printed in the console. This is because we instead sent the output of the aov call to a variable. If you just type the variable name, you will see the familiar output from the aov function: Sepal.Width.aov ## Call: ## aov(formula = Sepal.Width ~ Species, data = iris) ## ## Terms: ## Species Residuals ## Sum of Squares 11.34493 16.96200 ## Deg. of Freedom 2 147 ## ## Residual standard error: 0.3396877 ## Estimated effects may be unbalanced To see the results of the ANOVA, we call the summary() function: summary(object = Sepal.Width.aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 11.35 5.672 49.16 &lt;2e-16 *** ## Residuals 147 16.96 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The species do have significantly different sepal width (P &lt; 0.001). However, ANOVA does not tell us which species are different. We can run a post hoc test to assess how the species are different. A Tukey test comparing means would be one option. We will do the Tukey test after determining normality. Now, let’s take a look at the normality. First, we will plot the diagnostic figures. plot(Sepal.Width.aov) Most importantly, the dots in Q-Q plot (upper right) should align with the line pretty well. This figure is acceptable. If the dots deviate from the line too much, the data would not be considered normal. If you still perform the ANOVA, you should view your results critically (or ignore them, at worst). Please do not include such diagnostic figures in the main text of your manuscripts. This might qualify for a supplementary figure at most. Although we’ve also examined residuals with the QQ plot, we can also use a formal test: residuals_Sepal_Width &lt;- residuals(object = Sepal.Width.aov) shapiro.test(x = residuals_Sepal_Width) ## ## Shapiro-Wilk normality test ## ## data: residuals_Sepal_Width ## W = 0.98948, p-value = 0.323 A p-value of 0.323 suggested that the assumption of normality is reasonable. Recall that a residual is an “error” in result. More specifically, a residual is the difference of a given data point from the mean (\\(r = x - \\mu\\)). So far, we have demonstrated Normality in distribution. Homogeneity variance, and These two justified our choice for one-way ANOVA. The result of ANOVA also indicated that at least one species of the 3 has significantly different sepal width from others. Which one? To do this, we need to run “Post-Hoc” test. Let’s do Tukey Honest Significant Differences (HSD). The nice thing is that TukeyHSD() can directly take the result of ANOVA as the argument. TukeyHSD(Sepal.Width.aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Sepal.Width ~ Species, data = iris) ## ## $Species ## diff lwr upr p adj ## versicolor-setosa -0.658 -0.81885528 -0.4971447 0.0000000 ## virginica-setosa -0.454 -0.61485528 -0.2931447 0.0000000 ## virginica-versicolor 0.204 0.04314472 0.3648553 0.0087802 The difference between every pair are significant (\\(p &lt; 0.05\\)). 9.3.2 Non-parametric alternatives to ANOVA In reality, your data usually wouldn’t be as perfect as above. In case of a non-normal sample, there are two ways to address the problem: Apply appropriate data transformations techniques, or Use a non-parametric test I highly recommend you to explore the tricks of data transformation. If you can rescue it back to normal distribution, parametric tests usually can allow you to do more powerful analysis. If you have exhausted your attempts to data transformation, you may then use non-parametric tests. A note for Kruskal-Wallis H-test. When your data doesn’t satisfy the normality or equal variance assumption, ANOVA does not strictly apply. However, one-way ANOVA is not very sensitive to deviations from normality. Kruskal-Wallis doesn’t assume normality, but it does assume same distribution (equal variance). If your data do not meet either assumption, you would want to use Welch’s One-way Test. Now, let’s get back to gapminder data. Let’s add another categorical variable calle Income_Level. This time we will split by the quartiles. dat.1952 &lt;- my_gap %&gt;% filter(year == 1952) border_1952 &lt;- quantile(dat.1952$gdpPercap, c(.25, .50, .75)) dat.1952$Income_Level_1952 &lt;- cut(dat.1952$gdpPercap, c(0, border_1952[1], border_1952[2], border_1952[3], Inf), c(&#39;Low&#39;, &#39;Low Middle&#39;, &#39;High Middle&#39;, &#39;High&#39;)) head(dat.1952) ## # A tibble: 6 x 9 ## country continent year lifeExp pop gdpPercap X2007 demLev Income_Level_19… ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; ## 1 Albania Europe 1952 55.2 1.28e6 1601. 9 HighD… Low Middle ## 2 Algeria Africa 1952 43.1 9.28e6 2449. 2 MidDem High Middle ## 3 Angola Africa 1952 30.0 4.23e6 3521. -2 MidDem High Middle ## 4 Argent… Americas 1952 62.5 1.79e7 5911. 8 HighD… High ## 5 Austra… Oceania 1952 69.1 8.69e6 10040. 10 HighD… High ## 6 Austria Europe 1952 66.8 6.93e6 6137. 10 HighD… High dat.2007 &lt;- my_gap %&gt;% filter(year == 2007) border_2007 &lt;- quantile(dat.2007$gdpPercap, c(.25, .50, .75)) dat.2007$Income_Level_2007 &lt;- cut(dat.2007$gdpPercap, c(0, border_2007[1], border_2007[2], border_2007[3], Inf), c(&#39;Low&#39;, &#39;Low Middle&#39;, &#39;High Middle&#39;, &#39;High&#39;)) head(dat.2007) ## # A tibble: 6 x 9 ## country continent year lifeExp pop gdpPercap X2007 demLev Income_Level_20… ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; ## 1 Albania Europe 2007 76.4 3.60e6 5937. 9 HighD… Low Middle ## 2 Algeria Africa 2007 72.3 3.33e7 6223. 2 MidDem High Middle ## 3 Angola Africa 2007 42.7 1.24e7 4797. -2 MidDem Low Middle ## 4 Argent… Americas 2007 75.3 4.03e7 12779. 8 HighD… High Middle ## 5 Austra… Oceania 2007 81.2 2.04e7 34435. 10 HighD… High ## 6 Austria Europe 2007 79.8 8.20e6 36126. 10 HighD… High For now, let’s focus on the data of in 1952. ggplot(data = dat.1952, aes(x = Income_Level_1952, y = lifeExp)) + geom_boxplot() + theme_classic() We can also visualize life expectancy for each democracy level: ggplot(data = dat.1952, aes(x = demLev, y = lifeExp)) + geom_boxplot() + theme_classic() Let’s check the variance. leveneTest(lifeExp ~ Income_Level_1952, data = dat.1952) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 4.2319 0.006881 ** ## 128 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A p-value of 0.0047 suggested that the variances are significantly different. Therefore, we shoud not run ANOVA or Kruskal-Wallis. Let’s run Welch’s one-way test. result &lt;- oneway.test(lifeExp ~ Income_Level_1952, data = dat.1952) result ## ## One-way analysis of means (not assuming equal variances) ## ## data: lifeExp and Income_Level_1952 ## F = 63.15, num df = 3.000, denom df = 68.584, p-value &lt; 2.2e-16 A p-value of 2.2e-16 suggested that at least one category of Income_Level_1952 had values of lifeExp that are significantly different from others. Let’s run a Post-Hoc test to find out. Since we are running a non-parametric test, the appropriate test would be Games-Howell post-hoc test. Unfortunately, R does not have a built-in function for Games-Howell. Let’s define a function to do this task. Note: you don’t need to know how the code below works. games.howell &lt;- function(grp, obs) { #Create combinations combs &lt;- combn(unique(grp), 2) # Statistics that will be used throughout the calculations: # n = sample size of each group # groups = number of groups in data # Mean = means of each group sample # std = variance of each group sample n &lt;- tapply(obs, grp, length) groups &lt;- length(tapply(obs, grp, length)) Mean &lt;- tapply(obs, grp, mean) std &lt;- tapply(obs, grp, var) statistics &lt;- lapply(1:ncol(combs), function(x) { mean.diff &lt;- Mean[combs[2,x]] - Mean[combs[1,x]] # t-values t &lt;- abs(Mean[combs[1,x]] - Mean[combs[2,x]]) / sqrt((std[combs[1,x]] / n[combs[1,x]]) + (std[combs[2,x]] / n[combs[2,x]])) # Degrees of Freedom df &lt;- (std[combs[1,x]] / n[combs[1,x]] + std[combs[2,x]] / n[combs[2,x]])^2 / # numerator dof ((std[combs[1,x]] / n[combs[1,x]])^2 / (n[combs[1,x]] - 1) + # Part 1 of denominator dof (std[combs[2,x]] / n[combs[2,x]])^2 / (n[combs[2,x]] - 1)) # Part 2 of denominator dof # p-values p &lt;- ptukey(t * sqrt(2), groups, df, lower.tail = FALSE) # sigma standard error se &lt;- sqrt(0.5 * (std[combs[1,x]] / n[combs[1,x]] + std[combs[2,x]] / n[combs[2,x]])) # Upper Confidence Limit upper.conf &lt;- lapply(1:ncol(combs), function(x) { mean.diff + qtukey(p = 0.95, nmeans = groups, df = df) * se })[[1]] # Lower Confidence Limit lower.conf &lt;- lapply(1:ncol(combs), function(x) { mean.diff - qtukey(p = 0.95, nmeans = groups, df = df) * se })[[1]] # Group Combinations grp.comb &lt;- paste(combs[1,x], &#39;:&#39;, combs[2,x]) # Collect all statistics into list stats &lt;- list(grp.comb, mean.diff, se, t, df, p, upper.conf, lower.conf) }) # Unlist statistics collected earlier stats.unlisted &lt;- lapply(statistics, function(x) { unlist(x) }) # Create dataframe from flattened list results &lt;- data.frame(matrix(unlist(stats.unlisted), nrow = length(stats.unlisted), byrow=TRUE)) # Select columns set as factors that should be numeric and change with as.numeric results[c(2, 3:ncol(results))] &lt;- round(as.numeric(as.matrix(results[c(2, 3:ncol(results))])), digits = 3) # Rename data frame columns colnames(results) &lt;- c(&#39;groups&#39;, &#39;Mean Difference&#39;, &#39;Standard Error&#39;, &#39;t&#39;, &#39;df&#39;, &#39;p&#39;, &#39;upper limit&#39;, &#39;lower limit&#39;) return(results) } After defining the function, we can use it. If you decide to use the Games-Howell function, you can simply copy-and-paste it. Since this function is open-source code, citation is not necessary. games.howell(grp = dat.1952$Income_Level_1952, # Groups, the categorical variable obs = dat.1952$lifeExp) # Observations, the continuous variable ## groups Mean Difference Standard Error t df p ## 1 Low Middle : High Middle 8.807 1.444 4.311 58.295 0.00 ## 2 Low Middle : High 20.169 1.440 9.905 58.438 0.00 ## 3 Low Middle : Low -4.207 1.043 2.852 58.143 0.03 ## 4 High Middle : High 11.362 1.651 4.866 63.999 0.00 ## 5 High Middle : Low -13.014 1.319 6.974 48.168 0.00 ## 6 High : Low -24.376 1.315 13.113 48.302 0.00 ## upper limit lower limit ## 1 14.210 3.404 ## 2 25.555 14.784 ## 3 -0.305 -8.109 ## 4 17.522 5.203 ## 5 -8.048 -17.980 ## 6 -19.430 -29.323 9.3.3 Two-way ANOVA We can also look at 2 independent categorical variables together with a two-way ANOVA. This is as easy as calling aov() with an additional variable on the right side of the y ~ x formula. For example, we can take a look at both Income_Level_2007 and demLevel as explanatory variables to the response variable lifeExp. two_way_plus &lt;- aov(lifeExp ~ Income_Level_2007 + demLev, data = dat.2007) two_way_star &lt;- aov(lifeExp ~ Income_Level_2007 * demLev, data = dat.2007) You might have noticed that I used + to connect the 2 explanatory variables in the first line and * for the second line. How are they different? Take a look at the results below. two_way_plus ## Call: ## aov(formula = lifeExp ~ Income_Level_2007 + demLev, data = dat.2007) ## ## Terms: ## Income_Level_2007 demLev Residuals ## Sum of Squares 12199.101 437.708 6446.792 ## Deg. of Freedom 3 2 126 ## ## Residual standard error: 7.152973 ## Estimated effects may be unbalanced two_way_star ## Call: ## aov(formula = lifeExp ~ Income_Level_2007 * demLev, data = dat.2007) ## ## Terms: ## Income_Level_2007 demLev Income_Level_2007:demLev Residuals ## Sum of Squares 12199.101 437.708 662.338 5784.454 ## Deg. of Freedom 3 2 6 120 ## ## Residual standard error: 6.942895 ## Estimated effects may be unbalanced summary(two_way_plus) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Income_Level_2007 3 12199 4066 79.476 &lt;2e-16 *** ## demLev 2 438 219 4.277 0.0159 * ## Residuals 126 6447 51 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(two_way_star) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Income_Level_2007 3 12199 4066 84.36 &lt;2e-16 *** ## demLev 2 438 219 4.54 0.0126 * ## Income_Level_2007:demLev 6 662 110 2.29 0.0396 * ## Residuals 120 5784 48 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the test with *, there is one more term Income_Level_2007:demLev. This is the interaction between the two variables. In this case, the interaction of of the two variables also contribute significantly to the difference in lifeExp. For the purposes of the Youreka program, it doesn’t matter which method you use. 9.4 Linear regression 9.4.1 Basic concepts We have discussed extensively for the scenario where you have a continuous variable and a categorical variable. Now we will talk about what you do if both variables are continuous. For this final section, we will test for a relationship between life expectancy and per capita gross domestic product (GDP). As we did for the ANOVA analyses, it is usually a good idea to visually inspect the data when possible. Here we can use the plot function to create a scatterplot of the two columns of interest, lifeExp and gdpPercap. ggplot(data = dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() We can see immediately that this is unlikely a linear relationship. In this case, we will need to log-transform the GDP data to obtain a linear relationship. ggplot(data = dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10() Now that the data are properly transformed, we can create the linear model for the predictability of life expectancy based on gross domestic product. Before we do that let’s make it clear: From the scatter plot we can identify a positive relationship – when log(GDP per capita) increases, the life expectancy also tends to be higher. The tendency of one variable going up or down linearly with the increase of another variable is called “correlation.” The more consistent the points are with a LINEAR trend, the higher the closer the correlation is to -1 (for negative relationships) or +1 (for positive relationships). How fast one variable increases or decreases with the increase of another variable can be described by the slope of the fitting line. To estimate the slope, we need a linear model. We can only discuss strength of correlation with these linear regression, but NOT the causation. That is, correlation does NOT imply causation. We can plot the linear model easily: ggplot(data = dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10() + geom_smooth(method = &quot;lm&quot;) # lm = linear model ## `geom_smooth()` using formula &#39;y ~ x&#39; To get rid of the confidence band around the line, pass se = FALSE into geom_smooth(). ggplot(data = dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10() + geom_smooth(method = &quot;lm&quot;, se = FALSE) # lm = linear model ## `geom_smooth()` using formula &#39;y ~ x&#39; You can also customize the colour and thickness of the line. As always, use the ? operator to get the full documentation. 9.4.2 Pearson correlation Let’s look at the correlation. For normal distributed data, we calculate the Pearson correlation for the log-transformed variable. dat.2007$log_GDP &lt;- log(dat.2007$gdpPercap) # add new variable cor.test(x = dat.2007$log_GDP, y = dat.2007$lifeExp, method = &quot;pearson&quot;) # method options: pearson, kendall, spearman ## ## Pearson&#39;s product-moment correlation ## ## data: dat.2007$log_GDP and dat.2007$lifeExp ## t = 15.626, df = 130, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7389427 0.8599823 ## sample estimates: ## cor ## 0.8078163 The p-value suggests the correlation is significant. The correlation coefficient of 0.8 suggests a positive correation (y increases as x increases). In case you see a negative value, the correlation if negative (one variable going up while the other going down). Next we can construct a linear model. # Run a linear model lifeExp.v.gdp &lt;- lm(formula = lifeExp ~ log_GDP, data = dat.2007) # Investigate results of the model summary(lifeExp.v.gdp) ## ## Call: ## lm(formula = lifeExp ~ log_GDP, data = dat.2007) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.692 -2.711 1.441 4.652 13.362 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.7951 4.0045 1.197 0.233 ## log_GDP 7.1909 0.4602 15.626 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.142 on 130 degrees of freedom ## Multiple R-squared: 0.6526, Adjusted R-squared: 0.6499 ## F-statistic: 244.2 on 1 and 130 DF, p-value: &lt; 2.2e-16 The linear equation is: \\(\\text{lifeExp} = (7.1909 \\pm 0.4602) \\times \\text{log_GDP} + (4.7951 \\pm 4.0045)\\). Also notice that the correlation coefficient is \\(R^2 = 0.6526 \\Rightarrow R = \\sqrt{0.6526} = 0.8078\\), the same value returned by cor.test(). For our question, the relationship between life expectancy and GDP, focus on the coefficients section, specifically the line for log_GDP. First of all, there is a significant relationship between these two variables (p &lt; 2 x 10-16, or, as R reports in the Pr&gt;(|t|) column, p &lt; 2e-16). The Estimate column of the results lists a value of lifeExp.v.gdp$coefficients['log_GDP']. For every 10-fold increase in per capita GDP (remember we log10-transformed GDP), life expectancy increases by almost 7 years. The linear model relies assumes that your data is normally distributed. We can generate a diagnostic plot in the same way as one-way ANOVA. plot(lifeExp.v.gdp) Q-Q plot suggested this data deviates from normality. Let’s also take a look at the residues of the linear model: residuals_lm &lt;- residuals(object = lifeExp.v.gdp) shapiro.test(x = residuals_lm) ## ## Shapiro-Wilk normality test ## ## data: residuals_lm ## W = 0.89588, p-value = 3.911e-08 Indeed, Shapiro test also suggests the data deviates from normality. In this case, we should use the Spearman (or Kendall) correlation. 9.4.3 Spearman correlation If your variables are not normally distributed, you can use the non-parametric Spearman correlation as alternative. Instead of Pearson’s R, the Spearman test outputs rho (\\(\\rho\\)), cor.test(dat.2007$lifeExp, dat.2007$log_GDP, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: dat.2007$lifeExp and dat.2007$log_GDP ## S = 57612, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8496971 "],["data-visualization-basics.html", "10 Data visualization basics 10.1 A graphing template 10.2 Scatter plot 10.3 Aesthetic mappings", " 10 Data visualization basics “The simple graph has brought more information to the data analyst’s mind than any other device.” – John Tukey This lab is all about preparing publication-ready figures with ggplot2 and related packages. ggplot2 uses elegant syntax and it implements “The Layered Grammar of Graphics”. Before we begin, let’s review a few key points for good figures: Be clear and avoid confusion. Presenting too much information often results in messy figures. Figures inconsistent in colour, symbols, etc. can easily confuse readers. Only use additional aesthetic effects when necessary. Everything in a graph has a purpose. The primary objective of a figure is to inform, not to look fancy (though this is a plus). When in doubt, stick to black, white, and grey. Only use texts when necessary. Make text large. Never use Comic Sans as your font. Sans serif fonts such as Arial and Calibri are usually good bets. Key point: the plot depends on the variables. Some plots are more appropriate for visualization than others. You have an obligation to display data responsibly. Check out The R Graph Gallery for a “dictionary” on different visualizations and the code to create them. Recall that the tidyverse contains ggplot2 and dplyr among other packages. We’ll also load gapminder. library(gapminder) library(tidyverse) Hopefully by now you have a good idea of what the gapminder dataset looks like. Here’s a quick refresher: head(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 10.1 A graphing template Although we’ve presented graphs with ggplot2 in previous labs, let’s delve into the specifics of the syntax: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) Everything in the initial ggplot() function is passed into the subsequent functions (i.e., GEOM_FUNCTION()). When calling ggplot(), you don’t explicitly need to write &lt;ARGUMENT&gt; = (ex: data = gapminder, x = lifeExp) as long as you have the variables in the correct order – just be careful you don’t mix up x and y! ggplot2 works in a layer-by-layer manner. Take a look at this: ggplot(data = gapminder, aes(x = pop, y = gdpPercap)) ggplot(data = gapminder, aes(x = pop, y = gdpPercap)) + geom_point() Here, the first line initializes the object and the second line adds a layer of scatter points. Unlike plotting in base R, you don’t need to specify variables using the $ operator – ggplot2 is smart enough to call it automatically for you. I’d like to bring your attention to the + operator. This is how you add layers to the ggplot. Use + liberally to reduce excessively long lines; breaking long commands at appropriate places makes your code much more readable. 10.2 Scatter plot Use the scatter plot when you want to see the relationship between two continuous variables. ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() You also can save the object as a variable. To show the figure, use the show() function or simply call the object by its name. p &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() show(p) # method 1 p # also works This data might benefit using a log scale. You can either log-transform (log(gdpPercap)) or simply draw the x axis in log scale (scale_x_log10()). p &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() + scale_x_log10() show(p) 10.2.1 Trend lines It seems that there exists a positive correlation between the two variables – you might want to add a trend line. Remember, p is the object of scatter plot with x in log. We can just build from here. This just goes to show the beauty of layered graphical syntax. p + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; You find that R used ‘gam’ model as default. ‘gam’ is the generalized additive model. Without going into the mathematical details, you would expect a curve from gam. What if I want a straight line (i.e., linear regression)? I would want a straight line to start with. Let’s start with a linear model (lm). p + geom_smooth(method = &#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; We can also create a generalized linear model (glm). glm can be useful if your variables are not normally distributed. p + geom_smooth(method = &#39;glm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; It appears thatlm and glm don’t look too different. Notice that lm has a little shaded region around it. This correspondends to the confidence interval of 1 standard error. To get rid of it, specify se = FALSE. While we’re at it, let’s change the colour of the line to red and decrease its thickness. Let’s also remove that pesky grey background: scatter_trend &lt;- p + theme_classic() + # removes grey background geom_smooth(method = &#39;lm&#39;, se = F, # remove confidence band col = &#39;red&#39;, # change colour of line to red (hex colours also work: #FF0000) size = 0.75) # set width of line show(scatter_trend) ## `geom_smooth()` using formula &#39;y ~ x&#39; 10.2.2 Facets If you were paying careful attention, you may have noticed that we were using data from multiple years. However, this results in a messy, and potentially misleading, graph. Let’s fix this by plotting each year separately with the facet feature. ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + facet_wrap(vars(year), nrow = 3, ncol = 4) + geom_point(size = 0.5) + scale_x_log10() + theme_bw() # another ggplot2 theme We can use faceting to split by the combination of two variables. Here I will use facet_grid to put the same values of splitting variables on the same row/column. Note that you also need to specify vars(year) and vars(continent) instead of just year and continent. This is simply to help ggplot retrieve the levels in a particular column. gap.52.77.07 &lt;- gapminder %&gt;% filter(year %in% c(1952, 1977, 2007)) ggplot(data = gap.52.77.07, aes(x = gdpPercap, y = lifeExp)) + facet_grid(rows = vars(year), cols = vars(continent)) + geom_point() + scale_x_log10() + theme_bw() 10.3 Aesthetic mappings A scatter plot places dots using x and y coordinates. What if we want to show more detail, like which point(s) correspond to a particular group? For example, what if we want to see how life expectancy vs GDP per capita varies per continent in 1977? gap.77 &lt;- gapminder %&gt;% filter(year == 1977) ggplot(data = gap.77, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color=continent)) + scale_x_log10() You can see the points from some continents, like Europe and Africa, cluster at distinct positions. In addition to adding colours to show (categorical or continuous) groupings, we can also use Shape of points (categorical), Size of points (continuous), or Transparency/alpha of the points (continuous). These options are all specified within aesthetic mappings (aes()). That is, aes() is the place you specify how you present your variables. More specifically, it’s how you map your variables to various aesthetics. To repeat an earlier point, aes() within the ggplot() function applies to ALL layers, while those in other layers only applies to that specific layer. Here’s another example. Be careful with this as R interprets year as a continuous variable. Use as.factor() or factor() to circumvent this issue, ggplot(data = gap.52.77.07, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(col = as.factor(year))) + scale_x_log10() Going back to our original example of lifeExp vs gdpPercap by continent, I use stat_ellipse() to enclose the points within a 95% confidence interval. ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp)) + # color only applies to the points, not the eclipses geom_point(aes(color=continent)) + # stat_ellipse uses level=0.95 by default stat_ellipse() + scale_x_log10() We can also create multiple ellipses to group things together. Note that color = continent is the same as colour = continent and col = continent. ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp, color = continent)) + # color applies to both points and eclipses geom_point() + stat_ellipse() + scale_x_log10() ## Too few points to calculate an ellipse ## Warning: Removed 1 row(s) containing missing values (geom_path). What if we want to also visualize population size in addition to grouping by continent? ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color=continent, size=pop)) + scale_x_log10() Here we run into a minor problem: some dots are overlapping. We can fix this by applying geom_jitter() with partial transparency. ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp)) + geom_jitter(alpha=0.7, aes(color=continent, size=pop)) + scale_x_log10() Notice that geom_jitter() adds some random variation, or jitter, to each point. While this is a handy method to address overplotting, don’t rely on it too heavily. This is illustrated in the next plot: ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_jitter(aes(color=continent, size=pop, alpha=year)) + scale_x_log10() As you can see, this is a fancy figure, but also a messy figure. This plot has “information overload,” so we would like to simplify it. To reduce the amount of information, we come back to facets. Be careful with this one as R interprets year as a continuous variable. Use as.factor() or factor() to circumvent this issue, ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_jitter(alpha=0.65, aes(size=pop, color=as.factor(year))) + facet_wrap(vars(continent)) + scale_x_log10() Finally, we can change the labels and text formats. Here, we can save the figure to a .png format. Alternatively, you can save the figure using the Export tab in the Plots viewing panel in RStudio. p &lt;- ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_jitter(alpha=0.65, aes(size=pop, color=as.factor(year))) + facet_wrap(vars(continent)) + scale_x_log10() + labs(x = &quot;GDP per capita&quot;, y = &quot;Life Expectancy&quot;, size = &quot;Population&quot;, color = &quot;Year&quot;) + theme_bw() + # remove the gray background theme(text = element_text(size = 16)) # make texts larger show(p) ggsave(&quot;data/09_ggplot2/Life_GDP.png&quot;, plot = p) ## Saving 7 x 5 in image "],["getting-publication-ready.html", "11 Getting publication-ready 11.1 Line plot 11.2 Bar plot 11.3 Box plot 11.4 Histogram and density plot 11.5 Assembly of multiple figures (OPTIONAL) 11.6 Extras (OPTIONAL) 11.7 Additional resources", " 11 Getting publication-ready As the chapter name suggests, this chapter is all about getting publication-quality plots. library(tidyverse) library(gapminder) 11.1 Line plot A line plot is another way to visualize continuous variables. This is particularly useful when Your observations change over time and You want to demonstrate a causal relationship. Let’s elaborate on the second point. In the previous case for life expectancy and GDP, we could only observe a correlation, but cannot conclude a causal relationship. In some case, such as carrying out a laboratory experiment or a simulation study, you can precisely manipulate certain independent variables and measure other dependent variables. This way you can argue for a better causal relationship. For example, you can change the concentration of a drug treatment and measure the inhibition effect. Let’s take a look at the life expectancy of Africa over the years. Let’s first create a scatter plot. ggplot(data = subset(gapminder, continent == &quot;Europe&quot;), aes(x = year, y = lifeExp)) + geom_point() Since the points can correspond to the countries, we can connect them with lines. ggplot(data = subset(gapminder, continent == &quot;Europe&quot;), aes(x = year, y = lifeExp, colour = country)) + geom_point() + geom_line(aes(group = country)) Here the group = country specifies that points with the same values for the variable country should be connected in a line. Oftentimes you want to show line plots with mean values and error bars. Unfortunately, ggplot2 can’t automatically draw error bars – you have to explicitly specify the values. We’re going to address this in the next example. First, calculate the mean and SEM and save it to a new data frame. df &lt;- gapminder %&gt;% group_by(year, continent) %&gt;% summarise(mean_le = mean(lifeExp), sd=sd(lifeExp), sem = sd(lifeExp)/sqrt(n())) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` argument. Next, draw a line plot with this data frame. To draw error bars, we need to specify the upper and lower limits within geom_errorbar(). lineplot &lt;- ggplot(data = df, aes(x = year, y = mean_le, color = continent)) + geom_line() + geom_point() + geom_errorbar(aes(ymin = mean_le-sem, ymax = mean_le+sem), position = position_dodge(0.05)) # position_dodge() sets length of error bar caps show(lineplot) Finally, let’s format the figure nicely. lineplot &lt;- lineplot + labs(x = &quot;Year&quot;, y = &quot;Life Expectancy&quot;, color = &quot;Continent&quot;) + theme_classic() + # remove the gray background theme(text = element_text(size = 16)) # set font size show(lineplot) For a cleaner view with offset axis: lineplot &lt;- lineplot + theme(axis.line = element_blank(), # hide the default axes axis.ticks.length = unit(7, &#39;pt&#39;)) + # specify the breaks of y-axis scale_y_continuous(breaks=seq(40,80,20), limits=c(35,85), expand=c(0,0)) + # specify the breaks of x-axis scale_x_continuous(breaks=seq(1950, 2010, 20), limits=c(1945,2010), expand=c(0,0)) + # specify the location of the new y-axis geom_segment(y=40, yend=80, x=1945, xend=1945, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) + # specify the location of the new x-axis geom_segment(y=35, yend=35, x=1950, xend=2010, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) show(lineplot) ggsave(&quot;data/10_pubvis/life_year.png&quot;, plot = lineplot) ## Saving 7 x 5 in image 11.2 Bar plot 11.2.1 Basics Let’s look at the life expectancy of all continents in 1977 using a bar plot. Notice that instead of using dplyr, we can simply use the base R subset() function. Recall df from the previous section. ggplot(data = subset(df, year == 1977), aes(x=continent, y=mean_le)) + geom_col() Note by default, geom_col takes both x and y while geom_bar takes only x and plots the count on y. Just as before, we can add error bars: ggplot(data = subset(df, year == 1977), aes(x=continent, y=mean_le)) + geom_col() + geom_errorbar(aes(ymin = mean_le - sem, ymax = mean_le + sem), width=0.5, position=position_dodge(0.05)) We can compare 1977 and 2007 by settingfill = as.factor(year). p &lt;- ggplot(data = subset(df, year %in% c(1977, 2007)), aes(x=continent, y=mean_le, fill = as.factor(year))) + geom_col(position = position_dodge(), color = &quot;black&quot;) + geom_errorbar(aes(ymin = mean_le - sem, ymax = mean_le + sem), width=0.5, position=position_dodge(0.9)) show(p) Finally, let’s make this figure publication-ready: p &lt;- p + scale_fill_manual(values = c(&#39;black&#39;, &#39;white&#39;)) + labs(x = &#39;Continent&#39;, y=&#39;Mean life expectancy (years)&#39;, fill = &#39;Year&#39;) + theme_classic() + theme(text = element_text(size=16)) + # removes space between bottom of bars and x-axis scale_y_continuous(expand = c(0, 0)) p 11.2.2 Plotting significance (OPTIONAL) Next we can run some statistical tests and add significance stars (*) to the plot. We’ll compare if the life expectancy in Africa in 1977 and 2007 has changed. africa.1977.lifeExp &lt;- gapminder %&gt;% filter(continent == &#39;Africa&#39;, year == 1977) %&gt;% select(lifeExp) africa.2007.lifeExp &lt;- gapminder %&gt;% filter(continent == &#39;Africa&#39;, year == 2007) %&gt;% select(lifeExp) t.test(africa.1977.lifeExp, africa.2007.lifeExp) ## ## Welch Two Sample t-test ## ## data: africa.1977.lifeExp and africa.2007.lifeExp ## t = -3.195, df = 91.787, p-value = 0.001917 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -8.474086 -1.977145 ## sample estimates: ## mean of x mean of y ## 49.58042 54.80604 The p-value is 0.001917. Here are common ranges for different p-values: 0 **** 0.0001 *** 0.001 ** 0.01 * 0.05 ns 1. We should use ** in this case. To draw the significance stars, we need the ggsignif package. # install.packages(&quot;ggsignif&quot;) library(ggsignif) meanLE.77.07 &lt;- p + geom_signif(y_position = 60, xmin = 0.75, xmax = 1.25, # position of the stars annotations = &quot;**&quot;, tip_length = 0.05) meanLE.77.07 There are more ways to draw the significant stars in R. For example, ggpubr even allows you to run the tests and plot the stars in the same line. However, I do not encourage you to do so. Manually adding the stars might be a bit tedious (in terms of adjusting the positions and tip length), but you are not as restricted by the package in terms of the tests you can do. 11.3 Box plot Box plots are extremely versatile. Here are 2 reasons: You don’t need to calculate the mean and error for box plots (remember we used df for bar plots). Many high-profile journals ask authors to submit graphs that shows not only the statistical description (mean and error), but also the dots for the raw data. Box plots are well suited for this purpose. p &lt;- ggplot(data = subset(gapminder, year %in% c(1977, 2007)), aes(x = continent, y = lifeExp, color = as.factor(year))) + geom_boxplot(position = position_dodge(0.8)) show(p) Let’s add the data points. Here I use geom_jitter to avoid overlapping. Please note that position_jitterdodge introduces random noise to the x position of the points to make it easier to read. But since we are plotting against a categorical variable, the exact x position doesn’t matter. p &lt;- p + geom_point(position = position_jitterdodge(0.4), alpha = 0.65) Use stat_summary show data points and statistics together. p &lt;- ggplot(data = subset(gapminder, year %in% c(1977, 2007)), aes(x = continent, y = lifeExp, color = as.factor(year))) + geom_point(position = position_jitterdodge(0.4), alpha = 0.65) + theme_classic() + # adds error bar stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom=&quot;errorbar&quot;, width=0.3, position = position_dodge(0.7), size = 1) + # adds mean pooint to the error bar stat_summary(fun=mean, geom=&quot;point&quot;, position = position_dodge(0.7), size = 3) show(p) Add asterisks and format the plot. p &lt;- p + geom_signif(y_position = 85, xmin = 0.75, xmax = 1.25, annotations = &quot;**&quot;, tip_length = 0.02, color = &quot;black&quot;) + labs(x = &quot;Continents&quot;, y = &quot;Life Expectancy (years)&quot;, color = &quot;Year&quot;) + theme(text = element_text(size = 16)) # make text larger show(p) ## Warning: Computation failed in `stat_summary()`: ## Hmisc package required for this function We can again offset the axes. offset_box &lt;- p + theme(axis.line = element_blank(), # hide the default axes axis.ticks.length = unit(7, &#39;pt&#39;)) + # Specify the breaks of y-axis scale_y_continuous(breaks=seq(30,90,30), limits=c(25,95), expand=c(0,0)) + # Specify location of x-axis geom_segment(y=30, yend=90, x=0.4, xend=0.4, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) + # Specify location of y-axis geom_segment(y=25, yend=25, x=1, xend=5, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) show(offset_box) ## Warning: Computation failed in `stat_summary()`: ## Hmisc package required for this function Now we can save this publication-ready figure. ggsave(&#39;data/10_pubvis/life_year_jitter.png&#39;, plot = offset_box) ## Saving 7 x 5 in image ## Warning: Computation failed in `stat_summary()`: ## Hmisc package required for this function 11.4 Histogram and density plot Since these have been covered extensively in previous labs, I’m going to go through this section rather quickly. The histogram and density plot are great tools for looking at distributions of a single variable: ggplot(data = subset(gapminder, year == 2007), aes(x = lifeExp)) + geom_histogram(fill = &#39;#69B3A2&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(data = subset(gapminder, year == 2007), aes(x = lifeExp)) + geom_density(fill = &#39;#69B3A2&#39;) We can also use these plots to compare distributions. ggplot() + # 2007 data and label geom_density(data = subset(gapminder, year == 2007), aes(x = lifeExp, y = ..density..), fill = &#39;#69B3A2&#39;) + geom_label(aes(x=40, y=0.03, label=&#39;2007&#39;), colour = &#39;#69B3A2&#39;) + # 1977 data and label geom_density(data = subset(gapminder, year == 1977), aes(x = lifeExp, y = -..density..), fill = &#39;#404080&#39;) + geom_label(aes(x=40, y=-0.03, label=&#39;1977&#39;), colour = &#39;#404080&#39;) + theme_minimal() Perhaps a histogram would be better for our purposes. hist.07.77 &lt;- ggplot() + # 2007 data and label geom_histogram(data = subset(gapminder, year == 2007), aes(x = lifeExp, y = ..density..), fill = &#39;#69B3A2&#39;) + geom_label(aes(x=40, y=0.03, label=&#39;2007&#39;), colour = &#39;#69B3A2&#39;) + # 1977 data and label geom_histogram(data = subset(gapminder, year == 1977), aes(x = lifeExp, y = -..density..), fill = &#39;#404080&#39;) + geom_label(aes(x=40, y=-0.03, label=&#39;1977&#39;), colour = &#39;#404080&#39;) + theme_minimal() hist.07.77 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 11.5 Assembly of multiple figures (OPTIONAL) Many figures in academic journals include multiple subfigures within a figure. To assemble many graphs into one figure, usually we use a graphical design software such as Illustrator, Inkscape or even PowerPoint. If you are able to generate all subfigures within one single R script (so that all the ggplot2 objects are present together), you could go on to use R package patchwork to assemble them into a big figure. (Please note that this could be a very rare scenario when conducting serious research - Each subfigures may require intense computation and modelling work that are performed with several scripts. They may even come from different people - your teammates and collaborators. You don’t always have access to all te subfigures within one workspace. Most of the time you would still find yourself using graphical design softwares to assemble the figures.) # install.packages(&#39;patchwork&#39;) library(patchwork) Here is a super simple example: just add the plots together! Recall that these variables were saved throughout our lab. hist.07.77 + lineplot ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can also do something a bit more complicated: (hist.07.77 + lineplot) / offset_box ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Computation failed in `stat_summary()`: ## Hmisc package required for this function We can also add spacing between plots: patch &lt;- (hist.07.77 + plot_spacer() + lineplot) / (meanLE.77.07 + theme(text=element_text(size=12))) patch ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Most figures in papers are annotated. In most cases, plots are combined using a photoshop tool. Labels are usually also added with a photoshop tool. For whatever reason you wish to programmatically add labels, here’s howyou do it: (hist.07.77 + lineplot) / (meanLE.77.07 + theme(text=element_text(size=12))) + plot_annotation(tag_levels = &#39;A&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Note that tag_levels takes: ‘1’ for Arabic numerals, ‘A’ for uppercase Latin letters, ‘a’ for lowercase Latin letters, ‘I’ for uppercase Roman numerals, and ‘i’ for lowercase Roman numerals. More often than not, we want all of our figures to have the same dimensions. patchwork makes it easy for us to do so: aligned_plots &lt;- align_patches(lineplot, scatter_trend, meanLE.77.07, offset_box) for (p in aligned_plots) { plot(p) } That’s all there is to it! If you want more customization options, read the official documentation: https://patchwork.data-imaginist.com/articles/patchwork.html. 11.6 Extras (OPTIONAL) 11.6.1 Choropleth “The greatest value of a picture is when it forces us to notice what we never expected to see.” –John Tukey The choropleth is used to display differences in geographical regions using different colours/shades/patterns. To use map data, we need to install maps package. install.packages(&quot;maps&quot;) First, let’s load maps and retrieve all of the data from the year 2007. library(maps) dat2007 &lt;- gapminder %&gt;% filter(year == 2007) dat2007 &lt;- dat2007 %&gt;% rename(&#39;region&#39; = &#39;country&#39;) Now let’s get the world map data. This is necessary because it contains the longitude and latitudes we need to draw the map world_map &lt;- map_data(&quot;world&quot;) head(world_map) ## long lat group order region subregion ## 1 -69.89912 12.45200 1 1 Aruba &lt;NA&gt; ## 2 -69.89571 12.42300 1 2 Aruba &lt;NA&gt; ## 3 -69.94219 12.43853 1 3 Aruba &lt;NA&gt; ## 4 -70.00415 12.50049 1 4 Aruba &lt;NA&gt; ## 5 -70.06612 12.54697 1 5 Aruba &lt;NA&gt; ## 6 -70.05088 12.59707 1 6 Aruba &lt;NA&gt; Whereas gapminder names USA and UK ‘United States’ and ‘United Kingdom’ respectively, world_map names them by their abbreviations. Let’s rename the gapminder data. dat2007 &lt;- dat2007 %&gt;% mutate(region = fct_recode(region, &#39;USA&#39; = &#39;United States&#39;, &#39;UK&#39; = &#39;United Kingdom&#39;)) We’re almost there! Now, we need to merge the data. life.exp.map &lt;- left_join(world_map, dat2007, by = &quot;region&quot;) head(life.exp.map) ## long lat group order region subregion continent year lifeExp pop ## 1 -69.89912 12.45200 1 1 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 2 -69.89571 12.42300 1 2 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 3 -69.94219 12.43853 1 3 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 4 -70.00415 12.50049 1 4 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 5 -70.06612 12.54697 1 5 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 6 -70.05088 12.59707 1 6 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## gdpPercap ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 NA Finally, we can plot the data. We specify group to draw each country individually. # grey means no gapminder data ggplot(life.exp.map, aes(x=long, y=lat, group = group)) + geom_polygon(aes(fill = lifeExp), color = &quot;white&quot;) + scale_fill_viridis_b(option=&#39;D&#39;) + theme_bw() 11.6.2 Animations While we covered the most relevant data visualizations for your project, we’ve barely scratched the surface of what R can do. For example, you can create animations. Let’s install the gganimate package. install.packages(&#39;devtools&#39;) devtools::install_github(&#39;thomasp85/gganimate&#39;) Next, load the package: library(gganimate) Finally, we can create the plot! This might take a while, but I promise it will be worth the wait! ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, col = country)) + geom_point(alpha = 0.7, show.legend = F) + scale_colour_manual(values = country_colors) + scale_x_log10() + scale_size(range = c(2, 12)) + facet_wrap(~continent) + theme_bw() + theme(panel.grid = element_blank()) + # here is the animation code labs(title = &#39;Year: {frame_time}&#39;, x = &#39;GDP per capita&#39;, y = &#39;Life expectancy (years)&#39;) + transition_time(year) + ease_aes(&#39;linear&#39;) 11.7 Additional resources I highly recommend you read through these websites: From Data to Viz: https://www.data-to-viz.com Includes the visualization and the type of data it corresponds to. Patchwork: https://github.com/thomasp85/patchwork For creating multi-plot figures. Caveats: https://www.data-to-viz.com/caveats.html Pitfalls to avooid when creating figures. The Python Graph Gallery: https://python-graph-gallery.com If you’re more comfortable with Python. Includes the visualization and code to creat it. Data visualizations for various data types. Check out https://www.data-to-viz.com/ for an interactive version of this chart! "]]
