[["index.html", "A Gentle Introduction to Data Science with R Explore, analyze, and visualize data Welcome to Youreka Canada How to use this textbook Contributors Credit License", " A Gentle Introduction to Data Science with R Explore, analyze, and visualize data Eddie Guo Youreka Canada Programs Team Welcome to Youreka Canada Welcome to the Youreka Canada program! In this course, you will learn how to wrangle data, perform statistical tests, and visualize data with R. The purpose of this textbook is to provide a companion to the Youreka Canada program, which teaches introductory statistics, data science, and research methods. Here, we offer an intuitive approach to data science rather than a rigorous, proof-based course. As such, this textbook does not assume you have any prior knowledge other than basic arithmetic, and it should be accessible to both high school and undergraduate students. How to use this textbook Please note that this text goes into additional detail not covered in session. All optional material is marked as OPTIONAL in the headings. Again, note that this text complements the Youreka program. These notes are not a substitute for attending the Youreka sessions! This text focuses on how the material taught in-session can be applied using the R programming language. As with all things programming, the best way to learn is to actively code. That is, when you read this textbook, open RStudio and play around with the presented code‚Äîtry to find alternative solutions, or even break the code. Don‚Äôt be afraid to make mistakes, and soon enough, you will be confident to code on your own! Contributors This textbook was written by Eddie Guo, Pouria Torabi, Shuce Zhang, and Devin Aggarwal, who are part of the Youreka Canada Programs Team. A special thanks goes to Matthew Pietrosanu for his critical statistical review of the Youreka program. Credit This material was adapted from: Jennifer Bryan STAT 545 at UBC https://stat545.com/ Jennifer Bryan (2017). gapminder: Data from Gapminder. R package version 0.3.0. https://CRAN.R-project.org/package=gapminder License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share‚Äîcopy and redistribute the material in any medium or format Remix‚Äîremix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution‚ÄîYou must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike‚ÄîIf you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions‚ÄîYou may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["install.html", "1 Install R and RStudio 1.1 Installing R 1.2 Installing RStudio 1.3 Add-on packages", " 1 Install R and RStudio Why do we need to install both R and RStudio? Well, the answer is the following: R is the programming language whereas RStudio is the integrated development environment (IDE) for R. You can think of R as the thing that turns your code into commands that your computer runs and RStudio as a fancy text editor (although it is so much more than that!). 1.1 Installing R On your preferred web browser, navigate to the R project website to download R. On the left side bar, click on ‚ÄúCRAN‚Äù under ‚ÄúDownload‚Äù. Choose the mirror you wish to download from (e.g., https://mirror.rcg.sfu.ca/mirror/CRAN/) Download the correct version for your OS. Ensure to download the latest release of R. 1.2 Installing RStudio Go to the RStudio website. Navigate to the RStudio page and download RStudio Desktop. If you have a pre-existing installation of R and/or RStudio, we highly recommend that you update both. If you upgrade R, you‚Äôll need to update any packages you have installed. Type the following command into the Console in RStudio: update.packages(ask = FALSE, checkBuilt = TRUE) Once you‚Äôve installed and/or updated R and RStudio, open RStudio. You should get a window similar to this screenshot, but yours will be more boring because you haven‚Äôt written any code or made any figures yet! Place your cursor in the pane called ‚ÄúConsole‚Äù, which is where you interact with R. Type print('Hello World!') in the console and hit the enter or return key. You should see ‚ÄúHello World!‚Äù print to the screen. If you do, you‚Äôve succeeded in installing R and RStudio. 1.3 Add-on packages R contains a huge number of packages that enhances its functionality. People often share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN (e.g., the tidyverse), type this into the R console: install.packages(&#39;tidyverse&#39;, dependencies = TRUE) By including dependencies = TRUE, we are including any additional packages our target package requires. Please also install the followng packages using the method above: ggsignif: This package extends the ggplot2 package. It adds additional statistical visualization functions. Read more here. ggpubr: As the name suggests, ggpubr creates publication-quality graphs. Read more here. Without further ado, let‚Äôs jump into our adventure with R! "],["basics-of-r-part-1.html", "2 Basics of R (Part 1) 2.1 Objects 2.2 Function Basics 2.3 Basic math operators 2.4 Conditionals 2.5 Working directory", " 2 Basics of R (Part 1) We will begin our adventure by opening RStudio. If this is your first time opening RStudio, you should see the following panes: Console (entire left) Environment/History (upper right) Files/Plots/Packages/Help (lower right) You can change the default location of the panes, among many other things: Customizing RStudio. For now, place your cursor in the console so we can start coding with R! 2.1 Objects Here‚Äôs some basic information to get you started. R is an object-oriented programming language (OOP). This means that R creates different types of objects that we can manipulate with functions and operators. Our first operator will be the assignment operator; either &lt;- (a left arrow) or = (the equal sign). Let‚Äôs make our first assignment and inspect the object we‚Äôve just created: my_object &lt;- 5*10 my_object ## [1] 50 In plain English, the above snippet tells us that ‚Äúfive times ten is assigned to my_object‚Äù. Every value you assign to an object will be in this form: object_name &lt;- value By convention, we use &lt;- to assign variables. Don‚Äôt be lazy and use = to assign variables. Although it works, it will just sow confusion later. Instead, utilize RStudio‚Äôs keyboard shortcut: Alt+- (the minus sign). Notice that RStudio automagically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces. RStudio offers many handy keyboard shortcuts. The shortcut to rule them all is Alt+Shift+K (this brings up a keyboard shortcut reference card). Although object names are flexible, we need to follow some rules: Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. As a general rule of thumb, object names should be short and meaningful. Misleading or overly long object names will make it a pain to debug your code. Below are examples of various object name conventions. My best advice would be to pick one and stick with it. this_is_snake_case other.people.use.periods evenOthersUseCamelCase Let‚Äôs make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect the object we‚Äôve just created, try out RStudio‚Äôs auto-complete feature: type the first few characters, press TAB, add characters until you get what you want, then press return/enter. 2.2 Function Basics Functions are always followed by a pair of brackets (). R has a huge collection of built-in functions that can be accessed like this: functionName(arg1 = val1, arg2 = val2, ...) Notice that we use = instead of &lt;- within a function. Here, arg1 and arg2 are called the arguments of the function functionName(). Likewise, val1 and val2 are called the parameters of arg1 and arg2. Let‚Äôs try using seq() which makes regular sequences of numbers and, while we‚Äôre at it, demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Select seq() by using the up/down arrows to select. Notice the floating tool-tip-type help that pops up to remind you of a function‚Äôs arguments. If you want even more help, press F1 to get the full documentation in the help tab of the lower right pane. Now open the parentheses and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return/enter. RStudio also exits the parenthetical expression for you. seq(1, 10) ## [1] 1 2 3 4 5 6 7 8 9 10 The above snippet also demonstrates something about how R resolves function arguments. You can always specify name = value if you‚Äôre unsure. If you don‚Äôt, R attempts to resolve by position. In the above snippet, R assumed we wanted a sequence from = 1 that goes to = 10. Since we didn‚Äôt specify step size specified by the by argument, R uses the default value of 1 in this case. For functions I call often, I might use this resolve by position for the first argument or maybe the first two. After that, I always specify name = value. seq(from = 1, to = 10, by = 2) ## [1] 1 3 5 7 9 If you just make an assignment, you don‚Äôt see the assigned value. To show the assigned value, just call the variable. y &lt;- seq(1, 10) y ## [1] 1 2 3 4 5 6 7 8 9 10 You can shorten this common action by surrounding the assignment with parentheses. (y &lt;- seq(1, 10)) ## [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() ## [1] &quot;Sun Jan 10 12:16:08 2021&quot; Now look at your workspace ‚Äì in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() ## [1] &quot;my_object&quot; &quot;this_is_a_really_long_name&quot; ## [3] &quot;y&quot; ls() ## [1] &quot;my_object&quot; &quot;this_is_a_really_long_name&quot; ## [3] &quot;y&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) or click the broom icon in RStudio‚Äôs Environment pane. 2.3 Basic math operators Here are some basic math operations you can perform in R (no need to memorize this list!): 1 + 2 # addition 5 * 4 # multiplication 10 / 3 # division 6 ^ 2 # exponentiation sqrt(5) # square root exp(1) # Euler&#39;s number pi # 3.14159... log(exp(1)) # base e logarithm log10(100) # base 10 logarithm 5 %% 2 # modular division round(1.5579, digits = 2) # rounding 2.4 Conditionals Conditional statements checks if some condition is true or false using logical operators (operators that return either TRUE or FALSE). For example: 20 == 10*2 ## [1] TRUE &quot;hello&quot; == &quot;goodbye&quot; ## [1] FALSE These statements return a value is of type &quot;logical&quot;, which is either TRUE(shorthand: T) if the condition is satisfied, or FALSE (shorthand: F) if the condition is not satisfied. One important note is that TRUE and FALSE are objects on their own, rather than the strings ‚Äútrue‚Äù and ‚Äúfalse‚Äù. Conditional statements are made with a range of logical operators. So far, We‚Äôve seen: Operator Plain English == is equal to != is not equal to &lt; or &gt; is less than OR is greater than &lt;= or &gt;= is less than or equal to OR is greater than or equal to is.na() is an NA value There are other logical operators, including %in%, which checks if a value is present in a vector of possible values. Type the following statements into the console to see their output. &quot;aang&quot; == &quot;aang&quot; &quot;aang&quot; != &quot;kora&quot; 10 &lt; 5 10 &gt;= 5 is.na(&quot;toph&quot;) is.na(NA) &quot;zuko&quot; %in% c(&quot;aang&quot;, &quot;toph&quot;, &quot;katara&quot;) If you‚Äôre keen, you‚Äôll notice in the last line that we use c() to group objects together. This data structure is called a vector. As a brief introduction, vectors combine objects of the same type. Don‚Äôt worry too much about the specifics of vectors, as we will cover it in much greater depth in the next chapter. We can also combine conditions using the logical and (&amp;) along with the logical or (|). The logical &amp; returns TRUE if and only if both conditions are true, and it returns FALSE otherwise. Below is a truth table (don‚Äôt worry about memorizing it): P Q P &amp; Q P True True True True True False False True False True False True False False False False To contexualize this table, let‚Äôs look at the following examples: (5 &gt; 2) &amp; (6 &gt;= 10) ## [1] FALSE (5 &gt; 2) | (6 &gt;= 10) ## [1] TRUE 2.4.1 If statements Conditional statements generate logical values to filter inputs. if statements use conditional statements to control flow of a program. Below is the general form of an if statement: if (the conditional statement is TRUE) { do something } Let‚Äôs look at an example: x &lt;- 6 if (x &gt; 5) { x &lt;- x^2 } x ## [1] 36 x &gt; 5 is TRUE, so the code in the if runs x is now 6^2 or 36 Change x to 4 x &lt;- 4 if (x &gt; 5) { x &lt;- x^2 } x ## [1] 4 x &gt; 5 is FALSE, so the code in the if doesn‚Äôt run x is still 4 This is not a function, so everything that happens in the if statement influences the global environment Here‚Äôs a slightly more applied example: veg_type &lt;- &quot;tree&quot; volume &lt;- 16.08 if (veg_type == &quot;tree&quot;) { mass &lt;- 2.65 * volume^0.9 } mass ## [1] 32.27775 We often want to chose one of several options. We can add more conditions and associated actions with else if veg_type &lt;- &quot;grass&quot; volume &lt;- 16.08 if (veg_type == &quot;tree&quot;) { mass &lt;- 2.65 * volume^0.9 } else if (veg_type == &quot;grass&quot;) { mass &lt;- 0.65 * volume^1.2 } mass ## [1] 18.21615 Here, our script: 1. Checks the first condition 2. If TRUE runs that condition‚Äôs code and skips the rest 3. If not it checks the next one until it runs out of conditions We can specify what to do if none of the conditions are TRUE by using else on its own veg_type &lt;- &quot;shrub&quot; volume &lt;- 16.08 if (veg_type == &quot;tree&quot;) { mass &lt;- 2.65 * volume^0.9 } else if (veg_type == &quot;grass&quot;) { mass &lt;- 0.65 * volume^1.2 } else { mass &lt;- NA } mass ## [1] NA 2.4.2 Multiple ‚Äúif‚Äùs vs ‚Äúelse if‚Äù Multiple ifs check each conditional separately, which is very inefficient. That is, R executes code for all conditions that are TRUE. x &lt;- 5 if (x &gt; 2) { x * 2 } ## [1] 10 if (x &gt; 4) { x * 4 } ## [1] 20 else if checks each condition sequentially, and ONLY executes code for the first condition that is TRUE x &lt;- 5 if (x &gt; 2) { x * 2 } else if (x &gt; 4) { x * 4 } ## [1] 10 2.5 Working directory Any process running on your computer has a notion of its ‚Äúworking directory‚Äù. By default in R, a working directory is where R will look for files you ask it to load. It is also where any files you write to disk will go. You can explicitly check your working directory with: getwd() The working directory is also displayed at the top of the RStudio console. Notice that getwd() looks a lot like ‚Äúget working directory‚Äù. As a beginning R user, it‚Äôs OK let your home directory or any other weird directory on your computer be R‚Äôs working directory. Very soon, I urge you to evolve to the next level, where you organize your analytical projects into directories and, when working on project A, set R‚Äôs working directory to the associated directory. In case you‚Äôre curious, you can set R‚Äôs working directory at the command line like so: setwd(&quot;~/myCoolProject&quot;) The setwd() function is extremely useful for times you want to read in external data, such as a .csv file. Like getwd(), notice that setwd() looks a lot like ‚Äúset working directory‚Äù. Now, let‚Äôs start a mini-project! a &lt;- 2 b &lt;- -3 sig_sq &lt;- 0.5 x &lt;- runif(40) y &lt;- a + b*x + rnorm(40, sd = sqrt(sig_sq)) (avg_x &lt;- mean(x)) ## [1] 0.5064912 plot(x, y) abline(a, b, col = &quot;purple&quot;) dev.print(pdf, &quot;toy_line_plot.pdf&quot;) Don‚Äôt worry if your graph doesn‚Äôt look exactly like the provided output‚Äîrnorm() generates random numbers (from a normal distribution). Imagine that our mini-project is the start of an analysis, and you‚Äôre ready to preserve the logic and code. Here‚Äôs what I want you to do: Visit the History tab of the upper right pane and select the commands we just made. Click ‚ÄúTo Source‚Äù. Now you have a new pane containing a new R script. Click on the floppy disk to save. Give it a name ending in .R or .r. Note that, by default, it will go in the directory associated with your project. Quit RStudio and restart RStudio. Notice that things, by default, restore to where we were earlier (e.g. objects in the workspace, the command history, which files are open for editing, where we are in the file system browser, the working directory for the R process, etc.). Here is an exercise for you: Set a sample size n at the top (e.g. n &lt;- 40), and replace all the hard-coded 40‚Äôs with n. Change some other minor-but-detectable stuff. For example, alter the sample size n, the slope of the line b, the color of the linem ‚Ä¶ whatever. Practice the different ways to re-run the code: Walk through line by line by keyboard shortcut (Command+Enter) or mouse (click ‚ÄúRun‚Äù in the upper right corner of editor pane). Source the entire document‚Äîequivalent to entering source('toy-line.r') in the console‚Äîusing the keyboard shortcut (Command+Shift+S) or mouse (click ‚ÄúSource‚Äù in the upper right corner of editor pane or select from the mini-menu accessible from the associated down triangle). One day you‚Äôll want to recreate a figure or just simply understand where it came from. If you religously save figures to a file with R code and not ever ever ever the mouse or the clipboard, you will sing my praises one day. Trust me. 2.5.1 Other important things Below is a collection of important miscellaneous items to consider. R scripts are usually saved with a .R or .r suffix. Use this convention unless you have some extraordinary reason not to. Comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). Clean out the workspace (i.e., pretend like you‚Äôve just revisited this project after a long absence). You can do so by clicking the broom icon or by typing rm(list = ls()) into the console. This workflow will serve you well in the future: Create an RStudio project for an analytical project. Keep inputs there (we‚Äôll soon talk about importing). Keep scripts there; edit them, run them in bits or as a whole from there. Keep outputs there. Avoid using your mouse for your workflow. Firstly, using the keyboard is faster. Secondly, writing code instead of clicking helps with reproducibility. That is, it will be much easier to retrospectively determine how a numerical table or PDF was actually produced. Many experienced users never save the workspace, never save .RData files (I‚Äôm one of them), and never save or consult the history. Once/if you get to that point, there are options available in RStudio to disable the loading of .RData and permanently suppress the prompt on exit to save the workspace (go to Tools &gt; Options &gt; General). "],["basics-of-r-part-2.html", "3 Basics of R (Part 2) 3.1 Data structures 3.2 For loops 3.3 Functions", " 3 Basics of R (Part 2) In this chapter, we will learn powerful tools invaluable to our data science workflow. For instance, we will create our very own functions, which will allow us to solve increasingly complex problems. 3.1 Data structures Data structures determine the operations/methods/functions are available for each object. For example, you can do +/-/*// for numbers, but these operations will not be available for strings. What operations do you imagine would be useful for strings? paste(&quot;hello&quot;, &quot;world&quot;, sep = &quot;,&quot;) ## [1] &quot;hello,world&quot; Notice that single and double quotes can be used interchangeably, but double quotes are preferred. Single quotes are normally used to delimit characters within double quotes. 3.1.1 Vectors Vectors are a sequence of values with the same type. We can create vectors using c(), which stands for ‚Äúcombine‚Äù. (sites &lt;- c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) ## [1] &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; To access the elements inside a vector, we can do something called ‚Äúslicing‚Äù. To access a single item or multiple items, use the square bracket operator []. In general [] in R means, ‚Äúgive me a piece of something‚Äù. For example: sites[4] ## [1] &quot;c&quot; sites[1:3] ## [1] &quot;a&quot; &quot;a&quot; &quot;b&quot; In sites[1:3], the 1:3 creates a vector from 1 to 3, which is then used to subset multiple items in a vector. Here are some additional useful functions: length(sites) ## [1] 4 density_ha &lt;- c(2.8, 3.2, 1.5, 3.8) mean(density_ha) ## [1] 2.825 max(density_ha) ## [1] 3.8 min(density_ha) ## [1] 1.5 sum(density_ha) ## [1] 11.3 We can also use logical operators on vectors. In the next example, we compare a vector to a single value, and operator returns one logical per value. c(1, 1, 2, 3, 1) == 1 ## [1] TRUE TRUE FALSE FALSE TRUE In English, the above snippet checks if each value in the vector is equal to 1. This is essentially what goes on behind the scenes when we try to subset a vector, except subsetting only returns where the subset condition is TRUE. Let‚Äôs look at an example where we have a vector of sites and a vector of US states they occur in. site &lt;- c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;) state &lt;- c(&#39;FL&#39;, &#39;FL&#39;, &#39;GA&#39;, &#39;AL&#39;) Now, let‚Äôs check if the state is 'FL'. This should return a vector of TRUE and FALSE. state == &#39;FL&#39; ## [1] TRUE TRUE FALSE FALSE Now, let‚Äôs filter the site vector to return values where the state is equal to 'FL'. site[state == &#39;FL&#39;] ## [1] &quot;a&quot; &quot;b&quot; The above snippet is the equivalent to passing a vector of TRUE and FALSE values inside the square brackets: site[c(TRUE, TRUE, FALSE, FALSE)] ## [1] &quot;a&quot; &quot;b&quot; Now we turn our attention to multiple vectors. If our vectors are the same length, we can use math operations to combine each index element-wise. density_ha &lt;- c(2.8, 3.2, 1.5, 3.8) area_ha &lt;- c(3, 5, 1.9, 2.7) (total_number &lt;- density_ha * area_ha) ## [1] 8.40 16.00 2.85 10.26 Subsetting across multiple vectors is the same as that for a single vector: # recall: sites &lt;- c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;) # recall: area_ha &lt;- c(3, 5, 1.9, 2.7) area_ha[sites == &quot;a&quot;] ## [1] 3 5 The above code snippet selects all members of area_ha at the indices where sites == 'a'. This brings us to another important concept: == asks ‚Äúdoes it equal to?‚Äù in most programming languages. == is not the same as =. This is why we use &lt;- to assign things instead of =. We can also ask the question of ‚Äúdoes it not equal to?‚Äù area_ha[sites != &quot;a&quot;] ## [1] 1.9 2.7 Likewise, we can check ‚Äúis it greater or less than?‚Äù sites[area_ha &gt; 3] ## [1] &quot;a&quot; sites[area_ha &gt;= 3] ## [1] &quot;a&quot; &quot;a&quot; sites[area_ha &lt; 3] ## [1] &quot;b&quot; &quot;c&quot; Notice that all the questions we ask have a ‚Äúyes‚Äù (True) or ‚Äúno‚Äù (False) answer. The operators associated with these questions are called Boolean operators. Finally, we can subset a vector based on itself. sites[sites != &quot;a&quot;] ## [1] &quot;b&quot; &quot;c&quot; 3.1.2 Missing values So far we‚Äôve worked with data with no missing values. In real life, however, we often have missing values (NA values). Unfortunately for us, R does not get along with NA values. density_ha &lt;- c(2.8, 3.2, 1.5, NA) mean(density_ha) ## [1] NA Please note that NA is different from NULL. Take a look at the following example. na_vector &lt;- c(1, 2, 3, NA) null_vector &lt;- c(1, 2, 3, NULL) # look at the NA vector... na_vector ## [1] 1 2 3 NA # ... now look at the NULL vector null_vector ## [1] 1 2 3 # do the vectors work with a function such as mean()? mean(na_vector) ## [1] NA mean(null_vector) ## [1] 2 Why did we get NA? Well, it‚Äôs hard to say what a calculation including NA should be, so most calculations return NA when NA is in the data. One way to resolve this issue is to tell our function to remove the NA before executing: mean(density_ha, na.rm = TRUE) ## [1] 2.5 3.1.3 Data frames This is where things get really exciting! We will use these data structures extensively in the upcoming labs, so it‚Äôs important to pay attention here. A data frame is a list of equal length vectors grouped together. More importantly, a data frame can contain both categorical and numerical values, whereas a vector can only contain variables of the same type (i.e., all numerical, all categorical, etc.). surveys &lt;- data.frame(sites, density_ha, area_ha) surveys ## sites density_ha area_ha ## 1 a 2.8 3.0 ## 2 a 3.2 5.0 ## 3 b 1.5 1.9 ## 4 c NA 2.7 Here are some useful commands to investigate a data frame: str() returns the structure of a data frame. length() returns the length of a data frame. nrow() returns the number of rows of a data frame (same as length()) ncol() returns the number of columns of a data frame. str(surveys) ## &#39;data.frame&#39;: 4 obs. of 3 variables: ## $ sites : Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 1 2 3 ## $ density_ha: num 2.8 3.2 1.5 NA ## $ area_ha : num 3 5 1.9 2.7 length(surveys) ## [1] 3 nrow(surveys) ## [1] 4 ncol(surveys) ## [1] 3 Subsetting data frames is extremely similar to that for vectors. This time, however, we need to consider both rows and columns. We can access a specific member like this: my_data_frame[row, column] # visit one cell by row and column surveys[1, 2] ## [1] 2.8 # visit a range of rows and columns surveys[1:2, 2:3] ## density_ha area_ha ## 1 2.8 3 ## 2 3.2 5 # every row on the third column surveys[, 3] ## [1] 3.0 5.0 1.9 2.7 # visit column by name surveys[&#39;area_ha&#39;] ## area_ha ## 1 3.0 ## 2 5.0 ## 3 1.9 ## 4 2.7 # visit column by name (preferred method) surveys$area_ha ## [1] 3.0 5.0 1.9 2.7 # visit column by name surveys[[&#39;area_ha&#39;]] ## [1] 3.0 5.0 1.9 2.7 # visit multiple columns (preferred method) surveys[c(&#39;area_ha&#39;, &#39;sites&#39;)] ## area_ha sites ## 1 3.0 a ## 2 5.0 a ## 3 1.9 b ## 4 2.7 c 3.1.4 External data We can read in external data using theread.csv() function. The main argument is the location of the data, which is either a url or a path on your computer. shrub_data &lt;- read.csv(&#39;https://datacarpentry.org/semester-biology/data/shrub-dimensions-labeled.csv&#39;) 3.1.5 Factors Let‚Äôs use the str() function to get more information about our variable shrub_data. str(shrub_data) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ shrubID: Factor w/ 10 levels &quot;a1&quot;,&quot;a2&quot;,&quot;b1&quot;,..: 1 2 3 4 5 6 7 8 9 10 ## $ length : num 2.2 2.1 2.7 3 3.1 2.5 1.9 1.1 3.5 2.9 ## $ width : num 1.3 2.2 1.5 4.5 3.1 2.8 1.8 0.5 2 2.7 ## $ height : num 9.6 7.6 2.2 1.5 4 3 4.5 2.3 7.5 3.2 Notice that the shrubID column has type Factor. A factor is a special data type in R for categorical data. Factors are useful for statistics, but can mess up some aspects of computation as we‚Äôll see in future chapters. shrub_data &lt;- read.csv(&#39;https://datacarpentry.org/semester-biology/data/shrub-dimensions-labeled.csv&#39;, stringsAsFactors = FALSE) str(shrub_data) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ shrubID: chr &quot;a1&quot; &quot;a2&quot; &quot;b1&quot; &quot;b2&quot; ... ## $ length : num 2.2 2.1 2.7 3 3.1 2.5 1.9 1.1 3.5 2.9 ## $ width : num 1.3 2.2 1.5 4.5 3.1 2.8 1.8 0.5 2 2.7 ## $ height : num 9.6 7.6 2.2 1.5 4 3 4.5 2.3 7.5 3.2 3.1.6 Lists Lists are a vector-like structure that can store other objects/data structures. It‚Äôs sort of like a vector that holds vectors. The main difference between a list and data frame is that lists can have elements with an unequal length. sites &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) notes &lt;- &quot;It was a good day in the field today. Warm, sunny, lots of gators.&quot; helpers &lt;- 4 field_notes &lt;- list(sites, notes, helpers) field_notes[1] ## [[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; field_notes[[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; We can also give the values names and access them using the $ symbol (preferred) or via [&quot;variable_name&quot;] with subsetting. field_notes &lt;- list(my_sites = sites, notes = notes, my_helpers = helpers) field_notes$my_sites ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; field_notes[[&quot;my_sites&quot;]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; 3.2 For loops Loops are fundamental a programming concept as they get a lot of repetitive stuff done in very few lines of code. You can think of loops as something that performs the same operation on a lot of things. Here‚Äôs what the syntax of a loop looks like: for (item in list_of_items) { do_something(item) } And here is an example: for (i in 1:5) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 The above snippet is equivalent to this: print(1) print(2) print(3) print(4) print(5) You can also have multiple lines of code in the body of a loop. for (number in 1:5) { i &lt;- i*2 print(i) } ## [1] 10 ## [1] 20 ## [1] 40 ## [1] 80 ## [1] 160 In the previous examples, we used the dummy variables i and number to take on some range of values. Notice that i can be called anything you want. To contexualize this, let‚Äôs look at an example that calculates mass using the following fomula: \\(\\text{mass} = \\text{density} \\times \\text{volume}\\) density &lt;- 2.65 volumes &lt;- c(1.6, 3, 8) for (i in 1:length(volumes)) { mass &lt;- density * volumes[i] print(mass) } ## [1] 4.24 ## [1] 7.95 ## [1] 21.2 Looping with an index allows us to store results calculated in the loop. In the following snippet, we are going to create an empty vector with sthe length of our volumes vector. ( masses &lt;- vector(mode = &quot;numeric&quot;, length = length(volumes)) ) ## [1] 0 0 0 Note that &quot;numeric&quot; is the type of data we are going to store and length(volumes) is the desired length of our vector. Now, add each result in the correct position‚Äîfor each iteration through the loop, save the output in the empty vector (masses) at the i‚Äôth position. for (i in 1:length(volumes)){ mass &lt;- 2.65 * volumes[i] masses[i] &lt;- mass } masses ## [1] 4.24 7.95 21.20 3.2.1 Looping over multiple files We turn our attention now to a (slightly more) useful example: how do we analyze multiple files with similar contents? In this hypothetical example, we have 5 datasets with satellite coodinates at specific points orbiting the Earth. Suppose the files are similarly named (click on the files to download them): locations-2016-01-01.txt locations-2016-01-02.txt locations-2016-01-03.txt locations-2016-01-04.txt locations-2016-01-05.txt Our goal is to determine the number of satellite coordinates per file. First, retrieve the name of each file. data_files &lt;- list.files(path = &quot;data/02_intro-to-r/&quot;, pattern = &quot;locations-.*.txt&quot;, full.names = TRUE) Note that the asterisk in &quot;*.txt&quot; refers to ‚Äúany name in this directory‚Äù whereas the &quot;.txt&quot; part ensures we are only selecting .txt files. Next, determine the number of observations in each file. We will assume that each row corresponds to a single coordinate. results &lt;- vector(mode = &quot;integer&quot;, length = length(data_files)) for (i in 1:length(data_files)) { data &lt;- read.csv(data_files[i]) count &lt;- nrow(data) results[i] &lt;- count } Now, store the output in a data frame and associate the file name with the count. results &lt;- data.frame(file_name = character(length(data_files)), count = integer(length(data_files)), stringsAsFactors = FALSE) for (i in 1:length(data_files)){ data &lt;- read.csv(data_files[i]) count &lt;- nrow(data) results$file_name[i] &lt;- data_files[i] results$count[i] &lt;- count } results ## file_name count ## 1 data/02_intro-to-r//locations-2016-01-01.txt 4 ## 2 data/02_intro-to-r//locations-2016-01-02.txt 8 ## 3 data/02_intro-to-r//locations-2016-01-03.txt 10 ## 4 data/02_intro-to-r//locations-2016-01-04.txt 10 ## 5 data/02_intro-to-r//locations-2016-01-05.txt 12 Voila! With the knowledge in this chapter, you are ready to solve some real-world problems üòÑ 3.2.2 Nested loops (OPTIONAL) Sometimes, we need to loop over more than a single range of numbers. For example, what if we want to select all pixels on a 3x4 rectangular screen? Here, we need to cover both the ‚Äúx‚Äù and ‚Äúy‚Äù pixel coodinates: for (i in 1:3) { for (j in 1:4) { print(paste(&quot;i = &quot; , i, &quot;; j = &quot;, j)) } } ## [1] &quot;i = 1 ; j = 1&quot; ## [1] &quot;i = 1 ; j = 2&quot; ## [1] &quot;i = 1 ; j = 3&quot; ## [1] &quot;i = 1 ; j = 4&quot; ## [1] &quot;i = 2 ; j = 1&quot; ## [1] &quot;i = 2 ; j = 2&quot; ## [1] &quot;i = 2 ; j = 3&quot; ## [1] &quot;i = 2 ; j = 4&quot; ## [1] &quot;i = 3 ; j = 1&quot; ## [1] &quot;i = 3 ; j = 2&quot; ## [1] &quot;i = 3 ; j = 3&quot; ## [1] &quot;i = 3 ; j = 4&quot; Here‚Äôs a slightly more complex example that finds all prime numbers from 1 to 15: for (i in 2:15) { # assume each number is prime by default prime &lt;- TRUE for (j in 2:(i-1)) { # if i is divisible by any number from 2 to i-1, it is not a prime if (i%%j == 0) { prime &lt;- FALSE } } if (prime == TRUE | i == 2) { print(paste(i, &quot;is prime&quot;)) } } ## [1] &quot;2 is prime&quot; ## [1] &quot;3 is prime&quot; ## [1] &quot;5 is prime&quot; ## [1] &quot;7 is prime&quot; ## [1] &quot;11 is prime&quot; ## [1] &quot;13 is prime&quot; 3.3 Functions Sometimes, R will leave us wanting for custom functions. Luckily, we can define our own functions! This is the general syntax for a function: function_name &lt;- function(arguments) { output_value &lt;- do_something(inputs) return(output_value) } Remark: every function returns a value. Recall from your grade-school math class that functions take an input and return an output. In R, however, a function may or may not take user-defined input. This brings me to an extremely important point: creating a function does NOT run it. You must call the function to run it. A function is meant to be reusable‚Äîtreat it as such. # define the function calc_shrub_vol &lt;- function(length, width, height) { area &lt;- length * width volume &lt;- area * height return(volume) } # call the function calc_shrub_vol(0.8, 1.6, 2.0) ## [1] 2.56 As always, to save a function‚Äôs return value, you need to store the output in an appropriate data structure. shrub_vol &lt;- calc_shrub_vol(0.8, 1.6, 2.0) A helpful tip is to treat functions like a black box‚Äîthe only things a function ‚Äúknows‚Äù are the inputs we pass it. Likewise, the only thing R ‚Äúknows‚Äù about a function is the output the function returns. Let‚Äôs walk through calc_shrub_vol()‚Äôs execution (key terms are in bold): Call the calc_shrub_vol() function. Within the function, Assign 0.8 to length, 1.6 to width, and 2.0 to height inside the function. Calculate the area and assign it to area. Calculate volume and assign it to volume. Return volume as the function output. Assign the function‚Äôs output to a new variable called shrub_vol. Since R treats functions like a black box, you can‚Äôt access a variable that was created in a function. You must save the output of a function (to a variable) to use it later. 3.3.1 Conditionals within functions Recall that we used a conditional to estimate mass differently for different types of vegetation. Since this is the kind of code we are going to want to reuse, let‚Äôs move it into a function. est_mass &lt;- function(volume, veg_type) { if (veg_type == &quot;tree&quot;) { mass &lt;- 2.65 * volume^0.9 } else if (veg_type == &quot;grass&quot;) { mass &lt;- 0.65 * volume^1.2 } else { mass &lt;- NA } return(mass) } We can then run this function with different vegetation types and get different estimates for mass. est_mass(1.6, &quot;tree&quot;) ## [1] 4.045329 est_mass(1.6, &quot;grass&quot;) ## [1] 1.142503 est_mass(1.6, &quot;shrub&quot;) ## [1] NA Let‚Äôs walk through how est_mass(1.6, &quot;shrub&quot;) executes: When we call est_mass(), the function assigns 1.6 to volume and &quot;shrub&quot; to veg_type. The function checks if veg_type is equal to &quot;tree&quot;. It isn‚Äôt, so it checks if veg_type is equal to &quot;grass&quot;. It isn‚Äôt, so it goes to the else statement and executes the code in that block. Hee, the function assignsNA to mass. The function exits the if/else if/else blocks and returns the value for mass, which is NA. 3.3.2 Nested conditionals Occasionally, we need to make complex decisions that can‚Äôt be captured using a simple if/else if/else statement. For example, we might have different equations for some vegetation types based on the age of the plant. To solve this problem, we can ‚Äúnest‚Äù conditionals inside of one another. est_mass &lt;- function(volume, veg_type, age) { if (veg_type == &quot;tree&quot;) { # nested condition if (age &lt; 5) { mass &lt;- 1.6 * volume^0.8 } else { mass &lt;- 2.65 * volume^0.9 } } else if (veg_type == &quot;grass&quot; | veg_type == &quot;shrub&quot;) { mass &lt;- 0.65 * volume^1.2 } else { mass &lt;- NA } return(mass) } est_mass(1.6, &quot;tree&quot;, age = 2) ## [1] 2.330322 est_mass(1.6, &quot;shrub&quot;, age = 5) ## [1] 1.142503 Try to minimize nested functions whenever possible as it can be difficult to read. 3.3.3 Function arguments As seen previously, we can use custom inputs by defining an input argument. As lazy programmers, we usually want to call a function without typing much. This is where default arguments come in handy. For example, many of our shrubs are the same height so for those shrubs we only measure the length and width. We can set a default value for shrub height for cases where we don‚Äôt measure it. calc_shrub_vol &lt;- function(length, width, height = 1) { area &lt;- length * width volume &lt;- area * height return(volume) } calc_shrub_vol(0.8, 1.6) # default argument for height ## [1] 1.28 calc_shrub_vol(0.8, 1.6, 2.0) # default argument is overridden by 2,0 ## [1] 2.56 calc_shrub_vol(length = 0.8, width = 1.6, height = 2.0) ## [1] 2.56 As you could tell from the last two examples, you can override the default argument by providing your own value. Here are some additional points: You can always use names to assign a parameter to an argument. If not, using names then order is determined by parameter order. For example, First value is length, second value is width, third value is height. In many cases there are a lot of optional arguments. In this case, we can specify an argument by name to avoid confusion. Oftentimes, only the optional arguments are specified (i.e., those without a default value). In our case, we would write: calc_shrub_vol(0.8, 1.6, height = 2.0) ## [1] 2.56 3.3.4 Combining functions Here are some guidelines for creating good functions: Each function should be single conceptual chunk of code. Functions can be combined to perform larger tasks. est_shrub_mass &lt;- function(volume) { mass &lt;- 2.65 * volume^0.9 } shrub_volume &lt;- calc_shrub_vol(0.8, 1.6, 2.0) shrub_mass &lt;- est_shrub_mass(shrub_volume) We can nest functions. Below, the inner function executes before the outer function executes. shrub_mass &lt;- est_shrub_mass(calc_shrub_vol(0.8, 1.6, 2.0)) We need to be careful with this because nesting code can be difficult to read. As a general rule of thumb, don‚Äôt nest more than two functions. We can also call functions from inside other functions. This allows us to organize function calls into logical groups. est_shrub_mass_dim &lt;- function(length, width, height) { volume = calc_shrub_vol(length, width, height) mass &lt;- est_shrub_mass(volume) return(mass) } est_shrub_mass_dim(0.8, 1.6, 2.0) ## [1] 6.175354 Now that we‚Äôve got the basics of R under our belts, we can jump into the fun data science applications! "],["more-data-frames.html", "4 More data frames 4.1 Get the gapminder data 4.2 Explore gapminder 4.3 Data frames with dplyr", " 4 More data frames Whenever you have rectangular, ‚Äúspreadsheetey‚Äù data, your default data structure in R should be the data frame. Do not depart from this without good reason. Data frames are awesome because They neatly package related variables and maintain a ‚Äúrow-ordering‚Äù like that in a spreadsheet. This makes it easy to apply filters to rows and columns of interest. Most functions for inference, modelling, and graphing will happily take a data frame object. The set of packages known as the tidyverse takes data frames one step further and explicitly prioritizes the processing of data frames. Data frames, unlike general arrays or matrices in R, can hold variables of different flavours. For example, data frames can simultaneously hold character data (e.g., subject ID or name), quantitative data (e.g., white blood cell count), and categorical information (e.g., treated vs. untreated). If you use data structures that can only hold 1 type of data (e.g., matrices) for data analysis, you are likely to make the terrible mistake of spreading a dataset out over multiple, unlinked objects. Why? Because you can‚Äôt put character data, such as subject name, into the numeric matrix that holds white blood cell count. This fragmentation is a bad idea. 4.1 Get the gapminder data We will work with some of the data from the Gapminder project. The Gapminder project contains the gapminder dataset, which summarises the progression of countries over time for statistics like life expectancy and GDP. Let‚Äôs first install the package: install.packages(&quot;gapminder&quot;) Now load the package: library(gapminder) 4.2 Explore gapminder By loading the gapminder package, we now have access to a data frame by the same name. Get an overview of the data frame with str(), which displays the structure of an object. str(gapminder) ## tibble [1,704 √ó 6] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... ## $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... ## $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... ## $ gdpPercap: num [1:1704] 779 821 853 836 740 ... str() will provide a sensible description of almost anything and, worst case, nothing bad can actually happen. When in doubt, just use str() on your recently created objects to get ideas about what to do next. We could print the gapminder object itself to screen. However, if you‚Äôve used R before, you might be reluctant to do this, because large datasets fill your Console and provide very little insight. If you have not already done so, install the tidyverse meta-package now: install.packages(&quot;tidyverse&quot;) Now load it: library(tidyverse) class(gapminder) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; We can now print gapminder! Notice that the class (type of data structure) of the gapminder object is a tibble, the tidyverse‚Äôs version of R‚Äôs data frame. A tibble is also a data frame. gapminder ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # ‚Ä¶ with 1,694 more rows Although this seems like a lot of output, notice that tibbles provide a nice print method that shows the most important stuff and doesn‚Äôt fill up your console. Let‚Äôs make sense of the output: The first line refers to what we‚Äôre printing‚Äîa tibble with 1704 rows and 6 columns. Below each column heading, we see &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;. These refer to the variable type of that column. fct is a factor (kind of like a categorical variable), int stands for integer, and dbl stands for double (a number with decimal places). If you‚Äôre only interested in the first or last couple of rows, use head() or tail(). head() displays the first 6 rows of your data frame by default, and tail() shows the last 6 rows. head(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. tail(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Zimbabwe Africa 1982 60.4 7636524 789. ## 2 Zimbabwe Africa 1987 62.4 9216418 706. ## 3 Zimbabwe Africa 1992 60.4 10704340 693. ## 4 Zimbabwe Africa 1997 46.8 11404948 792. ## 5 Zimbabwe Africa 2002 40.0 11926563 672. ## 6 Zimbabwe Africa 2007 43.5 12311143 470. You can also specify the number of rows displayed by passing in a number. head(gapminder, n = 3) ## # A tibble: 3 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. Just for your reference, if you want to change a data frame into a tibble for nicer printing, use as_tibble()! as_tibble(my_data_frame) # my_data_frame is the thing we want to make a tibble Here are more ways to query basic info on a data frame: Function Description names() returns column names ncol() returns number of columns nrow() returns number of rows dim() returns # of rows by # of columns names(gapminder) ## [1] &quot;country&quot; &quot;continent&quot; &quot;year&quot; &quot;lifeExp&quot; &quot;pop&quot; &quot;gdpPercap&quot; ncol(gapminder) ## [1] 6 nrow(gapminder) ## [1] 1704 dim(gapminder) ## [1] 1704 6 A statistical summary of the data can be obtained with summary(). That is, each column‚Äôs statistics are shown separately. summary(gapminder) ## country continent year lifeExp ## Afghanistan: 12 Africa :624 Min. :1952 Min. :23.60 ## Albania : 12 Americas:300 1st Qu.:1966 1st Qu.:48.20 ## Algeria : 12 Asia :396 Median :1980 Median :60.71 ## Angola : 12 Europe :360 Mean :1980 Mean :59.47 ## Argentina : 12 Oceania : 24 3rd Qu.:1993 3rd Qu.:70.85 ## Australia : 12 Max. :2007 Max. :82.60 ## (Other) :1632 ## pop gdpPercap ## Min. :6.001e+04 Min. : 241.2 ## 1st Qu.:2.794e+06 1st Qu.: 1202.1 ## Median :7.024e+06 Median : 3531.8 ## Mean :2.960e+07 Mean : 7215.3 ## 3rd Qu.:1.959e+07 3rd Qu.: 9325.5 ## Max. :1.319e+09 Max. :113523.1 ## 4.2.1 Importing and exporting data We can export the data frame to a comma-separated values (.csv) file. write.csv(gapminder, file = &quot;data/03_data-frames/gapminder.csv&quot;) The ‚Äú.csv‚Äù extension stands for comma-separated values. This is the preferred way of importing and exporting data as it contains no formatting. You can also import a .csv file to Excel. On top of writing to a .csv file, we can also read .csv files into R. It‚Äôs as simple as read.csv()! gapminder2 &lt;- read.csv(&quot;data/03_data-frames/gapminder.csv&quot;, header = TRUE) class(gapminder2) ## [1] &quot;data.frame&quot; As you can see,read.csv() returns a data frame object by default. 4.2.2 Exploring variables in a data frame To specify a single variable from a data frame, use the dollar sign $. Let‚Äôs explore the numeric variable for life expectancy. head(gapminder$lifeExp) ## [1] 28.801 30.332 31.997 34.020 36.088 38.438 summary(gapminder$lifeExp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 23.60 48.20 60.71 59.47 70.85 82.60 hist(gapminder$lifeExp) Don‚Äôt worry too much about the code to make the figures right now‚Äîwe will learn how to visualize data in future lectures. For now, let‚Äôs continue to explore gapminder. Take a look at the year variable: class(gapminder$year) ## [1] &quot;integer&quot; Notice that year holds integers. On the other hand, continent holds categorical information, which is called a factor in R. class(gapminder$continent) ## [1] &quot;factor&quot; Now, I want to illustrate something important: summary(gapminder$year) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1952 1966 1980 1980 1993 2007 summary(gapminder$continent) ## Africa Americas Asia Europe Oceania ## 624 300 396 360 24 Notice that the same function returned different outputs for different variable types‚Äîforgetting this observation can lead to confusion in the future, so make sure to check your data before analysis! Let‚Äôs check out a couple more useful functions and highlight important ideas in the meantime. Within a given column/variable, table() returns the number of observations, levels() returns unique values, and nlevels() returns the number of unique values. table(gapminder$continent) ## ## Africa Americas Asia Europe Oceania ## 624 300 396 360 24 levels(gapminder$continent) ## [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; nlevels(gapminder$continent) ## [1] 5 The levels of the factor continent are ‚ÄúAfrica‚Äù, ‚ÄúAmericas‚Äù, etc.‚Äîthis is what‚Äôs usually presented to your eyeballs by R. Behind the scenes, R assigns integer values (i.e., 1, 2, 3, ‚Ä¶) to each level. Never ever ever forget this fact. Look at the result from str(gapminder$continent) if you are skeptical: str(gapminder$continent) ## Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... Specifically in modelling and figure-making, factors are anticipated and accommodated by the functions and packages you will want to exploit. Note that factors do NOT contain integers. Factors are a numerical way that R uses to represent categorical data. Tl;dr, factors are categorical variables whereas levels are unique values within a factor. 4.2.3 Data frame summary Use data frames and the tidyverse!! The tidyverse provides a special type of data frame called a ‚Äútibble‚Äù that has nice default printing behavior, among other benefits. When in doubt, str() something or print something. Always understand the basic extent of your data frames: number of rows and columns. Understand what your variable types are. Use factors!! (but with intention and care) Do basic statistical and visual sanity checking of each variable. Refer to variables by name (ex: gapminder$lifeExp) and NOT by column number. Your code will be more robust and readable. 4.3 Data frames with dplyr dplyr is a package for data manipulation developed by Hadley Wickham and Romain Francois. It is built to be fast, highly expressive, and open-minded about how your data is stored. It is installed as part of the the tidyverse meta-package and it is among the packages loaded via library(tidyverse). Here‚Äôs a bit of fun trivia: dplyr stands for ‚Äúdata frame pliers‚Äù. 4.3.1 Subsetting data If you feel the urge to store a little snippet of your data: canada &lt;- gapminder[241:252, ] Stop and ask yourself ‚Ä¶ Do I want to create a separate subset of my original data? If ‚ÄúYES,‚Äù use proper data aggregation techniques or don‚Äôt subset the data. Alternatively, only subset the data as a temporary measure while you develop your elegant code. If ‚ÄúNO,‚Äù then don‚Äôt subset! Copies and excerpts of your data clutter your workspace, invite mistakes, and sow general confusion. Avoid whenever possible. Reality can also lie somewhere in between. You will find the workflows presented below can help you accomplish your goals with minimal creation of temporary, intermediate objects. Recall therm() function, which removes unwanted variable(s). x &lt;- &#39;thing to not keep&#39; print(x) rm(x) # print(x) # gives an error because x is deleted 4.3.2 Filter rows with filter() filter() takes logical expressions and returns the rows for which all are TRUE. Use this when you want to subset observations based on values. The first argument is the name of the data frame. The subsequent arguments are the expressions that filter the dataframe. For example, let‚Äôs filter all rows from gapminder where life expectancy is less than 29 years. filter(gapminder, lifeExp &lt; 29) ## # A tibble: 2 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Rwanda Africa 1992 23.6 7290203 737. When you run this line of code, dplyr filters the data and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you need to use the assignment operator, &lt;-. Let‚Äôs try this out! Here we filter based on country and year: rwanda_gthan_1979 &lt;- filter(gapminder, country == &quot;Rwanda&quot;, year &gt; 1979) Compare with some base R code to accomplish the same things: gapminder[gapminder$lifeExp &lt; 29, ] # indexing is distracting subset(gapminder, country == &quot;Rwanda&quot;) # almost same as filter; quite nice actually What if you want to filter rows based on multiple values in a variable? For example, what if we want to filter all rows with either Rwanda or Afghanistan as countries? filter(gapminder, country == &quot;Rwanda&quot; | country == &quot;Afghanistan&quot;) Here we use a Boolean operator, |, which means ‚Äúor‚Äù. Boolean operators always return either TRUE or FALSE. Some other common ones are &amp; (and) and ! (not). What if we want to keep more than just 2 countries? One way would be to string Boolean operators together like so: country == &quot;Canada&quot; | country == &quot;Rwanda&quot; | country == &quot;Afghanistan | ... This, however, is very wordy. A useful shortcut is to use x %in% y. This selects every row where x is one of the values in y: filter(gapminder, country %in% c(&quot;Rwanda&quot;, &quot;Afghanistan&quot;)) filter(gapminder, country %in% c(&quot;Canada&quot;, &quot;Rwanda&quot;, &quot;Afghanistan&quot;)) Under no circumstances should you subset your data the way I did at first: excerpt &lt;- gapminder[241:252, ] Why is this a terrible idea? It is not self-documenting. What is so special about rows 241 through 252? It is fragile. This line of code will produce different results if someone changes the row order of gapminder, e.g. sorts the data earlier in the script. filter(gapminder, country == &quot;Canada&quot;) This call explains itself and is fairly robust. 4.3.3 Pipe operator %&gt;% Before we go any further, we should exploit the new pipe operator that the tidyverse imports from the magrittr package by Stefan Bache. Here‚Äôs what it looks like: %&gt;%. The RStudio keyboard shortcut: Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac). Let‚Äôs demo then I‚Äôll explain: gapminder %&gt;% head() ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. The above code is equivalent to head(gapminder). The pipe operator takes the thing on the left-hand-side and pipes it into the function call on the right-hand-side. It literally drops it in as the first argument. You can think of an argument as your input to a function. If you remember your grade school math, functions in R do exactly what you‚Äôve learned in school ‚Äì it takes inputs (arguments/parameters) and spits an output, or a return value. Never fear, you can still specify other arguments to this function! To see the first 3 rows of Gapminder, we could say head(gapminder, 3) or this: gapminder %&gt;% head(3) ## # A tibble: 3 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. You are probably not impressed yet, but the magic will happen soon. 4.3.4 Select Columns with select() Use select() to subset the data on variables or columns. Here‚Äôs a conventional call: select(gapminder, year, lifeExp) ## # A tibble: 1,704 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 ## 2 1957 30.3 ## 3 1962 32.0 ## 4 1967 34.0 ## 5 1972 36.1 ## 6 1977 38.4 ## 7 1982 39.9 ## 8 1987 40.8 ## 9 1992 41.7 ## 10 1997 41.8 ## # ‚Ä¶ with 1,694 more rows And here‚Äôs the same operation, but written with the pipe operator and piped through head(): gapminder %&gt;% select(year, lifeExp) %&gt;% head(4) ## # A tibble: 4 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 ## 2 1957 30.3 ## 3 1962 32.0 ## 4 1967 34.0 Think: ‚ÄúTake gapminder, then select the variables year and lifeExp, then show the first 4 rows.‚Äù If we didn‚Äôt have the pipe operator, this is what the above function would look like: head(select(gapminder, year, lifeExp), 4) ## # A tibble: 4 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 ## 2 1957 30.3 ## 3 1962 32.0 ## 4 1967 34.0 As you can see, this is way harder to read. That‚Äôs why the pipe operator is so useful. An important note is that select does not actually filter any rows. It simply selects columns. select() used alongisde everything() is also quite handy if you want to move variables within your data frame. The everything() function selects all variables not explicitly mentioned in select(). For example, let‚Äôs move year and continent to the front of the gapminder tibble: select(gapminder, year, continent, everything()) ## # A tibble: 1,704 x 6 ## year continent country lifeExp pop gdpPercap ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1952 Asia Afghanistan 28.8 8425333 779. ## 2 1957 Asia Afghanistan 30.3 9240934 821. ## 3 1962 Asia Afghanistan 32.0 10267083 853. ## 4 1967 Asia Afghanistan 34.0 11537966 836. ## 5 1972 Asia Afghanistan 36.1 13079460 740. ## 6 1977 Asia Afghanistan 38.4 14880372 786. ## 7 1982 Asia Afghanistan 39.9 12881816 978. ## 8 1987 Asia Afghanistan 40.8 13867957 852. ## 9 1992 Asia Afghanistan 41.7 16317921 649. ## 10 1997 Asia Afghanistan 41.8 22227415 635. ## # ‚Ä¶ with 1,694 more rows Here‚Äôs the data for Cambodia, but only certain variables‚Ä¶ gapminder %&gt;% filter(country == &quot;Cambodia&quot;) %&gt;% select(year, lifeExp) ## # A tibble: 12 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 39.4 ## 2 1957 41.4 ## 3 1962 43.4 ## 4 1967 45.4 ## 5 1972 40.3 ## 6 1977 31.2 ## 7 1982 51.0 ## 8 1987 53.9 ## 9 1992 55.8 ## 10 1997 56.5 ## 11 2002 56.8 ## 12 2007 59.7 ‚Ä¶ and what a typical base R call would look like: gapminder[gapminder$country == &quot;Cambodia&quot;, c(&quot;year&quot;, &quot;lifeExp&quot;)] ## # A tibble: 12 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 39.4 ## 2 1957 41.4 ## 3 1962 43.4 ## 4 1967 45.4 ## 5 1972 40.3 ## 6 1977 31.2 ## 7 1982 51.0 ## 8 1987 53.9 ## 9 1992 55.8 ## 10 1997 56.5 ## 11 2002 56.8 ## 12 2007 59.7 4.3.5 Pure, predictable, pipeable (OPTIONAL) We‚Äôve barely scratched the surface of dplyr but I want to point out key principles you may start to appreciate. If you‚Äôre new to R or ‚Äúprogramming with data‚Äù, feel free skip this section. dplyr‚Äôs verbs, such as filter() and select(), are what‚Äôs called pure functions. To quote from Wickham‚Äôs Advanced R Programming book: The functions that are the easiest to understand and reason about are pure functions: functions that always map the same input to the same output and have no other impact on the workspace. In other words, pure functions have no side effects: they don‚Äôt affect the state of the world in any way apart from the value they return. And finally, the data is always the very first argument of every dplyr function. 4.3.6 Additional resources dplyr official stuff package home on CRAN note there are several vignettes, with the introduction being the most relevant right now the one on window functions will also be interesting to you now development home on GitHub tutorial HW delivered (note this links to a DropBox folder) at useR! 2014 conference RStudio Data Wrangling cheatsheet, covering dplyr and tidyr. Remember you can get to these via Help &gt; Cheatsheets. Excellent slides on pipelines and dplyr by TJ Mahr, talk given to the Madison R Users Group. Blog post Hands-on dplyr tutorial for faster data manipulation in R by Data School, that includes a link to an R Markdown document and links to videos Cheatsheet from R Studio for dplyr. "],["more-dplyr.html", "5 More dplyr 5.1 Review and preparation 5.2 Use mutate() to add new variables 5.3 Use arrange() to row-order data 5.4 Use rename() to rename variables 5.5 Perform tasks on subsets with group_by() 5.6 Introduction to visualization (OPTIONAL) 5.7 Comprehensive practice 5.8 Data wrangling summary", " 5 More dplyr 5.1 Review and preparation In the previous chapter, we introduced three important data wrangling concepts: filter() for subsetting rows select() for subsetting columns (i.e., variables) The pipe operator %&gt;%, which feeds the left-hand side as the first argument to the expression on the right-hand side We also discussed dplyr‚Äôs role inside the tidyverse and tibbles: dplyr is a core package in the tidyverse meta-package. Since we often make incidental usage of the others, we will load dplyr and the others via library(tidyverse). The tidyverse embraces a special flavor of data frame, called a tibble. The gapminder dataset is stored as a tibble. Let‚Äôs load the tidyverse and gapminder. library(tidyverse) library(gapminder) We‚Äôre going to make changes to the gapminder tibble. To eliminate any fear that you‚Äôre damaging the data that comes with the package, let‚Äôs create an explicit copy of gapminder for our experiments. Don‚Äôt worry if you modify the gapminder package, since your changes are temporary (i.e., you can reload the gapminder to get a fresh dataset). my_gap &lt;- gapminder Pay close attention when we evaluate statements but let the output just print to screen‚Ä¶ ## let output print to screen, but do not store my_gap %&gt;% filter(country == &quot;Canada&quot;) ## # A tibble: 12 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Canada Americas 1952 68.8 14785584 11367. ## 2 Canada Americas 1957 70.0 17010154 12490. ## 3 Canada Americas 1962 71.3 18985849 13462. ## 4 Canada Americas 1967 72.1 20819767 16077. ## 5 Canada Americas 1972 72.9 22284500 18971. ## 6 Canada Americas 1977 74.2 23796400 22091. ## 7 Canada Americas 1982 75.8 25201900 22899. ## 8 Canada Americas 1987 76.9 26549700 26627. ## 9 Canada Americas 1992 78.0 28523502 26343. ## 10 Canada Americas 1997 78.6 30305843 28955. ## 11 Canada Americas 2002 79.8 31902268 33329. ## 12 Canada Americas 2007 80.7 33390141 36319. ‚Ä¶ versus when we assign the output to a new variable, or overwritting one that already exists. ## store the output as an R object my_precious &lt;- my_gap %&gt;% filter(country == &quot;Canada&quot;) 5.2 Use mutate() to add new variables Imagine we wanted to recover each country‚Äôs GDP. After all, the Gapminder data has a variable for population and GDP per capita. Let‚Äôs multiply them together to get the GDP of the whole country. The mutate() function defines and inserts new variables into a data frame/tibble. You can refer to existing variables by name. my_gap %&gt;% mutate(gdp = pop * gdpPercap) ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap gdp ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. ## 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. ## 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. ## 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. ## 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. ## 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. ## 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. ## # ‚Ä¶ with 1,694 more rows If you don‚Äôt want to add a new column to your tibble, you can use transmute(). It works just like mutate() except it only keeps the column(s) you specify. Let‚Äôs save our output in a new tibble called gap_gdp. Recall that saving the return of functions generally suppresses printing to the console. If you want to see the output, either type or print the variable. gap_gdp &lt;- my_gap %&gt;% transmute(country, gdp = pop * gdpPercap) gap_gdp # or use print(gap_gdp) ## # A tibble: 1,704 x 2 ## country gdp ## &lt;fct&gt; &lt;dbl&gt; ## 1 Afghanistan 6567086330. ## 2 Afghanistan 7585448670. ## 3 Afghanistan 8758855797. ## 4 Afghanistan 9648014150. ## 5 Afghanistan 9678553274. ## 6 Afghanistan 11697659231. ## 7 Afghanistan 12598563401. ## 8 Afghanistan 11820990309. ## 9 Afghanistan 10595901589. ## 10 Afghanistan 14121995875. ## # ‚Ä¶ with 1,694 more rows Hmmm‚Ä¶ those GDP numbers are almost uselessly large and abstract. Consider the advice of Randall Munroe of xkcd: One thing that bothers me is large numbers presented without context‚Ä¶ ‚ÄòIf I added a zero to this number, would the sentence containing it mean something different to me?‚Äô If the answer is ‚Äòno,‚Äô maybe the number has no business being in the sentence in the first place.&quot; Maybe it would be more meaningful to consumers of my tables and figures to stick with GDP per capita. But what if I reported GDP per capita, relative to some benchmark country. Since Canada is my home country, I‚Äôll go with that. I need to create a new variable that is gdpPercap divided by Canadian gdpPercap, taking care that I always divide two numbers that pertain to the same year. Here is what we need to do: Filter down to the rows for Canada. Create a new temporary variable in my_gap: Extract the gdpPercap variable from the Canadian data. Replicate it once per country in the dataset, so it has the right length. Divide raw gdpPercap by this Canadian figure. Discard the temporary variable of replicated Canadian gdpPercap. ctib &lt;- my_gap %&gt;% filter(country == &quot;Canada&quot;) ## this is a semi-dangerous way to add this variable ## I&#39;d prefer to join on year, but we haven&#39;t covered joins yet my_gap &lt;- my_gap %&gt;% mutate(tmp = rep(ctib$gdpPercap, nlevels(country)), gdpPercapRel = gdpPercap / tmp, tmp = NULL) Note that, mutate() builds new variables sequentially so you can reference earlier ones (like tmp) when defining later ones (like gdpPercapRel). Also, you can get rid of a variable by setting it to NULL. How could we sanity check that this worked? The Canadian values for gdpPercapRel better all be 1! my_gap %&gt;% filter(country == &quot;Canada&quot;) %&gt;% select(country, year, gdpPercapRel) ## # A tibble: 12 x 3 ## country year gdpPercapRel ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Canada 1952 1 ## 2 Canada 1957 1 ## 3 Canada 1962 1 ## 4 Canada 1967 1 ## 5 Canada 1972 1 ## 6 Canada 1977 1 ## 7 Canada 1982 1 ## 8 Canada 1987 1 ## 9 Canada 1992 1 ## 10 Canada 1997 1 ## 11 Canada 2002 1 ## 12 Canada 2007 1 I perceive Canada to be a ‚Äúhigh GDP‚Äù country, so I predict that the distribution of gdpPercapRel is located below 1, possibly even well below. Check your intuition! summary(my_gap$gdpPercapRel) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.007236 0.061648 0.171521 0.326659 0.446564 9.534690 The relative GDP per capita numbers are, in general, well below 1. We see that most of the countries covered by this dataset have substantially lower GDP per capita, relative to Canada, across the entire time period. Remember: Trust No One. Including (especially?) yourself. Always try to find a way to check that you‚Äôve done what meant to. Prepare to be horrified. 5.3 Use arrange() to row-order data The arrange() function reorders rows in a data frame/tibble. Imagine you wanted this data ordered by year then country, as opposed to by country then year. Remember, to save the output, you must assign it to a variable. my_gap %&gt;% arrange(year, country) ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap gdpPercapRel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 0.0686 ## 2 Albania Europe 1952 55.2 1282697 1601. 0.141 ## 3 Algeria Africa 1952 43.1 9279525 2449. 0.215 ## 4 Angola Africa 1952 30.0 4232095 3521. 0.310 ## 5 Argentina Americas 1952 62.5 17876956 5911. 0.520 ## 6 Australia Oceania 1952 69.1 8691212 10040. 0.883 ## 7 Austria Europe 1952 66.8 6927772 6137. 0.540 ## 8 Bahrain Asia 1952 50.9 120447 9867. 0.868 ## 9 Bangladesh Asia 1952 37.5 46886859 684. 0.0602 ## 10 Belgium Europe 1952 68 8730405 8343. 0.734 ## # ‚Ä¶ with 1,694 more rows Or maybe you want just the data from 2007, sorted on life expectancy? my_gap %&gt;% filter(year == 2007) %&gt;% arrange(lifeExp) ## # A tibble: 142 x 7 ## country continent year lifeExp pop gdpPercap gdpPercapRel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Swaziland Africa 2007 39.6 1.13e6 4513. 0.124 ## 2 Mozambique Africa 2007 42.1 2.00e7 824. 0.0227 ## 3 Zambia Africa 2007 42.4 1.17e7 1271. 0.0350 ## 4 Sierra Leone Africa 2007 42.6 6.14e6 863. 0.0237 ## 5 Lesotho Africa 2007 42.6 2.01e6 1569. 0.0432 ## 6 Angola Africa 2007 42.7 1.24e7 4797. 0.132 ## 7 Zimbabwe Africa 2007 43.5 1.23e7 470. 0.0129 ## 8 Afghanistan Asia 2007 43.8 3.19e7 975. 0.0268 ## 9 Central African Repub‚Ä¶ Africa 2007 44.7 4.37e6 706. 0.0194 ## 10 Liberia Africa 2007 45.7 3.19e6 415. 0.0114 ## # ‚Ä¶ with 132 more rows Oh, you‚Äôd like to sort on life expectancy in descending order? Then use desc(). my_gap %&gt;% filter(year == 2007) %&gt;% arrange(desc(lifeExp)) ## # A tibble: 142 x 7 ## country continent year lifeExp pop gdpPercap gdpPercapRel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. 0.872 ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. 1.09 ## 3 Iceland Europe 2007 81.8 301931 36181. 0.996 ## 4 Switzerland Europe 2007 81.7 7554661 37506. 1.03 ## 5 Australia Oceania 2007 81.2 20434176 34435. 0.948 ## 6 Spain Europe 2007 80.9 40448191 28821. 0.794 ## 7 Sweden Europe 2007 80.9 9031088 33860. 0.932 ## 8 Israel Asia 2007 80.7 6426679 25523. 0.703 ## 9 France Europe 2007 80.7 61083916 30470. 0.839 ## 10 Canada Americas 2007 80.7 33390141 36319. 1 ## # ‚Ä¶ with 132 more rows I advise that your analyses NEVER rely on rows or variables being in a specific order. But it‚Äôs still true that human beings write the code and the interactive development process can be much nicer if you reorder the rows of your data as you go along. Also, once you are preparing tables for human eyeballs, it is imperative that you step up and take control of row order. 5.4 Use rename() to rename variables When I started programming, I was a camelCase person, but now I‚Äôm all about snake_case. Let‚Äôs rename some variables! my_gap %&gt;% rename(life_exp = lifeExp, gdp_percap = gdpPercap, gdp_percap_rel = gdpPercapRel) ## # A tibble: 1,704 x 7 ## country continent year life_exp pop gdp_percap gdp_percap_rel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 0.0686 ## 2 Afghanistan Asia 1957 30.3 9240934 821. 0.0657 ## 3 Afghanistan Asia 1962 32.0 10267083 853. 0.0634 ## 4 Afghanistan Asia 1967 34.0 11537966 836. 0.0520 ## 5 Afghanistan Asia 1972 36.1 13079460 740. 0.0390 ## 6 Afghanistan Asia 1977 38.4 14880372 786. 0.0356 ## 7 Afghanistan Asia 1982 39.9 12881816 978. 0.0427 ## 8 Afghanistan Asia 1987 40.8 13867957 852. 0.0320 ## 9 Afghanistan Asia 1992 41.7 16317921 649. 0.0246 ## 10 Afghanistan Asia 1997 41.8 22227415 635. 0.0219 ## # ‚Ä¶ with 1,694 more rows I did NOT assign the post-rename object back to my_gap because that would make the chunks in this tutorial harder to copy/paste and run out of order. In real life, I would probably assign this back to my_gap, in a data preparation script, and proceed with the new variable names. 5.4.1 Use select() to rename and reposition variables You‚Äôve seen simple uses of select(). There are two tricks you might enjoy: select() can rename the variables you request to keep. select() can be used with everything() to hoist a variable up to the front of the tibble. my_gap %&gt;% filter(country == &quot;Burundi&quot;, year &gt; 1996) %&gt;% select(yr = year, lifeExp, gdpPercap) %&gt;% select(gdpPercap, everything()) ## # A tibble: 3 x 3 ## gdpPercap yr lifeExp ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 463. 1997 45.3 ## 2 446. 2002 47.4 ## 3 430. 2007 49.6 everything() is one of several helpers for variable selection. Read the documentation to see the rest. 5.5 Perform tasks on subsets with group_by() I have found collaborators love to ask seemingly innocuous questions like, ‚Äúwhich country experienced the sharpest 5-year drop in life expectancy?‚Äù. In fact, that is a totally natural question to ask. But if you are using a language that doesn‚Äôt know about data, it‚Äôs an incredibly annoying question to answer. dplyr offers powerful tools to solve this class of problem. group_by() adds extra structure to your dataset ‚Äì grouping information ‚Äì which lays the groundwork for computations within the groups. summarize() takes a dataset with \\(n\\) observations, computes requested summaries, and returns a dataset with 1 observation. Window functions take a dataset with \\(n\\) observations and return a dataset with \\(n\\) observations. You can also do very general computations on your groups with do(). Combined with the verbs you already know, these new tools allow you to solve an extremely diverse set of problems with relative ease. 5.5.1 Counting Let‚Äôs start with simple counting. How many observations do we have per continent? The n() function counts the number of observations in a particular group. my_gap %&gt;% group_by(continent) %&gt;% summarize(n = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 Let us pause here to think about the tidyverse. You could get these same frequencies using table() from base R. table(gapminder$continent) ## ## Africa Americas Asia Europe Oceania ## 624 300 396 360 24 str(table(gapminder$continent)) ## &#39;table&#39; int [1:5(1d)] 624 300 396 360 24 ## - attr(*, &quot;dimnames&quot;)=List of 1 ## ..$ : chr [1:5] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; ... But the object of class table that is returned makes downstream computation a bit fiddlier than we would like. For example, it‚Äôs too bad the continent levels come back only as names and not as a proper factor, with the original set of levels. The tally() function is a convenient function that counts rows. my_gap %&gt;% group_by(continent) %&gt;% tally() ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 The count() function is an even more convenient function that does both grouping and counting. my_gap %&gt;% count(continent) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 What if we wanted to add the number of unique countries for each continent? You can compute multiple summaries inside summarize(). Use the n_distinct() function to count the number of distinct countries within each continent. my_gap %&gt;% group_by(continent) %&gt;% summarize(n = n(), n_countries = n_distinct(country)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 3 ## continent n n_countries ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 Africa 624 52 ## 2 Americas 300 25 ## 3 Asia 396 33 ## 4 Europe 360 30 ## 5 Oceania 24 2 5.5.2 General summarization The functions you‚Äôll apply within summarize() include classical statistical summaries, like mean(), median(), var(), sd(), mad(), IQR(), min(), and max(). Remember they are functions that take \\(n\\) inputs and distill them down into 1 output. Although this may be statistically ill-advised, let‚Äôs compute the average life expectancy by continent. my_gap %&gt;% group_by(continent) %&gt;% summarize(avg_lifeExp = mean(lifeExp)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## continent avg_lifeExp ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 48.9 ## 2 Americas 64.7 ## 3 Asia 60.1 ## 4 Europe 71.9 ## 5 Oceania 74.3 summarize_at() applies the same summary function(s) to multiple variables. Let‚Äôs compute average and median life expectancy and GDP per capita by continent by year ‚Ä¶ but only for 1952 and 2007. my_gap %&gt;% filter(year %in% c(1952, 2007)) %&gt;% group_by(continent, year) %&gt;% summarize_at(vars(lifeExp, gdpPercap), funs(mean, median)) ## # A tibble: 10 x 6 ## # Groups: continent [5] ## continent year lifeExp_mean gdpPercap_mean lifeExp_median gdpPercap_median ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 1952 39.1 1253. 38.8 987. ## 2 Africa 2007 54.8 3089. 52.9 1452. ## 3 Americas 1952 53.3 4079. 54.7 3048. ## 4 Americas 2007 73.6 11003. 72.9 8948. ## 5 Asia 1952 46.3 5195. 44.9 1207. ## 6 Asia 2007 70.7 12473. 72.4 4471. ## 7 Europe 1952 64.4 5661. 65.9 5142. ## 8 Europe 2007 77.6 25054. 78.6 28054. ## 9 Oceania 1952 69.3 10298. 69.3 10298. ## 10 Oceania 2007 80.7 29810. 80.7 29810. Let‚Äôs focus just on Asia. What are the minimum and maximum life expectancies seen by year? my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% group_by(year) %&gt;% summarize(min_lifeExp = min(lifeExp), max_lifeExp = max(lifeExp)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 12 x 3 ## year min_lifeExp max_lifeExp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1952 28.8 65.4 ## 2 1957 30.3 67.8 ## 3 1962 32.0 69.4 ## 4 1967 34.0 71.4 ## 5 1972 36.1 73.4 ## 6 1977 31.2 75.4 ## 7 1982 39.9 77.1 ## 8 1987 40.8 78.7 ## 9 1992 41.7 79.4 ## 10 1997 41.8 80.7 ## 11 2002 42.1 82 ## 12 2007 43.8 82.6 Of course it would be much more interesting to see which country contributed these extreme observations. Is the minimum (maximum) always coming from the same country?We will tackle this with window functions shortly. 5.5.3 Computing with group-wise summaries Don‚Äôt worry too much about this section if all the data wrangling is starting to become overwhelming ‚Äì it‚Äôs mainly here for the curious. Let‚Äôs make a new variable that is the years of life expectancy gained (lost) relative to 1952, for each individual country. We group by country and use mutate() to make a new variable. The first() function extracts the first value from a vector. Notice that first() is operating on the vector of life expectancies within each country group. new_var &lt;- my_gap %&gt;% group_by(country) %&gt;% select(country, year, lifeExp) %&gt;% mutate(lifeExp_gain = lifeExp - first(lifeExp)) %&gt;% filter(year &lt; 1963) new_var ## # A tibble: 426 x 4 ## # Groups: country [142] ## country year lifeExp lifeExp_gain ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 28.8 0 ## 2 Afghanistan 1957 30.3 1.53 ## 3 Afghanistan 1962 32.0 3.20 ## 4 Albania 1952 55.2 0 ## 5 Albania 1957 59.3 4.05 ## 6 Albania 1962 64.8 9.59 ## 7 Algeria 1952 43.1 0 ## 8 Algeria 1957 45.7 2.61 ## 9 Algeria 1962 48.3 5.23 ## 10 Angola 1952 30.0 0 ## # ‚Ä¶ with 416 more rows Within country, we take the difference between life expectancy in year \\(i\\) and life expectancy in 1952. Therefore we always see zeroes for 1952 and, for most countries, a sequence of positive and increasing numbers. 5.5.4 Window functions (OPTIONAL) Window functions take \\(n\\) inputs and give back \\(n\\) outputs. Furthermore, the output depends on all the values. So rank() is a window function but sum() is not. Here we use window functions based on ranks and offsets. Let‚Äôs revisit the worst and best life expectancies in Asia over time, but retaining info about which country contributes these extreme values. my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% group_by(year) %&gt;% filter(min_rank(desc(lifeExp)) &lt; 2 | min_rank(lifeExp) &lt; 2) %&gt;% arrange(year) ## # A tibble: 24 x 3 ## # Groups: year [12] ## year country lifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Afghanistan 28.8 ## 2 1952 Israel 65.4 ## 3 1957 Afghanistan 30.3 ## 4 1957 Israel 67.8 ## 5 1962 Afghanistan 32.0 ## 6 1962 Israel 69.4 ## 7 1967 Afghanistan 34.0 ## 8 1967 Japan 71.4 ## 9 1972 Afghanistan 36.1 ## 10 1972 Japan 73.4 ## # ‚Ä¶ with 14 more rows We see that (min = Afghanistan, max = Japan) is the most frequent result, but Cambodia and Israel pop up at least once each as the min or max, respectively. That table should make you impatient for our upcoming work on tidying and reshaping data! Wouldn‚Äôt it be nice to have one row per year? How did that actually work? First, I store and view a partial that leaves off the filter() statement. All of these operations should be familiar. asia &lt;- my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% group_by(year) asia ## # A tibble: 396 x 3 ## # Groups: year [12] ## year country lifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Afghanistan 28.8 ## 2 1957 Afghanistan 30.3 ## 3 1962 Afghanistan 32.0 ## 4 1967 Afghanistan 34.0 ## 5 1972 Afghanistan 36.1 ## 6 1977 Afghanistan 38.4 ## 7 1982 Afghanistan 39.9 ## 8 1987 Afghanistan 40.8 ## 9 1992 Afghanistan 41.7 ## 10 1997 Afghanistan 41.8 ## # ‚Ä¶ with 386 more rows Now we apply a window function: min_rank(). Since asia is grouped by year, min_rank() operates within mini-datasets, each for a specific year. Applied to the variable lifeExp, min_rank() returns the rank of each country‚Äôs observed life expectancy. FYI, the min part just specifies how ties are broken. Here is an explicit peek at these within-year life expectancy ranks, in both the (default) ascending and descending order. If you specify rank(), ties will be denoted by .5. For instance: x &lt;- c(1, 2, 3, 3, 4) min_rank(x) ## [1] 1 2 3 3 5 rank(x) ## [1] 1.0 2.0 3.5 3.5 5.0 For concreteness, I use mutate() to actually create these variables, even though I dropped this in the solution above. Let‚Äôs look at a bit of that. asia %&gt;% mutate(le_rank = min_rank(lifeExp), le_desc_rank = min_rank(desc(lifeExp))) %&gt;% filter(country %in% c(&quot;Afghanistan&quot;, &quot;Japan&quot;, &quot;Thailand&quot;), year &gt; 1995) ## # A tibble: 9 x 5 ## # Groups: year [3] ## year country lifeExp le_rank le_desc_rank ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1997 Afghanistan 41.8 1 33 ## 2 2002 Afghanistan 42.1 1 33 ## 3 2007 Afghanistan 43.8 1 33 ## 4 1997 Japan 80.7 33 1 ## 5 2002 Japan 82 33 1 ## 6 2007 Japan 82.6 33 1 ## 7 1997 Thailand 67.5 12 22 ## 8 2002 Thailand 68.6 12 22 ## 9 2007 Thailand 70.6 12 22 Afghanistan tends to present 1‚Äôs in the le_rank variable, Japan tends to present 1‚Äôs in the le_desc_rank variable and other countries, like Thailand, present less extreme ranks. You can understand the original filter() statement now: filter(min_rank(desc(lifeExp)) &lt; 2 | min_rank(lifeExp) &lt; 2) These two sets of ranks are formed on-the-fly, within year group, and filter() retains rows with rank less than 2, which means ‚Ä¶ the row with rank = 1. Since we do for ascending and descending ranks, we get both the min and the max. If we had wanted just the min OR the max, an alternative approach using top_n() would have worked. my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% arrange(year) %&gt;% group_by(year) %&gt;% #top_n(1, wt = lifeExp) ## gets the max top_n(1, wt = desc(lifeExp)) ## gets the min ## # A tibble: 12 x 3 ## # Groups: year [12] ## year country lifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Afghanistan 28.8 ## 2 1957 Afghanistan 30.3 ## 3 1962 Afghanistan 32.0 ## 4 1967 Afghanistan 34.0 ## 5 1972 Afghanistan 36.1 ## 6 1977 Cambodia 31.2 ## 7 1982 Afghanistan 39.9 ## 8 1987 Afghanistan 40.8 ## 9 1992 Afghanistan 41.7 ## 10 1997 Afghanistan 41.8 ## 11 2002 Afghanistan 42.1 ## 12 2007 Afghanistan 43.8 5.6 Introduction to visualization (OPTIONAL) Although we will get into more serious plotting in future chapters, I want to give you a taste of the excitement to come. Here, we will get sampling of the almighty ggplot2 package. Let‚Äôs look at a few basic examples. If you want to compare continuous data with a few categories, either a bar plot or box plot would be a good bet. Let‚Äôs look at the 1952 gapminder data. dat.1952 &lt;- my_gap %&gt;% filter(year == 1952) ggplot(data = dat.1952, aes(x=continent, y=lifeExp)) + geom_dotplot(binaxis = &quot;y&quot;, stackdir = &quot;center&quot;, dotsize = 0.5) + geom_boxplot(alpha=0.3) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. Look at this figure, what would you comment on the mean and variance of the data? Have you identified any outliers? Now suppose we had no idea about what our data looks like, but we want to check the relationship between 2 continuous variables. A great place to start would be a scatter plot: ggplot(data = dat.1952, aes(x = gdpPercap, y = lifeExp)) + geom_point() The scatter plot shows an upwards relationship‚Äîwe will quantify this correlation in a future chapter. To make gdpPercap look more like a straight line, we can plot it in a base 10 log scale using the function scale_x_log10(). While we‚Äôre at it, let‚Äôs also add colours to label different continents. ggplot(data = dat.1952, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color = continent)) + scale_x_log10() We can also remove the grey background by setting the theme: ggplot(data = dat.1952, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color = continent)) + scale_x_log10() + theme_classic() Don‚Äôt worry too much about figures right now. We will cover data visualization in much more depth in future lessons. 5.7 Comprehensive practice So let‚Äôs answer a ‚Äúsimple‚Äù question: which country experienced the sharpest 5-year drop in life expectancy (le)? Recall that this excerpt of the gapminder data only has data every five years, e.g. for 1952, 1957, etc. So this really means looking at life expectancy changes between adjacent timepoints. At this point, the question is just too easy to answer, so find life expectancy by continent while we‚Äôre at it. my_gap %&gt;% select(country, year, continent, lifeExp) %&gt;% group_by(continent, country) %&gt;% # within country, take (lifeExp in year i) - (lifeExp in year i - 1) # positive means lifeExp went up, negative means it went down mutate(le_delta = lifeExp - lag(lifeExp)) %&gt;% # within country, retain the worst lifeExp change = smallest or most negative summarize(worst_le_delta = min(le_delta, na.rm = TRUE)) %&gt;% # within continent, retain the row with the lowest worst_le_delta top_n(-1, wt = worst_le_delta) %&gt;% arrange(worst_le_delta) ## `summarise()` regrouping output by &#39;continent&#39; (override with `.groups` argument) ## # A tibble: 5 x 3 ## # Groups: continent [5] ## continent country worst_le_delta ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Africa Rwanda -20.4 ## 2 Asia Cambodia -9.10 ## 3 Americas El Salvador -1.51 ## 4 Europe Montenegro -1.46 ## 5 Oceania Australia 0.170 Now this data is interesting. Take a look at the life expectancy in Rwanda in 1987 and in 1992. gapminder %&gt;% select(country, year, lifeExp) %&gt;% filter(year == 1987 | year == 1992, country == &#39;Rwanda&#39;) ## # A tibble: 2 x 3 ## country year lifeExp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Rwanda 1987 44.0 ## 2 Rwanda 1992 23.6 Ponder the real-life implications of this output for a while. What you‚Äôre seeing here is genocide in dry statistics on average life expectancy. 5.8 Data wrangling summary Wow, we covered a lot of data wrangling! Don‚Äôt wory if you don‚Äôt understand everything the first time around. Programming takes practice, and practice makes perfect. Here are some general remarks: Break your code into pieces starting at the top, and inspect the intermediate results. That‚Äôs certainly how I was able to write such a thing. The commands presented in this lab do not leap fully formed out of anyone‚Äôs forehead‚Äîthey are built up gradually, with lots of errors and refinements along the way. If your statements are difficult to read, by all means break it into pieces and make some intermediate objects. Your code should be easy to read and write when you‚Äôre done. The functions presented here should cover most of your basic data wrangling needs. If you ever need to do something more complicated, search it up! Although I have programmed for many years, I still need to do a quick Google search for documentation and StackOverflow solutions. "],["central-limit-theorem.html", "6 Central limit theorem 6.1 Central limit theorem for means 6.2 Central limit theorem for probability", " 6 Central limit theorem In this chapter, we will investigate the intuition behind the central limit theorem (CLT). In short, the CLT states that if you have a bunch of samples and plotted the distribution of means (not individual observations!), the distribution would look normal regardless of the population distribution. This theorem is incredibly powerful, and we will explore how it applies to data analysis in future labs. 6.1 Central limit theorem for means As always, let‚Äôs load the tidyverse. library(tidyverse) Many observed quantities follow normal distribution. Imagine we have a population following normal distribution that has a mean of 10 and standard deviation of 2. If we draw samples from it, are we able to estimate its mean? In this example, we will use rnorm, a random number generate for this simulation. rnorm returns a vector of random numbers from the specifies distribution. samp10 &lt;- rnorm(10, mean=10, sd=2) # generate 10 random numbers samp10 ## [1] 14.555767 8.988130 12.218985 9.219520 8.155603 9.022171 9.750132 ## [8] 8.658725 9.715076 10.666202 mean(samp10) ## [1] 10.09503 Apparently the mean is not 10, but it is pretty close. This should make sense; when we draw a sample to estimate the mean, we may get very close to the desired ‚Äútrue mean‚Äù, but we also expect some error. What if I repeat the estimation above with 10 samples? That is, what if each time I use 10 randomly generated numbers to estimate the mean, and I do this for many times? means10 &lt;- as.vector(NA) # Doing experiments for 10000 times for (i in 1:10000) { # Each time I draw 10 random numbers from the normal distribution samp10 &lt;- rnorm(10, mean=10, sd=2) # I calculate the mean of these 10 numbers and record it means10[i] &lt;- mean(samp10) } plot(density(means10)) What does this figure mean? Of the 10000 estimations that we did, most estimations were very close to 10. The probability to overestimate and underestimate decreases as the estimation deviates from 10, our ‚Äútrue mean‚Äù. This time, let‚Äôs make estimations with a larger sample size, say 100 and 1000. Will a larger sample size reduce the error of estimation? Don‚Äôt worry too much about what the code is doing‚Äîjust focus on the figure. means100 &lt;- as.vector(NA) means1000 &lt;- as.vector(NA) # Doing experiments for 10000 times for (i in 1:10000) { # Each time I draw 10 random numbers from the normal distribution samp100 &lt;- rnorm(100, mean=10, sd=2) # I calculate the mean of these 10 numbers and record it means100[i] &lt;- mean(samp100) samp1000 &lt;- rnorm(1000, mean=10, sd=2) # I calculate the mean of these 10 numbers and record it means1000[i] &lt;- mean(samp1000) } df &lt;- rbind( data.frame(means = means10, sample_size = &quot;n=10&quot;), data.frame(means = means100, sample_size = &quot;n=100&quot;), data.frame(means = means1000, sample_size = &quot;n=1000&quot;) ) ggplot(df, aes(x = means, fill = sample_size)) + geom_density(alpha = 0.3) + theme_classic() With a larger sample size, your estimation for the mean will have a smaller error. 6.2 Central limit theorem for probability Now I would like to estimate the probability of getting a ‚Äúhead‚Äù of when I flip a coin. Each time I flip a coin, if I end up with a ‚Äúhead‚Äù, I record it as a 1. If I get a ‚Äútail‚Äù, I will record it as a 0. If I flip the coin for 10 times where I have 6 ‚Äúheads‚Äù and 4 ‚Äútails‚Äù, the frequency of getting ‚Äúheads‚Äù would be 6/10 = 0.6. If I flip the coin for a sufficiently large amount of times, we would like to expect the frequency to approach the theoretical 0.5. Is this the case? means10 &lt;- as.vector(NA) means100 &lt;- as.vector(NA) means1000 &lt;- as.vector(NA) means10000 &lt;- as.vector(NA) for (i in 1:10000) { sample10 &lt;- sample(c(1,0), prob = c(0.5, 0.5), replace = TRUE, size = 10) means10[i] &lt;- sum(sample10) / 10 sample100 &lt;- sample(c(1,0), prob = c(0.5, 0.5), replace = TRUE, size = 100) means100[i] &lt;- sum(sample100) / 100 sample1000 &lt;- sample(c(1,0), prob = c(0.5, 0.5), replace = TRUE, size = 1000) means1000[i] &lt;- sum(sample1000) / 1000 sample10000 &lt;- sample(c(1,0), prob = c(0.5, 0.5), replace = TRUE, size = 10000) means10000[i] &lt;- sum(sample10000) / 10000 } df &lt;- rbind( data.frame(means = means10, sample_size = &quot;10 trials&quot;), data.frame(means = means100, sample_size = &quot;100 trials&quot;), data.frame(means = means1000, sample_size = &quot;1000 trials&quot;), data.frame(means = means10000, sample_size = &quot;10000 trials&quot;) ) ggplot(df, aes(x = means, fill = sample_size)) + geom_density(alpha = 0.3) + theme_classic() Can you explain the pattern we observed with sample size of 10? How about the height and width of other curves? What conclusion could we draw? This example may appear intuitive: of course when you flip the coin for many times, you will most likely get a 50% chance of ‚Äúheads‚Äù. How about an ‚Äúuneven‚Äù coin that preferably lands with a ‚Äúhead‚Äù with 75% chance? In this case we change the prob in the sample() function with this new probability. means10 &lt;- as.vector(NA) means100 &lt;- as.vector(NA) means1000 &lt;- as.vector(NA) means10000 &lt;- as.vector(NA) for (i in 1:10000) { sample10 &lt;- sample(c(1,0), prob = c(0.75, 0.25), replace = TRUE, size = 10) means10[i] &lt;- sum(sample10) / 10 sample100 &lt;- sample(c(1,0), prob = c(0.75, 0.25), replace = TRUE, size = 100) means100[i] &lt;- sum(sample100) / 100 sample1000 &lt;- sample(c(1,0), prob = c(0.75, 0.25), replace = TRUE, size = 1000) means1000[i] &lt;- sum(sample1000) / 1000 sample10000 &lt;- sample(c(1,0), prob = c(0.75, 0.25), replace = TRUE, size = 10000) means10000[i] &lt;- sum(sample10000) / 10000 } df &lt;- rbind( data.frame(means = means10, sample_size = &quot;10 numbers&quot;), data.frame(means = means100, sample_size = &quot;100 numbers&quot;), data.frame(means = means1000, sample_size = &quot;1000 numbers&quot;), data.frame(means = means10000, sample_size = &quot;10000 numbers&quot;) ) library(ggplot2) ggplot(df, aes(x = means, fill = sample_size)) + geom_density(alpha = 0.3) + theme_classic() Indeed, the peaks converged again and shifted to the new position of 0.75. As you can see, the bigger your sample size, the less variability there is, and the more the distribution looks like a normal distribution. More precisely, the bigger your sample size, the distribution of the sample means will be normally distributed, even if the population is not normally distributed. A good rule of ‚Äúsufficiently large sample size‚Äù is n ‚â• 30. This example shows the power of the CLT‚Äîit allows us to predict a sampling distribution regardless of the original population. NOTE: The CLT says NOTHING about the individual sample points themselves. Remember our original data data points are either 0 or 1. However, the mean is a continuous variable. "],["basic-statistical-tests.html", "7 Basic statistical tests 7.1 Getting ready 7.2 Student‚Äôs t-test 7.3 Chi-squared test 7.4 Visualizing data distributions (OPTIONAL)", " 7 Basic statistical tests R contains extremely powerful tools for data science. These tools are either built-in or available from packages. Thoughout this section we hope to demonstrate best practices organizing, analyzing, and visualizing data in R. 7.1 Getting ready We will again work with the gapminder dataset. Let‚Äôs load the usual packages. library(gapminder) library(tidyverse) Now that we‚Äôve loaded our packages, let‚Äôs briefly re-explore gapminder. When you get a new dataset, your first action as a good data scientist should be to explore it. str(gapminder) ## tibble [1,704 √ó 6] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... ## $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... ## $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... ## $ gdpPercap: num [1:1704] 779 821 853 836 740 ... names(gapminder) ## [1] &quot;country&quot; &quot;continent&quot; &quot;year&quot; &quot;lifeExp&quot; &quot;pop&quot; &quot;gdpPercap&quot; head(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. Note that we have 1,704 observations (rows). The variables country and continent are considered as ‚Äúfactor‚Äù, which is a catagorical data. ‚Äúfactor‚Äù is useful in that you can deal with a finite number of discrete values. We can use levels() to ask what catagories there are. # there are 142 countries, but for the sake of space, we&#39;re only checking the first 5 head(levels(gapminder$country)) ## [1] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;Angola&quot; &quot;Argentina&quot; ## [6] &quot;Australia&quot; levels(gapminder$continent) ## [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; 7.2 Student‚Äôs t-test Let‚Äôs start by asking the mean life expectancy of the continents in 1952. gapminder %&gt;% filter(year == 1952) %&gt;% group_by(continent) %&gt;% summarise(mean(lifeExp)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## continent `mean(lifeExp)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 39.1 ## 2 Americas 53.3 ## 3 Asia 46.3 ## 4 Europe 64.4 ## 5 Oceania 69.3 The life expectancy of Europe is about 64.4 years. It seems close to 65, the standard age often associated with retirement in Canada (and when full pension benefits become available!). Is this statistically significantly different from 65 years? To answer this question, we can use a one-sample t-test. We will test the sample (life expectancy measured in Europe in 1952) against our null hypothesis that there is no significant difference between the life expectancy in Europe (64.4 years) and 65 years. # selection method 1: base R method1 &lt;- gapminder$lifeExp[gapminder$continent==&#39;Europe&#39; &amp; gapminder$year==1952] # selection method 2: use dplyr method2 &lt;- gapminder %&gt;% filter(continent == &#39;Europe&#39;, year == 1952) %&gt;% select(lifeExp) # checking if these two methods give identical outputs identical(method1, method2$lifeExp) ## [1] TRUE # let&#39;s rename the variable for interpretability Euro.life.1952 &lt;- method1 t.test(Euro.life.1952, mu = 65, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: Euro.life.1952 ## t = -0.50931, df = 29, p-value = 0.6144 ## alternative hypothesis: true mean is not equal to 65 ## 95 percent confidence interval: ## 62.03323 66.78377 ## sample estimates: ## mean of x ## 64.4085 Notice that p-value is 0.6144. Usually, we choose the alpha (\\(\\alpha\\)) to be 0.05. Since p &lt; \\(\\alpha\\), we conclude that the life expectancy of Europeans in 1952 doesn‚Äôt give us evidence indicating a difference in life expectancy from 65. We can also plot this: ggplot() + geom_density(aes(Euro.life.1952)) + geom_vline(xintercept = 65) Note: This is NOT a figure you would include in an academic paper as the quality is quite low. We‚Äôre visualizing this just so we have a better idea of what‚Äôs going on with the data. The non-parametric test alternative to one-sample t-test is Wilcoxon signed-rank test. wilcox.test(Euro.life.1952, mu=65) ## ## Wilcoxon signed rank test ## ## data: Euro.life.1952 ## V = 238, p-value = 0.9193 ## alternative hypothesis: true location is not equal to 65 The non-parametric test gave us the same conclusion. CAUTION: although we obtained the same results with both the parametric t-test and non-parametric signed-rank test, their use cases are VERY different. We prefer to use parametric tests because they give us more statistical power. Only use non-parametric tests with sample sizes less than 30 and if the data is not normally distributed. Does Asia and Africa differ in life expectancy in 1952? To compare two groups of data, we need a two-sample t-test. As.Af &lt;- gapminder %&gt;% filter(year==1952) %&gt;% filter(continent %in% c(&quot;Africa&quot;, &quot;Asia&quot;)) As.Af ## # A tibble: 85 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Algeria Africa 1952 43.1 9279525 2449. ## 3 Angola Africa 1952 30.0 4232095 3521. ## 4 Bahrain Asia 1952 50.9 120447 9867. ## 5 Bangladesh Asia 1952 37.5 46886859 684. ## 6 Benin Africa 1952 38.2 1738315 1063. ## 7 Botswana Africa 1952 47.6 442308 851. ## 8 Burkina Faso Africa 1952 32.0 4469979 543. ## 9 Burundi Africa 1952 39.0 2445618 339. ## 10 Cambodia Asia 1952 39.4 4693836 368. ## # ‚Ä¶ with 75 more rows We can plot this: # you don&#39;t need to explicitly declare data = As.Af and aes(x=continent, y=lifeExp) # just make sure your variables are in the correct order ggplot(As.Af, aes(continent, lifeExp)) + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, dotsize=0.65) + geom_boxplot(alpha=0.3) + labs(x=&#39;Continent&#39;, y=&#39;Life expectancy (yrs)&#39;) + theme_classic() ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. Here we want to run a two-sample t-test. Before we do that, we‚Äôll need to check if the 2 samples have the same variance. Recall that different t-tests assume different variances: If you assume equal variance, you would use Student‚Äôs t-test. If variances are unequal, use Welch‚Äôs t-test. A good rule of thumb is if the larger standard deviation (SD) divded by the smaller SD is less than 2 (SD(larger)/SD(smaller) &lt; 2), then you can assume equal variance. Alternatively, you can test for equal variances: library(car) # for Levene&#39;s test leveneTest(y = As.Af$lifeExp, group = As.Af$continent) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 12.514 0.0006644 *** ## 83 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since \\(p = 0.0006644 &lt; 0.05\\), the two samples have significantly different variances. Indeed, the width of the boxplots in the figure above suggested this difference. Because we have different variances, we need to use Welch‚Äôs t-test. By default, t.test() assumes unequal variance. If this wasn‚Äôt the case, we would add an additional argument called var.equal = FALSE to t.test(). t.test(lifeExp ~ continent, As.Af, alternative = &quot;two.sided&quot;) ## ## Welch Two Sample t-test ## ## data: lifeExp by continent ## t = -4.0599, df = 44.637, p-value = 0.0001952 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.741084 -3.616704 ## sample estimates: ## mean in group Africa mean in group Asia ## 39.13550 46.31439 The non-parametric test in this case would be the independent 2-group Mann-Whitney U Test. wilcox.test(lifeExp ~ continent, As.Af) ## ## Wilcoxon rank sum test with continuity correction ## ## data: lifeExp by continent ## W = 443, p-value = 0.0001857 ## alternative hypothesis: true location shift is not equal to 0 Next, let‚Äôs take a look at life expectancy in 2007: gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarise(meanlife = mean(lifeExp)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## continent meanlife ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 54.8 ## 2 Americas 73.6 ## 3 Asia 70.7 ## 4 Europe 77.6 ## 5 Oceania 80.7 Has the life expectancy in Africa changed to that in 1952? We can answer this question with a two-sample t-test. This time, we would like to match the countries. First, let‚Äôs generate a long data frame. Africa &lt;- gapminder %&gt;% filter(continent==&quot;Africa&quot;) %&gt;% select(country, year, lifeExp) head(Africa) ## # A tibble: 6 x 3 ## country year lifeExp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Algeria 1952 43.1 ## 2 Algeria 1957 45.7 ## 3 Algeria 1962 48.3 ## 4 Algeria 1967 51.4 ## 5 Algeria 1972 54.5 ## 6 Algeria 1977 58.0 Second, let‚Äôs look at how the life expectancy changed over the years. p &lt;- ggplot(data = Africa, aes(x = year, y = lifeExp)) + geom_point(aes(color = country)) + geom_line(aes(group = country, color=country)) show(p) Since the life expectancy in 1952 and 2007 look interesting, let‚Äôs visualize it: # selecting rows with years 1952 and 2007 Africa.1952.2007 &lt;- Africa %&gt;% filter(year %in% c(1952, 2007)) # plotting p &lt;- ggplot(data = Africa.1952.2007, aes(x=as.factor(year), y=lifeExp)) + geom_point(aes(color=country)) + geom_line(aes(group = country, color=country)) show(p) Most of the countries have improved, while a few have decreased life expectancy. Before testing this observation, we should reorganize our data into a nice (wide) shape. Africa.wide &lt;- spread(Africa, year, lifeExp) Africa.wide ## # A tibble: 52 x 13 ## country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Algeria 43.1 45.7 48.3 51.4 54.5 58.0 61.4 65.8 67.7 69.2 ## 2 Angola 30.0 32.0 34 36.0 37.9 39.5 39.9 39.9 40.6 41.0 ## 3 Benin 38.2 40.4 42.6 44.9 47.0 49.2 50.9 52.3 53.9 54.8 ## 4 Botswa‚Ä¶ 47.6 49.6 51.5 53.3 56.0 59.3 61.5 63.6 62.7 52.6 ## 5 Burkin‚Ä¶ 32.0 34.9 37.8 40.7 43.6 46.1 48.1 49.6 50.3 50.3 ## 6 Burundi 39.0 40.5 42.0 43.5 44.1 45.9 47.5 48.2 44.7 45.3 ## 7 Camero‚Ä¶ 38.5 40.4 42.6 44.8 47.0 49.4 53.0 55.0 54.3 52.2 ## 8 Centra‚Ä¶ 35.5 37.5 39.5 41.5 43.5 46.8 48.3 50.5 49.4 46.1 ## 9 Chad 38.1 39.9 41.7 43.6 45.6 47.4 49.5 51.1 51.7 51.6 ## 10 Comoros 40.7 42.5 44.5 46.5 48.9 50.9 52.9 54.9 57.9 60.7 ## # ‚Ä¶ with 42 more rows, and 2 more variables: `2002` &lt;dbl&gt;, `2007` &lt;dbl&gt; The wide data frame is to align the data from the same country to the same row, so that they have the same index when we call different columns. Now, we can run a paired t-test. Think about what we are testing in the code below. t.test(Africa.wide$&#39;2007&#39;, Africa.wide$&#39;1952&#39;, alternative=&quot;two.sided&quot;, paired=T) ## ## Paired t-test ## ## data: Africa.wide$&quot;2007&quot; and Africa.wide$&quot;1952&quot; ## t = 13.042, df = 51, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 13.25841 18.08267 ## sample estimates: ## mean of the differences ## 15.67054 Note in this process we didn‚Äôt check the variance. Is this a problem? Why or why not? Recall that the paired t-test is actually a one-sample t-test on paired differences. Similarly, we could again call wilcox.test to run the paired version. wilcox.test(Africa.wide$&#39;2007&#39;, Africa.wide$&#39;1952&#39;, paired=T) ## ## Wilcoxon signed rank test with continuity correction ## ## data: Africa.wide$&quot;2007&quot; and Africa.wide$&quot;1952&quot; ## V = 1369, p-value = 6.087e-10 ## alternative hypothesis: true location shift is not equal to 0 7.3 Chi-squared test 7.3.1 \\(\\chi^2\\) test for goodness-of-fit This section requires basic of knowledge of Mendelian genetics regarding dominant and recessive alleles. Recall that crossing two heterozygotes (Aa x Aa) produces offspring with dominant and recessive phenotypes with an expected ratio of 3:1. \\[\\begin{array}{c|cc} &amp; \\mathbf{A} &amp; \\mathbf{a} \\\\ \\hline \\mathbf{A} &amp; AA &amp; Aa \\\\ \\mathbf{a} &amp; Aa &amp; aa \\end{array}\\] Also recall that a dihybrid cross (AaBb x AaBb) produces offspring of 4 phenotypes with an expected ratio of 9:3:3:1. \\[\\begin{array}{c|cccc} &amp; \\mathbf{AB} &amp; \\mathbf{Ab} &amp; \\mathbf{aB} &amp; \\mathbf{ab} \\\\ \\hline \\mathbf{AB} &amp; AABB &amp; AABb &amp; AaBB &amp; AaBb \\\\ \\mathbf{Ab} &amp; AABb &amp; AAbb &amp; AaBb &amp; Aabb \\\\ \\mathbf{aB} &amp; AaBb &amp; AaBb &amp; aaBB &amp; aaBb \\\\ \\mathbf{ab} &amp; AaBb &amp; Aabb &amp; aaBb &amp; aabb \\end{array}\\] Now, let‚Äôs focus on Mendel‚Äôs data from his original paper: Mendel, Gregor. 1866. Versuche √ºber Plflanzenhybriden. Verhandlungen des naturforschenden Vereines in Br√ºnn, Bd. IV f√ºr das Jahr 1865, Abhandlungen, 3‚Äì47. In his experiment for seed color, the F2 generation produced 6022 yellow, and 2001 green seeds. Thus, the ratio of yellow:green was 3.01:1. Obviously, this ratio is not the exact theoretical ratio of 3:1. A meaningful question would be this: Is the discrepancy appeared because of random fluctuation, or is the observed ratio significantly different from 3:1? To examine whether the observed count fits a theoretical ratio, we will uses the \\(\\chi^2\\) test for goodness-of-fit. chisq.test(x = c(6022, 2001), # the observed data p = c(0.75, 0.25)) # the theoretical probability ## ## Chi-squared test for given probabilities ## ## data: c(6022, 2001) ## X-squared = 0.014999, df = 1, p-value = 0.9025 A p-value of 0.9025 suggested a good match of observed data with the theoretical values. That is, the differences are not significant. Let‚Äôs assume Mendel had observed a 1000 times larger number of seeds, with the same proportion. That is, 6,022,000 yellow and 2,001,000 green. Obviously this ratio is still 3.01:1. Would it still be a good fit for the theoretical value? chisq.test(x = c(6022000, 2001000), # The observed data, 1000 times larger p = c(0.75, 0.25)) # The theoretical probability ## ## Chi-squared test for given probabilities ## ## data: c(6022000, 2001000) ## X-squared = 14.999, df = 1, p-value = 0.0001076 This time, p = 0.0001076, suggesting a significant deviation from the theoretical ratio. As an extension of CLT, when you sample a large enough sample, the ratio of the categories should approach the true value. In other words, \\(\\chi^2\\) test should be increasingly sensitive to small deviations when the sample size increases. 7.3.2 \\(\\chi^2\\) test for independence In another experiment, Mendel looked at two pairs of phenotypes of the F2 generation of a double-heterozygote. Below is what he saw: 315 round and yellow, 101 wrinkled and yellow, 108 round and green, 32 wrinkled and green. Before we examine the 9:3:3:1 ratio, we want to ask if the two loci are independent of each other. That is, will being yellow increase or decrease the chance of being round (and vice versa)? To run the \\(\\chi^2\\) test for independence, we will first need a contingency table. This time we will manually build a data frame for this purpose. Mendel2loci &lt;- data.frame( yellow = c(315, 101), green = c(108, 32) ) # adding rownames rownames(Mendel2loci) &lt;- c(&quot;round&quot;, &quot;wrinkled&quot;) # printing the data frame Mendel2loci ## yellow green ## round 315 108 ## wrinkled 101 32 Next we will run the \\(\\chi^2\\) test. The null hypothesis is that the distribution is independent of the groups. chisq.test(Mendel2loci) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: Mendel2loci ## X-squared = 0.051332, df = 1, p-value = 0.8208 The p-value of 0.8208, so we cannot reject the null hypothesis. Therefore, we should consider the two traits as independent. Again, we could try to test for its goodness-of-fit. This time we will not need a contingency table. chisq.test(x = c(315, 101, 108, 32), p = c(9/16, 3/16, 3/16, 1/16)) ## ## Chi-squared test for given probabilities ## ## data: c(315, 101, 108, 32) ## X-squared = 0.47002, df = 3, p-value = 0.9254 Thus, the data fits the 9:3:3:1 ratio well. 7.3.3 \\(\\chi^2\\) test for homogeneity The homogeneity test works the same way as an independence test ‚Äì the only difference lies in the experimentally design. A test for independence draws samples from the same population, and look at two or more categorical variables. A test for homogeneity draws sample from 2 or more subgroups of the population, and looks at another categorical variable. The subgroup itself serves as a variable. Recall the hypotheses for the test for homogeneity: H\\(_0\\): the distribution of a categorical response variable is the same in each subgroup. H\\(_a\\): the distribution is not the same in each subgroup. Let‚Äôs work through a real-life example. div.blue { background-color:#e6f0ff; border-radius: 10px; padding: 20px; } Remdesivir and COVID-19 Remdesivir is an antiviral drug previously tested in animal models infected with coronaviruses like SARS and MERS. As of May 2020, remdesivir had temporary approval from the FDA for use in severely ill COVID-19 patients, and it was the subject of numerous ongoing studies. A randomized controlled trial conducted in China enrolled 236 patients with severe COVID-19 symptoms; 158 were assigned to receive remdesivir and 78 to receive a placebo. In the remdesivir group, 103 patients showed clinical improvement; in the placebo group, 45 patients showed clinical improvement. A placebo is a ‚Äúfake‚Äù treatment. That is, placebos do not contain any active substances that affect health. Reference Wang, Y., Zhang, D., Du, G., Du, R., Zhao, J., Jin, Y., ‚Ä¶ Wang, C. (2020). Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial. The Lancet. https://doi.org/10.1016/S0140-6736(20)31022-9 If we consider the treatment and the placebo group as two subgroups of the population, we would expect the ratios of clinical improvement to be different. Let‚Äôs do a \\(\\chi^2\\) test for homogeneity. We will start with a contingency table. rem_cont &lt;- data.frame(treatment = c(103, 158-103), placebo = c(45, 78-45)) rownames(rem_cont) &lt;- c(&quot;improvement&quot;, &quot;no improvement&quot;) rem_cont ## treatment placebo ## improvement 103 45 ## no improvement 55 33 Next we will run the test. Before we run the test, answer the following questions: What is our null hypothesis? What is our alternative hypothesis? chisq.test(rem_cont) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: rem_cont ## X-squared = 0.95518, df = 1, p-value = 0.3284 What does this result mean? 7.3.4 Fisher‚Äôs exact test (OPTIONAL) If the count in any cell of our contigency table is less than 5, the \\(\\chi^2\\) test will not be useful because of its probability distribution assumption. In this case, we will use Fisher‚Äôs exact test. The hypotheses of Fisher‚Äôs exact same as that of the \\(\\chi^2\\) test. Fisher‚Äôs exact test can be used for either homogeneity or independence, depending on your experimental design. Suppose we have a sample 10 times smaller for the Remdesivir trial: small_rem &lt;- data.frame(treatment = c(10, 16-10), placebo = c(4, 8-4)) rownames(rem_cont) &lt;- c(&quot;improvement&quot;, &quot;no improvement&quot;) small_rem ## treatment placebo ## 1 10 4 ## 2 6 4 We have many cells with &lt;5 observations. In this case let‚Äôs run Fisher‚Äôs exact test. fisher.test(small_rem) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: small_rem ## p-value = 0.6734 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.2140763 12.7643113 ## sample estimates: ## odds ratio ## 1.630755 The p-value is 0.6734. What does this result mean? The odds ratio is yet another useful measurement you will often see in medical science articles. For the sake of time, I will leave it to you if you wish to read up on it. 7.3.5 Comparison of proportions (OPTIONAL) In the Remdesivir study, the participants were randomly assigned to each group. Thus, the groups can be treated as independent. It is also reasonable to assume independence of patients within each group. Suppose we have two proportions \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\). Then, the normal model can be applied to the difference of the two proportions, \\(\\hat{p}_1 - \\hat{p}_2\\), if the following assumptions are fulfilled: The sampling distribution for each sample proportion is nearly normal. The samples are independent random samples from the relevant populations and are independent of each other. Each sample proportion approximately follows a normal model when \\(n_1p_1\\), \\(n_1(1 - p_1)\\), \\(n_2p_2\\), and \\(n_2(1-p_2)\\) are all are \\(\\geq 10\\). To check success-failure in the context of a confidence interval, use \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\). The standard error of the difference in sample proportions is \\[\\sqrt{\\dfrac{p_1(1-p_1)}{n_1} + \\dfrac{p_2(1-p_2)}{n_2}}. \\] For hypothesis testing, an estimate of \\(p\\) is used to compute the standard error of \\(\\hat{p}_1 - \\hat{p}_2\\): \\(\\hat{p}\\), the weighted average of the sample proportions \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\), \\[\\hat{p} = \\dfrac{n_1\\hat{p}_1 + n_2\\hat{p}_2}{n_1 + n_2} = \\dfrac{x_1 + x_2}{n_1 + n_2}. \\] To check success-failure in the context of hypothesis testing, check that \\(\\hat{p}n_1\\) and \\(\\hat{p}n_2\\) are both \\(\\geq 10\\). In this case, let‚Äôs calculate the The pooled proportion \\(\\hat{p}\\): \\[\\hat{p} = \\dfrac{x_1 + x_2}{n_1 + n_2} = 0.627\\] x = c(103, 45) n = c(158, 78) p.hat.vector = x/n p.hat.vector ## [1] 0.6518987 0.5769231 #use r as a calculator p.hat.pooled = sum(x)/sum(n) p.hat.pooled ## [1] 0.6271186 Next we will check the success-failure condition, which is, \\(\\hat{p}n_1\\) and \\(\\hat{p}n_2\\) are both \\(\\geq 10\\). #check success-failure n*p.hat.pooled ## [1] 99.08475 48.91525 n*(1 - p.hat.pooled) ## [1] 58.91525 29.08475 The success-failure condition is met; the expected number of successes and failures are all larger than 10. #conduct inference prop.test(x = x, n = n) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: x out of n ## X-squared = 0.95518, df = 1, p-value = 0.3284 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.06703113 0.21698245 ## sample estimates: ## prop 1 prop 2 ## 0.6518987 0.5769231 In this example, we tested \\(H_0: p_1 = p_2\\) against \\(H_a: p_1 \\neq p_2\\) Here, \\(p_1\\) represents the population proportion of clinical improvement in COVID-19 patients treated with remdesivir, and \\(p_2\\) represents the population proportion of clinical improvement in COVID-19 patients treated with a placebo. By convention, \\(\\alpha = 0.05\\). The \\(p\\)-value is 0.3284, which is greater than \\(\\alpha\\). We conclude that there is insufficient evidence to reject the null hypothesis. Although the proportion of patients who experienced clinical improvement about 7% higher in the remdesivir group, this difference is not big enough to show that remdesivir is more effective than a placebo. 7.3.6 Contingency tables Let‚Äôs come back to the data of Asia and Africa in 1952. Take a look at the distribution of the life expectancy for all countries in both continents. summary(As.Af) ## country continent year lifeExp ## Afghanistan: 1 Africa :52 Min. :1952 Min. :28.80 ## Algeria : 1 Americas: 0 1st Qu.:1952 1st Qu.:37.00 ## Angola : 1 Asia :33 Median :1952 Median :40.54 ## Bahrain : 1 Europe : 0 Mean :1952 Mean :41.92 ## Bangladesh : 1 Oceania : 0 3rd Qu.:1952 3rd Qu.:45.01 ## Benin : 1 Max. :1952 Max. :65.39 ## (Other) :79 ## pop gdpPercap ## Min. : 60011 Min. : 298.9 ## 1st Qu.: 1022556 1st Qu.: 684.2 ## Median : 3379468 Median : 1077.3 ## Mean : 19211739 Mean : 2783.3 ## 3rd Qu.: 8550362 3rd Qu.: 1828.2 ## Max. :556263527 Max. :108382.4 ## Notice that the median of life expectancy is 40.54. That is, half of the countries had life expectancy greater than 40.54, and the other half less than 40.54. Let‚Äôs define a catagorical variable: the countries with life expectancy &gt; 40.54 years are ‚Äúlonger_lived‚Äù, and the others are ‚Äúshorter_lived‚Äù. As.Af[&quot;long_short&quot;] &lt;- NA As.Af$long_short[As.Af$lifeExp &gt; 40.54] &lt;- &quot;longer_lived&quot; As.Af$long_short[is.na(As.Af$long_short)] &lt;- &quot;shorter_lived&quot; Now let‚Äôs see if the longer lived or shorter lived variable is independent of the continent variable. We realize that both variables are categorical. In this case, we will use chi-squared test. First, we will make a contingency table of the two variables. As.Af &lt;- droplevels(As.Af) cont &lt;- table(As.Af$continent, As.Af$long_short) cont ## ## longer_lived shorter_lived ## Africa 21 31 ## Asia 22 11 Here, our null hypothesis is that countries are independent of the continent it‚Äôs a part of. Likewise, our alternative hypothesis is that countries are dependent (not independent) of the continent it‚Äôs a part of. chisq.test(cont) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: cont ## X-squared = 4.5769, df = 1, p-value = 0.03241 Given that \\(p &lt; 0.05\\), our null hypothesis that the two variables are independent is rejected. Whether a country is longer lived or shorter lived is dependent on the continent it is located in. N.B., this is only a comparison between Africa and Asia, and does not hold true for all continents. 7.4 Visualizing data distributions (OPTIONAL) Knowing the properties of the normal distribution is essential in understanding the normal distribution. The position of the peak indicates the mean, whereas the spread of the curve indicates the variance. Although you might not think your data follows a bell curve, let‚Äôs take a look at this example for our exercise. Let‚Äôs first install a package that helps us create ridgeline plots. install.packages(&quot;ggridges&quot;) Here we will plot the distribution of the life expectancy of African countries in different years. For each year, distributions are sectioned into quartiles. What could you say about the trend over the years? Please discuss both the mean and variance. What does it mean? library(ggridges) # getting all rows with Africa as the continent Africa.all &lt;- gapminder %&gt;% filter(continent == &quot;Africa&quot;, year &gt; 1990) # plotting p &lt;- ggplot(Africa.all, aes(lifeExp, as.factor(year), fill=factor(stat(quantile)))) + stat_density_ridges(quantiles=4, quantile_lines=T, geom = &#39;density_ridges_gradient&#39;) + scale_fill_viridis_d(name=&#39;Quartile&#39;) + labs(x=&#39;Life expectancy (yrs)&#39;, y=&#39;Year&#39;) + theme_classic() show(p) ## Picking joint bandwidth of 3.6 Let‚Äôs take a look at the mean and standard deviation to see if your guess is correct. Africa.all %&gt;% select(c(year, lifeExp)) %&gt;% group_by(as.factor(year)) %&gt;% summarize(mean_life = mean(lifeExp), sd_life = sd(lifeExp)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 3 ## `as.factor(year)` mean_life sd_life ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1992 53.6 9.46 ## 2 1997 53.6 9.10 ## 3 2002 53.3 9.59 ## 4 2007 54.8 9.63 This visualization shows the same information as that in with the density plots, but in a more digestible manner. p &lt;- ggplot(data = Africa.all, aes(x=as.factor(year), y=lifeExp)) + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, binwidth = 1) + geom_boxplot(alpha=0.3) show(p) You may want to remove the gray background and decrease dot size. This is as easy as specifying the dotsize parameter and adding theme_classic(). There a lot more themes out there! Check them out here. p &lt;- ggplot(data = Africa.all, aes(x=as.factor(year), y=lifeExp)) + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, binwidth = 1, dotsize = 0.65) + geom_boxplot(alpha=0.3) + theme_classic() show(p) While we‚Äôre at it, let‚Äôs also rename the x- and y-axis. Since we‚Äôve saved the plot already, let‚Äôs add a label layer to the plot. p &lt;- p + labs(x = &#39;Year&#39;, y = &#39;Life expectancy (yrs)&#39;) show(p) Now this figure is publication-ready. "],["comparing-multiple-means.html", "8 Comparing multiple means 8.1 Loading packages 8.2 Merging datasets 8.3 One-way ANOVA 8.4 Linear regression", " 8 Comparing multiple means Throughout this lab, we will provide a pipeline to help you wrangle data, perform statistical analyses, and (perhaps most importantly) visualize data in R. Here, we will learn how to compare the means using parametric tests and medians using non-parametric tests of multiple groups. 8.1 Loading packages Let‚Äôs load the usual packages. library(gapminder) library(car) # car stands for Companion to Applied Regression library(tidyverse) 8.2 Merging datasets In this section, we will learn how to merge datasets. We will use something called democracy index (democracy score) and convert it into categorical data. As the name suggests, democracy index measures the degree of democracy of a country on a scale from 0 to 10, with higher scores being correlated with greater democracy. In our dataset, however, the scale is from -10 to 10. This data set has been pre-cleaned and made available on gapminder. Alternatively, download the file by clicking here. Let‚Äôs load our dataset. democracy.raw &lt;- read.csv(file = &quot;data/07_multi-compare/democracy_score_use_as_color.csv&quot;, header = TRUE) The first thing you should do with new data is explore it. Since the output is quite large, we‚Äôll only show the first row, but you should definitely take a deeper look. head(democracy.raw, n=1) ## country X1800 X1801 X1802 X1803 X1804 X1805 X1806 X1807 X1808 X1809 X1810 ## 1 Afghanistan -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1811 X1812 X1813 X1814 X1815 X1816 X1817 X1818 X1819 X1820 X1821 X1822 X1823 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1824 X1825 X1826 X1827 X1828 X1829 X1830 X1831 X1832 X1833 X1834 X1835 X1836 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1837 X1838 X1839 X1840 X1841 X1842 X1843 X1844 X1845 X1846 X1847 X1848 X1849 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1850 X1851 X1852 X1853 X1854 X1855 X1856 X1857 X1858 X1859 X1860 X1861 X1862 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1863 X1864 X1865 X1866 X1867 X1868 X1869 X1870 X1871 X1872 X1873 X1874 X1875 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1876 X1877 X1878 X1879 X1880 X1881 X1882 X1883 X1884 X1885 X1886 X1887 X1888 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1889 X1890 X1891 X1892 X1893 X1894 X1895 X1896 X1897 X1898 X1899 X1900 X1901 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1902 X1903 X1904 X1905 X1906 X1907 X1908 X1909 X1910 X1911 X1912 X1913 X1914 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1915 X1916 X1917 X1918 X1919 X1920 X1921 X1922 X1923 X1924 X1925 X1926 X1927 ## 1 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 -6 ## X1928 X1929 X1930 X1931 X1932 X1933 X1934 X1935 X1936 X1937 X1938 X1939 X1940 ## 1 -6 -6 -6 -6 -6 -6 -6 -8 -8 -8 -8 -8 -8 ## X1941 X1942 X1943 X1944 X1945 X1946 X1947 X1948 X1949 X1950 X1951 X1952 X1953 ## 1 -8 -8 -8 -8 -10 -10 -10 -10 -10 -10 -10 -10 -10 ## X1954 X1955 X1956 X1957 X1958 X1959 X1960 X1961 X1962 X1963 X1964 X1965 X1966 ## 1 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -7 -7 -7 ## X1967 X1968 X1969 X1970 X1971 X1972 X1973 X1974 X1975 X1976 X1977 X1978 X1979 ## 1 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 -7 0 -10 ## X1980 X1981 X1982 X1983 X1984 X1985 X1986 X1987 X1988 X1989 X1990 X1991 X1992 ## 1 -10 -10 -10 -10 -10 -10 -10 -10 -10 -8 -8 -8 0 ## X1993 X1994 X1995 X1996 X1997 X1998 X1999 X2000 X2001 X2002 X2003 X2004 X2005 ## 1 0 0 0 -7 -7 -7 -7 -7 NA NA NA NA NA ## X2006 X2007 X2008 X2009 X2010 X2011 ## 1 NA NA NA NA NA NA Don‚Äôt forget about str() and summary()! str(democracy.raw) summary(democracy.raw) As you can see, there is a lot of missing data (denoted by NA). NA values are often problematic for analyses, so we would like to either remove them or impute (estimate) them. In our case, let‚Äôs get rid of the rws with missing data for the year 2007 (the X2007 column). dem07 &lt;- democracy.raw %&gt;% select(country, X2007) %&gt;% # choose filter(!is.na(X2007)) # selecting all non-NA rows Here, is.na() will return TRUE for missing data. Recall that ! is the NOT logical operator (i.e., !TRUE is equivalent to FALSE and vice versa. It follows that !is.na() returns true for non-empty data. Let‚Äôs take a looks at how the democracy score is distributed. Here, I‚Äôd like to treat each democracy score as a factor. ggplot(dem07, aes(as.factor(X2007))) + geom_bar() Before we do anything, let‚Äôs look at some potentially interesting counts. First, we‚Äôll look at two ways to count ‚Äúlow-level‚Äù countries. To do so, we will arbitrarily define any democracy score \\(\\leq\\) -3 as low-level. Now, we will count the number of countries in each group. nrow(dem07[dem07$X2007 &lt;= -3,]) # base R ## [1] 39 dem07 %&gt;% filter(X2007 &lt;= -3) %&gt;% nrow() # with dplyr pipe operator ## [1] 39 Let‚Äôs define medium-level countries as having a democracy score betwen -2 and 5 inclusive. nrow(dem07[dem07$X2007 &gt;= -2 &amp; dem07$X2007 &lt;= 5,]) # base R ## [1] 32 dem07 %&gt;% filter(X2007 &gt;= -2 &amp; X2007 &lt;= 5) %&gt;% nrow() # with dplyr pipe operator ## [1] 32 Exercise: count the number of high-level countries using both base R and dplyr. High-level countries will be defined as those with democracy score greater or equal to 6. Now let‚Äôs actually assign a new categorical variable to each country (row) using the cut() function. Let‚Äôs call the new row demLev (our shorthand for democracy level). tempDemLev &lt;- cut(dem07$X2007, c(-Inf, -2.5, 4.5, Inf), c(&quot;LowDem&quot;, &quot;MidDem&quot;, &quot;HighDem&quot;)) dem07$demLev &lt;- tempDemLev # base R method # dem07 &lt;- dem07 %&gt;% mutate(demLev = tempDemLev) # dplyr method head(dem07) ## country X2007 demLev ## 1 Albania 9 HighDem ## 2 Algeria 2 MidDem ## 3 Angola -2 MidDem ## 4 Argentina 8 HighDem ## 5 Armenia 5 HighDem ## 6 Australia 10 HighDem Note: The first argument for cut() takes a vector, the second takes the vector for cutoff thresholds, and the third are names of the bins defined by the cutoffs. We can now merge this new data with gapminder. The main idea of merging is to add the new variables as columns. The identifier of our observations will be country. Since we are taking data from different sources, a given country might exist in one data frame but not the other. Furthermore, the two data sets might use different names for the countries. Before merging, let‚Äôs check the data we want to merge. Note that str_detect() finds all instances where a particular string is in a column. # let&#39;s check how they name Korea dem07 %&gt;% filter(str_detect(country, &#39;Korea&#39;)) ## country X2007 demLev ## 1 North Korea -10 LowDem ## 2 South Korea 8 HighDem gapminder %&gt;% filter(str_detect(country, &#39;Korea&#39;)) ## # A tibble: 24 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Korea, Dem. Rep. Asia 1952 50.1 8865488 1088. ## 2 Korea, Dem. Rep. Asia 1957 54.1 9411381 1571. ## 3 Korea, Dem. Rep. Asia 1962 56.7 10917494 1622. ## 4 Korea, Dem. Rep. Asia 1967 59.9 12617009 2144. ## 5 Korea, Dem. Rep. Asia 1972 64.0 14781241 3702. ## 6 Korea, Dem. Rep. Asia 1977 67.2 16325320 4106. ## 7 Korea, Dem. Rep. Asia 1982 69.1 17647518 4107. ## 8 Korea, Dem. Rep. Asia 1987 70.6 19067554 4106. ## 9 Korea, Dem. Rep. Asia 1992 70.0 20711375 3726. ## 10 Korea, Dem. Rep. Asia 1997 67.7 21585105 1691. ## # ‚Ä¶ with 14 more rows Now that we have a clear idea of which each looks like, we need to determine the potential differences between them. For example, you can‚Äôt merge 'Korea, Dem. Rep.' with 'Korea' since the two strings are not exactly equal. # setdiff() finds the differences between values in each dataset # unique() ensures that there are no duplicate values setdiff(unique(dem07$country), unique(gapminder$country)) ## [1] &quot;Armenia&quot; &quot;Azerbaijan&quot; &quot;Belarus&quot; ## [4] &quot;Bhutan&quot; &quot;Cape Verde&quot; &quot;Cyprus&quot; ## [7] &quot;Estonia&quot; &quot;Fiji&quot; &quot;Georgia&quot; ## [10] &quot;Guyana&quot; &quot;Kazakhstan&quot; &quot;Kyrgyz Republic&quot; ## [13] &quot;Lao&quot; &quot;Latvia&quot; &quot;Lithuania&quot; ## [16] &quot;Moldova&quot; &quot;North Korea&quot; &quot;North Macedonia&quot; ## [19] &quot;Papua New Guinea&quot; &quot;Qatar&quot; &quot;Russia&quot; ## [22] &quot;Solomon Islands&quot; &quot;South Korea&quot; &quot;Suriname&quot; ## [25] &quot;Tajikistan&quot; &quot;Timor-Leste&quot; &quot;Turkmenistan&quot; ## [28] &quot;Ukraine&quot; &quot;United Arab Emirates&quot; &quot;Uzbekistan&quot; ## [31] &quot;Yemen&quot; setdiff(unique(gapminder$country), unique(dem07$country)) ## [1] &quot;Afghanistan&quot; &quot;Bosnia and Herzegovina&quot; &quot;Hong Kong, China&quot; ## [4] &quot;Iceland&quot; &quot;Korea, Dem. Rep.&quot; &quot;Korea, Rep.&quot; ## [7] &quot;Puerto Rico&quot; &quot;Reunion&quot; &quot;Sao Tome and Principe&quot; ## [10] &quot;Taiwan&quot; &quot;West Bank and Gaza&quot; &quot;Yemen, Rep.&quot; It looks like we need to change ‚ÄúSouth Korea‚Äù to ‚ÄúKorea, Rep.‚Äù, and ‚ÄúYemen‚Äù to ‚ÄúYemen, Rep.‚Äù. We can do this using the factor recode function: fct_recode() dem07 &lt;- dem07 %&gt;% mutate(country = fct_recode(country, &#39;Korea, Rep.&#39; = &#39;South Korea&#39;, &#39;Yemen, Rep.&#39; = &#39;Yemen&#39;)) Finally, let‚Äôs can merge the two data frames using a left join. There are many types of joins (right join, inner join, etc.), and you can check them out here. # need to filter out missing data! my_gap &lt;- gapminder %&gt;% left_join(dem07, by = &quot;country&quot;) %&gt;% filter(!is.na(demLev)) %&gt;% filter(!is.na(lifeExp)) # let&#39;s see what the data looks like now head(my_gap) ## # A tibble: 6 x 8 ## country continent year lifeExp pop gdpPercap X2007 demLev ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; ## 1 Albania Europe 1952 55.2 1282697 1601. 9 HighDem ## 2 Albania Europe 1957 59.3 1476505 1942. 9 HighDem ## 3 Albania Europe 1962 64.8 1728137 2313. 9 HighDem ## 4 Albania Europe 1967 66.2 1984060 2760. 9 HighDem ## 5 Albania Europe 1972 67.7 2263554 3313. 9 HighDem ## 6 Albania Europe 1977 68.9 2509048 3533. 9 HighDem Please note that demLev was based on the score of 2007. We don‚Äôt do that here, but you could also include the levels based on the scores from different years. 8.3 One-way ANOVA 8.3.1 The iris dataset The iris dataset contains information about three species of flowers: setosa, veriscolor, and virginia. Iris is a built-in dataset, meaning we can call it without reading it in. iris$Species refers to one column in iris. That is, the column with the name of the species (setosa, versicolor, or virginica). We can see how many rows and columns are in a data.frame with the dim command. dim(iris) prints out the number of rows (150) and the number of columns (5): head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Analysis of Variance (ANOVA) allows us to test whether there are differences in the mean between multiple samples. The question we will address is: Are there differences in average sepal width among the three species? To run an ANOVA, we need to check if The variance is is equal for each group, and The data distributes normally within each group. Let‚Äôs address the first point. leveneTest(Sepal.Width ~ Species, data = iris) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.5902 0.5555 ## 147 A p-value of 0.5555 suggested that the variances are not significantly different. This means we should proceed with a parametric test like ANOVA (otherwise, use the Kruskal-Wallis test). Keep in mind we haven‚Äôt yet checked the normality. We will do it after running ANOVA. We start by building an analysis of variance model with the aov() function: In this case, we pass two arguments to the aov() function: For the formula parameter, we pass Sepal.Width ~ Species. This format is used throughout R for describing relationships we are testing. The format is y ~ x, where the response variables (e.g. y) are to the left of the tilde (~) and the predictor variables (e.g. x) are to the right of the tilde. In this example, we are asking if petal length is significantly different among the three species. We also need to tell R where to find the Sepal.Width and Species data, so we pass the variable name of the iris data.frame to the data parameter. But we want to store the model, not just print it to the screen, so we use the assignment operator &lt;- to store the product of the aov function in a variable of our choice Sepal.Width.aov &lt;- aov(formula = Sepal.Width ~ Species, data = iris) Notice how when we execute this command, nothing printed in the console. This is because we instead sent the output of the aov call to a variable. If you just type the variable name, you will see the familiar output from the aov function: Sepal.Width.aov ## Call: ## aov(formula = Sepal.Width ~ Species, data = iris) ## ## Terms: ## Species Residuals ## Sum of Squares 11.34493 16.96200 ## Deg. of Freedom 2 147 ## ## Residual standard error: 0.3396877 ## Estimated effects may be unbalanced To see the results of the ANOVA, we call the summary() function: summary(object = Sepal.Width.aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 11.35 5.672 49.16 &lt;2e-16 *** ## Residuals 147 16.96 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The species do have significantly different sepal width (P &lt; 0.001). However, ANOVA does not tell us which species are different. We can run a post hoc test to assess how the species are different. A Tukey test comparing means would be one option. We will do the Tukey test after determining normality. Now, let‚Äôs take a look at the normality. First, we will plot the diagnostic figures. plot(Sepal.Width.aov) Most importantly, the dots in Q-Q plot (upper right) should align with the line pretty well. This figure is acceptable. If the dots deviate from the line too much, the data would not be considered normal. If you still perform the ANOVA, you should view your results critically (or ignore them, at worst). Please do not include such diagnostic figures in the main text of your manuscripts. This might qualify for a supplementary figure at most. Although we‚Äôve also examined residuals with the QQ plot, we can also use a formal test: residuals_Sepal_Width &lt;- residuals(object = Sepal.Width.aov) shapiro.test(x = residuals_Sepal_Width) ## ## Shapiro-Wilk normality test ## ## data: residuals_Sepal_Width ## W = 0.98948, p-value = 0.323 A p-value of 0.323 suggested that the assumption of normality is reasonable. Recall that a residual is an ‚Äúerror‚Äù in result. More specifically, a residual is the difference of a given data point from the mean (\\(r = x - \\mu\\)). So far, we have demonstrated Normality in distribution. Homogeneity variance, and These two justified our choice for one-way ANOVA. The result of ANOVA also indicated that at least one species of the 3 has significantly different sepal width from others. Which one? To do this, we need to run ‚ÄúPost-Hoc‚Äù test. Let‚Äôs do Tukey Honest Significant Differences (HSD). The nice thing is that TukeyHSD() can directly take the result of ANOVA as the argument. TukeyHSD(Sepal.Width.aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Sepal.Width ~ Species, data = iris) ## ## $Species ## diff lwr upr p adj ## versicolor-setosa -0.658 -0.81885528 -0.4971447 0.0000000 ## virginica-setosa -0.454 -0.61485528 -0.2931447 0.0000000 ## virginica-versicolor 0.204 0.04314472 0.3648553 0.0087802 The difference between every pair are significant (\\(p &lt; 0.05\\)). 8.3.2 Non-parametric alternatives to ANOVA In reality, your data usually wouldn‚Äôt be as perfect as above. In case of a non-normal sample, there are two ways to address the problem: Apply appropriate data transformations techniques, or Use a non-parametric test I highly recommend you to explore the tricks of data transformation. If you can rescue it back to normal distribution, parametric tests usually can allow you to do more powerful analysis. If you have exhausted your attempts to data transformation, you may then use non-parametric tests. A note for Kruskal-Wallis H-test. When your data doesn‚Äôt satisfy the normality or equal variance assumption, ANOVA does not strictly apply. However, one-way ANOVA is not very sensitive to deviations from normality. Kruskal-Wallis doesn‚Äôt assume normality, but it does assume same distribution (equal variance). If your data do not meet either assumption, you would want to use Welch‚Äôs One-way Test. Now, let‚Äôs get back to gapminder data. Let‚Äôs add another categorical variable calle Income_Level. This time we will split by the quartiles. dat.1952 &lt;- my_gap %&gt;% filter(year == 1952) border_1952 &lt;- quantile(dat.1952$gdpPercap, c(.25, .50, .75)) dat.1952$Income_Level_1952 &lt;- cut(dat.1952$gdpPercap, c(0, border_1952[1], border_1952[2], border_1952[3], Inf), c(&#39;Low&#39;, &#39;Low Middle&#39;, &#39;High Middle&#39;, &#39;High&#39;)) head(dat.1952) ## # A tibble: 6 x 9 ## country continent year lifeExp pop gdpPercap X2007 demLev Income_Level_19‚Ä¶ ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; ## 1 Albania Europe 1952 55.2 1.28e6 1601. 9 HighD‚Ä¶ Low Middle ## 2 Algeria Africa 1952 43.1 9.28e6 2449. 2 MidDem High Middle ## 3 Angola Africa 1952 30.0 4.23e6 3521. -2 MidDem High Middle ## 4 Argent‚Ä¶ Americas 1952 62.5 1.79e7 5911. 8 HighD‚Ä¶ High ## 5 Austra‚Ä¶ Oceania 1952 69.1 8.69e6 10040. 10 HighD‚Ä¶ High ## 6 Austria Europe 1952 66.8 6.93e6 6137. 10 HighD‚Ä¶ High dat.2007 &lt;- my_gap %&gt;% filter(year == 2007) border_2007 &lt;- quantile(dat.2007$gdpPercap, c(.25, .50, .75)) dat.2007$Income_Level_2007 &lt;- cut(dat.2007$gdpPercap, c(0, border_2007[1], border_2007[2], border_2007[3], Inf), c(&#39;Low&#39;, &#39;Low Middle&#39;, &#39;High Middle&#39;, &#39;High&#39;)) head(dat.2007) ## # A tibble: 6 x 9 ## country continent year lifeExp pop gdpPercap X2007 demLev Income_Level_20‚Ä¶ ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; ## 1 Albania Europe 2007 76.4 3.60e6 5937. 9 HighD‚Ä¶ Low Middle ## 2 Algeria Africa 2007 72.3 3.33e7 6223. 2 MidDem High Middle ## 3 Angola Africa 2007 42.7 1.24e7 4797. -2 MidDem Low Middle ## 4 Argent‚Ä¶ Americas 2007 75.3 4.03e7 12779. 8 HighD‚Ä¶ High Middle ## 5 Austra‚Ä¶ Oceania 2007 81.2 2.04e7 34435. 10 HighD‚Ä¶ High ## 6 Austria Europe 2007 79.8 8.20e6 36126. 10 HighD‚Ä¶ High For now, let‚Äôs focus on the data of in 1952. ggplot(data = dat.1952, aes(x = Income_Level_1952, y = lifeExp)) + geom_boxplot() + theme_classic() We can also visualize life expectancy for each democracy level: ggplot(data = dat.1952, aes(x = demLev, y = lifeExp)) + geom_boxplot() + theme_classic() Let‚Äôs check the variance. leveneTest(lifeExp ~ Income_Level_1952, data = dat.1952) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 4.2319 0.006881 ** ## 128 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A p-value of 0.0047 suggested that the variances are significantly different. Therefore, we shoud not run ANOVA or Kruskal-Wallis. Let‚Äôs run Welch‚Äôs one-way test. result &lt;- oneway.test(lifeExp ~ Income_Level_1952, data = dat.1952) result ## ## One-way analysis of means (not assuming equal variances) ## ## data: lifeExp and Income_Level_1952 ## F = 63.15, num df = 3.000, denom df = 68.584, p-value &lt; 2.2e-16 A p-value of 2.2e-16 suggested that at least one category of Income_Level_1952 had values of lifeExp that are significantly different from others. Let‚Äôs run a Post-Hoc test to find out. Since we are running a non-parametric test, the appropriate test would be Games-Howell post-hoc test. Unfortunately, R does not have a built-in function for Games-Howell. Let‚Äôs define a function to do this task. Note: you don‚Äôt need to know how the code below works. games.howell &lt;- function(grp, obs) { #Create combinations combs &lt;- combn(unique(grp), 2) # Statistics that will be used throughout the calculations: # n = sample size of each group # groups = number of groups in data # Mean = means of each group sample # std = variance of each group sample n &lt;- tapply(obs, grp, length) groups &lt;- length(tapply(obs, grp, length)) Mean &lt;- tapply(obs, grp, mean) std &lt;- tapply(obs, grp, var) statistics &lt;- lapply(1:ncol(combs), function(x) { mean.diff &lt;- Mean[combs[2,x]] - Mean[combs[1,x]] # t-values t &lt;- abs(Mean[combs[1,x]] - Mean[combs[2,x]]) / sqrt((std[combs[1,x]] / n[combs[1,x]]) + (std[combs[2,x]] / n[combs[2,x]])) # Degrees of Freedom df &lt;- (std[combs[1,x]] / n[combs[1,x]] + std[combs[2,x]] / n[combs[2,x]])^2 / # numerator dof ((std[combs[1,x]] / n[combs[1,x]])^2 / (n[combs[1,x]] - 1) + # Part 1 of denominator dof (std[combs[2,x]] / n[combs[2,x]])^2 / (n[combs[2,x]] - 1)) # Part 2 of denominator dof # p-values p &lt;- ptukey(t * sqrt(2), groups, df, lower.tail = FALSE) # sigma standard error se &lt;- sqrt(0.5 * (std[combs[1,x]] / n[combs[1,x]] + std[combs[2,x]] / n[combs[2,x]])) # Upper Confidence Limit upper.conf &lt;- lapply(1:ncol(combs), function(x) { mean.diff + qtukey(p = 0.95, nmeans = groups, df = df) * se })[[1]] # Lower Confidence Limit lower.conf &lt;- lapply(1:ncol(combs), function(x) { mean.diff - qtukey(p = 0.95, nmeans = groups, df = df) * se })[[1]] # Group Combinations grp.comb &lt;- paste(combs[1,x], &#39;:&#39;, combs[2,x]) # Collect all statistics into list stats &lt;- list(grp.comb, mean.diff, se, t, df, p, upper.conf, lower.conf) }) # Unlist statistics collected earlier stats.unlisted &lt;- lapply(statistics, function(x) { unlist(x) }) # Create dataframe from flattened list results &lt;- data.frame(matrix(unlist(stats.unlisted), nrow = length(stats.unlisted), byrow=TRUE)) # Select columns set as factors that should be numeric and change with as.numeric results[c(2, 3:ncol(results))] &lt;- round(as.numeric(as.matrix(results[c(2, 3:ncol(results))])), digits = 3) # Rename data frame columns colnames(results) &lt;- c(&#39;groups&#39;, &#39;Mean Difference&#39;, &#39;Standard Error&#39;, &#39;t&#39;, &#39;df&#39;, &#39;p&#39;, &#39;upper limit&#39;, &#39;lower limit&#39;) return(results) } After defining the function, we can use it. If you decide to use the Games-Howell function, you can simply copy-and-paste it. Since this function is open-source code, citation is not necessary. games.howell(grp = dat.1952$Income_Level_1952, # Groups, the categorical variable obs = dat.1952$lifeExp) # Observations, the continuous variable ## groups Mean Difference Standard Error t df p ## 1 Low Middle : High Middle 8.807 1.444 4.311 58.295 0.00 ## 2 Low Middle : High 20.169 1.440 9.905 58.438 0.00 ## 3 Low Middle : Low -4.207 1.043 2.852 58.143 0.03 ## 4 High Middle : High 11.362 1.651 4.866 63.999 0.00 ## 5 High Middle : Low -13.014 1.319 6.974 48.168 0.00 ## 6 High : Low -24.376 1.315 13.113 48.302 0.00 ## upper limit lower limit ## 1 14.210 3.404 ## 2 25.555 14.784 ## 3 -0.305 -8.109 ## 4 17.522 5.203 ## 5 -8.048 -17.980 ## 6 -19.430 -29.323 8.3.3 Two-way ANOVA We can also look at 2 independent categorical variables together with a two-way ANOVA. This is as easy as calling aov() with an additional variable on the right side of the y ~ x formula. For example, we can take a look at both Income_Level_2007 and demLevel as explanatory variables to the response variable lifeExp. two_way_plus &lt;- aov(lifeExp ~ Income_Level_2007 + demLev, data = dat.2007) two_way_star &lt;- aov(lifeExp ~ Income_Level_2007 * demLev, data = dat.2007) You might have noticed that I used + to connect the 2 explanatory variables in the first line and * for the second line. How are they different? Take a look at the results below. two_way_plus ## Call: ## aov(formula = lifeExp ~ Income_Level_2007 + demLev, data = dat.2007) ## ## Terms: ## Income_Level_2007 demLev Residuals ## Sum of Squares 12199.101 437.708 6446.792 ## Deg. of Freedom 3 2 126 ## ## Residual standard error: 7.152973 ## Estimated effects may be unbalanced two_way_star ## Call: ## aov(formula = lifeExp ~ Income_Level_2007 * demLev, data = dat.2007) ## ## Terms: ## Income_Level_2007 demLev Income_Level_2007:demLev Residuals ## Sum of Squares 12199.101 437.708 662.338 5784.454 ## Deg. of Freedom 3 2 6 120 ## ## Residual standard error: 6.942895 ## Estimated effects may be unbalanced summary(two_way_plus) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Income_Level_2007 3 12199 4066 79.476 &lt;2e-16 *** ## demLev 2 438 219 4.277 0.0159 * ## Residuals 126 6447 51 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(two_way_star) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Income_Level_2007 3 12199 4066 84.36 &lt;2e-16 *** ## demLev 2 438 219 4.54 0.0126 * ## Income_Level_2007:demLev 6 662 110 2.29 0.0396 * ## Residuals 120 5784 48 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the test with *, there is one more term Income_Level_2007:demLev. This is the interaction between the two variables. In this case, the interaction of of the two variables also contribute significantly to the difference in lifeExp. For the purposes of the Youreka program, it doesn‚Äôt matter which method you use. 8.4 Linear regression 8.4.1 Basic concepts We have discussed extensively for the scenario where you have a continuous variable and a categorical variable. Now we will talk about what you do if both variables are continuous. For this final section, we will test for a relationship between life expectancy and per capita gross domestic product (GDP). As we did for the ANOVA analyses, it is usually a good idea to visually inspect the data when possible. Here we can use the plot function to create a scatterplot of the two columns of interest, lifeExp and gdpPercap. ggplot(data = dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() We can see immediately that this is unlikely a linear relationship. In this case, we will need to log-transform the GDP data to obtain a linear relationship. ggplot(data = dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10() Now that the data are properly transformed, we can create the linear model for the predictability of life expectancy based on gross domestic product. Before we do that let‚Äôs make it clear: From the scatter plot we can identify a positive relationship ‚Äì when log(GDP per capita) increases, the life expectancy also tends to be higher. The tendency of one variable going up or down linearly with the increase of another variable is called ‚Äúcorrelation‚Äù. The more consistent the points are with a LINEAR trend, the higher the closer the correlation is to -1 (for negative relationships) or +1 (for positive relationships). How fast one variable increases or decreases with the increase of another variable can be described by the slope of the fitting line. To estimate the slope, we need a linear model. We can only discuss strength of correlation with these linear regression, but NOT the causation. That is, correlation does NOT imply causation. We can plot the linear model easily: ggplot(data = dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10() + geom_smooth(method = &quot;lm&quot;) # lm = linear model ## `geom_smooth()` using formula &#39;y ~ x&#39; To get rid of the confidence band around the line, pass le = FALSE into geom_smooth(). ggplot(data = dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10() + geom_smooth(method = &quot;lm&quot;, se = FALSE) # lm = linear model ## `geom_smooth()` using formula &#39;y ~ x&#39; You can also customize the colour and thickness of the line. As always, use the ? operator to get the full documentation. 8.4.2 Pearson correlation Let‚Äôs look at the correlation. For normal distributed data, we calculate the Pearson correlation for the log-transformed variable. dat.2007$log_GDP &lt;- log(dat.2007$gdpPercap) # add new variable cor.test(x = dat.2007$log_GDP, y = dat.2007$lifeExp, method = &quot;pearson&quot;) # method options: pearson, kendall, spearman ## ## Pearson&#39;s product-moment correlation ## ## data: dat.2007$log_GDP and dat.2007$lifeExp ## t = 15.626, df = 130, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7389427 0.8599823 ## sample estimates: ## cor ## 0.8078163 The p-value suggests the correlation is significant. The correlation coefficient of 0.8 suggests a positive correation (y increases as x increases). In case you see a negative value, the correlation if negative (one variable going up while the other going down). Next we can construct a linear model. # Run a linear model lifeExp.v.gdp &lt;- lm(formula = lifeExp ~ log_GDP, data = dat.2007) # Investigate results of the model summary(lifeExp.v.gdp) ## ## Call: ## lm(formula = lifeExp ~ log_GDP, data = dat.2007) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.692 -2.711 1.441 4.652 13.362 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.7951 4.0045 1.197 0.233 ## log_GDP 7.1909 0.4602 15.626 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.142 on 130 degrees of freedom ## Multiple R-squared: 0.6526, Adjusted R-squared: 0.6499 ## F-statistic: 244.2 on 1 and 130 DF, p-value: &lt; 2.2e-16 The linear equation is: \\(\\text{lifeExp} = (7.1909 \\pm 0.4602) \\times \\text{log_GDP} + (4.7951 \\pm 4.0045)\\). Also notice that the correlation coefficient is \\(R^2 = 0.6526 \\Rightarrow R = \\sqrt{0.6526} = 0.8078\\), the same value returned by cor.test(). For our question, the relationship between life expectancy and GDP, focus on the coefficients section, specifically the line for log_GDP. First of all, there is a significant relationship between these two variables (p &lt; 2 x 10-16, or, as R reports in the Pr&gt;(|t|) column, p &lt; 2e-16). The Estimate column of the results lists a value of lifeExp.v.gdp$coefficients['log_GDP']. For every 10-fold increase in per capita GDP (remember we log10-transformed GDP), life expectancy increases by almost 7 years. The linear model relies assumes that your data is normally distributed. We can generate a diagnostic plot in the same way as one-way ANOVA. plot(lifeExp.v.gdp) Q-Q plot suggested this data deviates from normality. Let‚Äôs also take a look at the residues of the linear model: residuals_lm &lt;- residuals(object = lifeExp.v.gdp) shapiro.test(x = residuals_lm) ## ## Shapiro-Wilk normality test ## ## data: residuals_lm ## W = 0.89588, p-value = 3.911e-08 Indeed, Shapiro test also suggests the data deviates from normality. In this case, we should use the Spearman (or Kendall) correlation. 8.4.3 Spearman correlation If your variables are not normally distributed, you can use the non-parametric Spearman correlation as alternative. Instead of Pearson‚Äôs R, the Spearman test outputs rho (\\(\\rho\\)), cor.test(dat.2007$lifeExp, dat.2007$log_GDP, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: dat.2007$lifeExp and dat.2007$log_GDP ## S = 57612, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8496971 "],["data-visualization-basics.html", "9 Data visualization basics 9.1 A graphing template 9.2 Scatter plot 9.3 Aesthetic mappings", " 9 Data visualization basics ‚ÄúThe simple graph has brought more information to the data analyst‚Äôs mind than any other device.‚Äù ‚Äì John Tukey This lab is all about preparing publication-ready figures with ggplot2 and related packages. ggplot2 uses elegant syntax and it implements ‚ÄúThe Layered Grammar of Graphics‚Äù. Before we begin, let‚Äôs review a few key points for good figures: Be clear and avoid confusion. Presenting too much information often results in messy figures. Figures inconsistent in colour, symbols, etc. can easily confuse readers. Only use additional aesthetic effects when necessary. Everything in a graph has a purpose. The primary objective of a figure is to inform, not to look fancy (though this is a plus). When in doubt, stick to black, white, and grey. Only use texts when necessary. Make text large. Never use Comic Sans as your font. Sans serif fonts such as Arial and Calibri are usually good bets. Key point: the plot depends on the variables. Some plots are more appropriate for visualization than others. You have an obligation to display data responsibly. Check out The R Graph Gallery for a ‚Äúdictionary‚Äù on different visualizations and the code to create them. Recall that the tidyverse contains ggplot2 and dplyr among other packages. We‚Äôll also load gapminder. library(gapminder) library(tidyverse) Hopefully by now you have a good idea of what the gapminder dataset looks like. Here‚Äôs a quick refresher: head(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 9.1 A graphing template Although we‚Äôve presented graphs with ggplot2 in previous labs, let‚Äôs delve into the specifics of the syntax: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) Everything in the initial ggplot() function is passed into the subsequent functions (i.e., GEOM_FUNCTION()). When calling ggplot(), you don‚Äôt explicitly need to write &lt;ARGUMENT&gt; = (ex: data = gapminder, x = lifeExp) as long as you have the variables in the correct order ‚Äì just be careful you don‚Äôt mix up x and y! ggplot2 works in a layer-by-layer manner. Take a look at this: ggplot(data = gapminder, aes(x = pop, y = gdpPercap)) ggplot(data = gapminder, aes(x = pop, y = gdpPercap)) + geom_point() Here, the first line initializes the object and the second line adds a layer of scatter points. Unlike plotting in base R, you don‚Äôt need to specify variables using the $ operator ‚Äì ggplot2 is smart enough to call it automatically for you. I‚Äôd like to bring your attention to the + operator. This is how you add layers to the ggplot. Use + liberally to reduce excessively long lines; breaking long commands at appropriate places makes your code much more readable. 9.2 Scatter plot Use the scatter plot when you want to see the relationship between two continuous variables. ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() You also can save the object as a variable. To show the figure, use the show() function or simply call the object by its name. p &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() show(p) # method 1 p # also works This data might benefit using a log scale. You can either log-transform (log(gdpPercap)) or simply draw the x axis in log scale (scale_x_log10()). p &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() + scale_x_log10() show(p) 9.2.1 Trend lines It seems that there exists a positive correlation between the two variables ‚Äì you might want to add a trend line. Remember, p is the object of scatter plot with x in log. We can just build from here. This just goes to show the beauty of layered graphical syntax. p + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; You find that R used ‚Äògam‚Äô model as default. ‚Äògam‚Äô is the generalized additive model. Without going into the mathematical details, you would expect a curve from gam. What if I want a straight line (i.e., linear regression)? I would want a straight line to start with. Let‚Äôs start with a linear model (lm). p + geom_smooth(method = &#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; We can also create a generalized linear model (glm). glm can be useful if your variables are not normally distributed. p + geom_smooth(method = &#39;glm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; It appears thatlm and glm don‚Äôt look too different. Notice that lm has a little shaded region around it. This correspondends to the confidence interval of 1 standard error. To get rid of it, specify se = FALSE. While we‚Äôre at it, let‚Äôs change the colour of the line to red and decrease its thickness. Let‚Äôs also remove that pesky grey background: scatter_trend &lt;- p + theme_classic() + # removes grey background geom_smooth(method = &#39;lm&#39;, se = F, # remove confidence band col = &#39;red&#39;, # change colour of line to red (hex colours also work: #FF0000) size = 0.75) # set width of line show(scatter_trend) ## `geom_smooth()` using formula &#39;y ~ x&#39; 9.2.2 Facets If you were paying careful attention, you may have noticed that we were using data from multiple years. However, this results in a messy, and potentially misleading, graph. Let‚Äôs fix this by plotting each year separately with the facet feature. ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + facet_wrap(vars(year), nrow = 3, ncol = 4) + geom_point(size = 0.5) + scale_x_log10() + theme_bw() # another ggplot2 theme We can use faceting to split by the combination of two variables. Here I will use facet_grid to put the same values of splitting variables on the same row/column. Note that you also need to specify vars(year) and vars(continent) instead of just year and continent. This is simply to help ggplot retrieve the levels in a particular column. gap.52.77.07 &lt;- gapminder %&gt;% filter(year %in% c(1952, 1977, 2007)) ggplot(data = gap.52.77.07, aes(x = gdpPercap, y = lifeExp)) + facet_grid(rows = vars(year), cols = vars(continent)) + geom_point() + scale_x_log10() + theme_bw() 9.3 Aesthetic mappings A scatter plot places dots using x and y coordinates. What if we want to show more detail, like which point(s) correspond to a particular group? For example, what if we want to see how life expectancy vs GDP per capita varies per continent in 1977? gap.77 &lt;- gapminder %&gt;% filter(year == 1977) ggplot(data = gap.77, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color=continent)) + scale_x_log10() You can see the points from some continents, like Europe and Africa, cluster at distinct positions. In addition to adding colours to show (categorical or continuous) groupings, we can also use Shape of points (categorical), Size of points (continuous), or Transparency/alpha of the points (continuous). These options are all specified within aesthetic mappings (aes()). That is, aes() is the place you specify how you present your variables. More specifically, it‚Äôs how you map your variables to various aesthetics. To repeat an earlier point, aes() within the ggplot() function applies to ALL layers, while those in other layers only applies to that specific layer. Here‚Äôs another example. Be careful with this as R interprets year as a continuous variable. Use as.factor() or factor() to circumvent this issue, ggplot(data = gap.52.77.07, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(col = as.factor(year))) + scale_x_log10() Going back to our original example of lifeExp vs gdpPercap by continent, I use stat_elipse() to enclose the points within a 95% confidence interval. ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp)) + # color only applies to the points, not the eclipses geom_point(aes(color=continent)) + # stat_ellipse uses level=0.95 by default stat_ellipse() + scale_x_log10() We can also create multiple ellipses to group things together. Note that color = continent is the same as colour = continent and col = continent. ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp, color = continent)) + # color applies to both points and eclipses geom_point() + stat_ellipse() + scale_x_log10() ## Too few points to calculate an ellipse ## Warning: Removed 1 row(s) containing missing values (geom_path). What if we want to also visualize population size in addition to grouping by continent? ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color=continent, size=pop)) + scale_x_log10() Here we run into a minor problem: some dots are overlapping. We can fix this by applying geom_jitter() with partial transparency. ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp)) + geom_jitter(alpha=0.7, aes(color=continent, size=pop)) + scale_x_log10() Notice that geom_jitter() adds some random variation, or jitter, to each point. While this is a handy method to address overplotting, don‚Äôt rely on it too heavily. This is illustrated in the next plot: ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_jitter(aes(color=continent, size=pop, alpha=year)) + scale_x_log10() As you can see, this is a fancy figure, but also a messy figure. This plot has ‚Äúinformation overload,‚Äù so we would like to simplify it. To reduce the amount of information, we come back to facets. Be careful with this one as R interprets year as a continuous variable. Use as.factor() or factor() to circumvent this issue, ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_jitter(alpha=0.65, aes(size=pop, color=as.factor(year))) + facet_wrap(vars(continent)) + scale_x_log10() Finally, we can change the labels and text formats. Here, we can save the figure to a .png format. Alternatively, you can save the figure using the Export tab in the Plots viewing panel in RStudio. p &lt;- ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_jitter(alpha=0.65, aes(size=pop, color=year)) + facet_wrap(vars(continent)) + scale_x_log10() + labs(x = &quot;GDP per capita&quot;, y = &quot;Life Expectancy&quot;, size = &quot;Population&quot;, color = &quot;Year&quot;) + theme_bw() + # remove the gray background theme(text = element_text(size = 16)) # make texts larger show(p) ggsave(&quot;data/09_ggplot2/Life_GDP.png&quot;, plot = p) ## Saving 7 x 5 in image "],["getting-publication-ready.html", "10 Getting publication-ready 10.1 Line plot 10.2 Bar plot 10.3 Box plot 10.4 Histogram and density plot 10.5 Assembly of multiple figures (OPTIONAL) 10.6 Extras (OPTIONAL) 10.7 Additional resources", " 10 Getting publication-ready As the chapter name suggests, this chapter is all about getting publication-quality plots. library(tidyverse) library(gapminder) 10.1 Line plot A line plot is another way to visualize continuous variables. This is particularly useful when Your observations change over time and You want to demonstrate a causal relationship. Let‚Äôs elaborate on the second point. In the previous case for life expectancy and GDP, we could only observe a correlation, but cannot conclude a causal relationship. In some case, such as carrying out a laboratory experiment or a simulation study, you can precisely manipulate certain independent variables and measure other dependent variables. This way you can argue for a better causal relationship. For example, you can change the concentration of a drug treatment and measure the inhibition effect. Let‚Äôs take a look at the life expectancy of Africa over the years. Let‚Äôs first create a scatter plot. ggplot(data = subset(gapminder, continent == &quot;Europe&quot;), aes(x = year, y = lifeExp)) + geom_point() Since the points can correspond to the countries, we can connect them with lines. ggplot(data = subset(gapminder, continent == &quot;Europe&quot;), aes(x = year, y = lifeExp, colour = country)) + geom_point() + geom_line(aes(group = country)) Here the group = country specifies that points with the same values for the variable country should be connected in a line. Oftentimes you want to show line plots with mean values and error bars. Unfortunately, ggplot2 can‚Äôt automatically draw error bars ‚Äì you have to explicitly specify the values. We‚Äôre going to address this in the next example. First, calculate the mean and SEM and save it to a new data frame. df &lt;- gapminder %&gt;% group_by(year, continent) %&gt;% summarise(mean_le = mean(lifeExp), sd=sd(lifeExp), sem = sd(lifeExp)/sqrt(n())) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) Next, draw a line plot with this data frame. To draw error bars, we need to specify the upper and lower limits within geom_errorbar(). lineplot &lt;- ggplot(data = df, aes(x = year, y = mean_le, color = continent)) + geom_line() + geom_point() + geom_errorbar(aes(ymin = mean_le-sem, ymax = mean_le+sem), position = position_dodge(0.05)) # position_dodge() sets length of error bar caps show(lineplot) Finally, let‚Äôs format the figure nicely. lineplot &lt;- lineplot + labs(x = &quot;Year&quot;, y = &quot;Life Expectancy&quot;, color = &quot;Continent&quot;) + theme_classic() + # remove the gray background theme(text = element_text(size = 16)) # set font size show(lineplot) For a cleaner view with offset axis: lineplot &lt;- lineplot + theme(axis.line = element_blank(), # hide the default axes axis.ticks.length = unit(7, &#39;pt&#39;)) + # specify the breaks of y-axis scale_y_continuous(breaks=seq(40,80,20), limits=c(35,85), expand=c(0,0)) + # specify the breaks of x-axis scale_x_continuous(breaks=seq(1950, 2010, 20), limits=c(1945,2010), expand=c(0,0)) + # specify the location of the new y-axis geom_segment(y=40, yend=80, x=1945, xend=1945, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) + # specify the location of the new x-axis geom_segment(y=35, yend=35, x=1950, xend=2010, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) show(lineplot) ggsave(&quot;data/10_pubvis/life_year.png&quot;, plot = lineplot) ## Saving 7 x 5 in image 10.2 Bar plot 10.2.1 Basics Let‚Äôs look at the life expectancy of all continents in 1977 using a bar plot. Notice that instead of using dplyr, we can simply use the base R subset() function. Recall df from the previous section. ggplot(data = subset(df, year == 1977), aes(x=continent, y=mean_le)) + geom_col() Note by default, geom_col takes both x and y while geom_bar takes only x and plots the count on y. Just as before, we can add error bars: ggplot(data = subset(df, year == 1977), aes(x=continent, y=mean_le)) + geom_col() + geom_errorbar(aes(ymin = mean_le - sem, ymax = mean_le + sem), width=0.5, position=position_dodge(0.05)) We can compare 1977 and 2007 by settingfill = as.factor(year). p &lt;- ggplot(data = subset(df, year %in% c(1977, 2007)), aes(x=continent, y=mean_le, fill = as.factor(year))) + geom_col(position = position_dodge(), color = &quot;black&quot;) + geom_errorbar(aes(ymin = mean_le - sem, ymax = mean_le + sem), width=0.5, position=position_dodge(0.9)) show(p) Finally, let‚Äôs make this figure publication-ready: p &lt;- p + scale_fill_manual(values = c(&#39;black&#39;, &#39;white&#39;)) + labs(x = &#39;Continent&#39;, y=&#39;Mean life expectancy (years)&#39;, fill = &#39;Year&#39;) + theme_classic() + theme(text = element_text(size=16)) + # removes space between bottom of bars and x-axis scale_y_continuous(expand = c(0, 0)) p 10.2.2 Plotting significance (OPTIONAL) Next we can run some statistical tests and add significance stars (*) to the plot. We‚Äôll compare if the life expectancy in Africa in 1977 and 2007 has changed. africa.1977.lifeExp &lt;- gapminder %&gt;% filter(continent == &#39;Africa&#39;, year == 1977) %&gt;% select(lifeExp) africa.2007.lifeExp &lt;- gapminder %&gt;% filter(continent == &#39;Africa&#39;, year == 2007) %&gt;% select(lifeExp) t.test(africa.1977.lifeExp, africa.2007.lifeExp) ## ## Welch Two Sample t-test ## ## data: africa.1977.lifeExp and africa.2007.lifeExp ## t = -3.195, df = 91.787, p-value = 0.001917 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -8.474086 -1.977145 ## sample estimates: ## mean of x mean of y ## 49.58042 54.80604 The p-value is 0.001917. Here are common ranges for different p-values: 0 **** 0.0001 *** 0.001 ** 0.01 * 0.05 ns 1. We should use ** in this case. To draw the significance stars, we need the ggsignif package. # install.packages(&quot;ggsignif&quot;) library(ggsignif) meanLE.77.07 &lt;- p + geom_signif(y_position = 60, xmin = 0.75, xmax = 1.25, # position of the stars annotations = &quot;**&quot;, tip_length = 0.05) meanLE.77.07 There are more ways to draw the significant stars in R. For example, ggpubr even allows you to run the tests and plot the stars in the same line. However, I do not encourage you to do so. Manually adding the stars might be a bit tedious (in terms of adjusting the positions and tip length), but you are not as restricted by the package in terms of the tests you can do. 10.3 Box plot Box plots are extremely versatile. Here are 2 reasons: You don‚Äôt need to calculate the mean and error for box plots (remember we used df for bar plots). Many high-profile journals ask authors to submit graphs that shows not only the statistical description (mean and error), but also the dots for the raw data. Box plots are well suited for this purpose. p &lt;- ggplot(data = subset(gapminder, year %in% c(1977, 2007)), aes(x = continent, y = lifeExp, color = as.factor(year))) + geom_boxplot(position = position_dodge(0.8)) show(p) Let‚Äôs add the data points. Here I use geom_jitter to avoid overlapping. Please note that position_jitterdodge introduces random noise to the x position of the points to make it easier to read. But since we are plotting against a categorical variable, the exact x position doesn‚Äôt matter. p &lt;- p + geom_point(position = position_jitterdodge(0.4), alpha = 0.65) Use stat_summary show data points and statistics together. p &lt;- ggplot(data = subset(gapminder, year %in% c(1977, 2007)), aes(x = continent, y = lifeExp, color = as.factor(year))) + geom_point(position = position_jitterdodge(0.4), alpha = 0.65) + theme_classic() + # adds error bar stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom=&quot;errorbar&quot;, width=0.3, position = position_dodge(0.7), size = 1) + # adds mean pooint to the error bar stat_summary(fun=mean, geom=&quot;point&quot;, position = position_dodge(0.7), size = 3) show(p) Add asterisks and format the plot. p &lt;- p + geom_signif(y_position = 85, xmin = 0.75, xmax = 1.25, annotations = &quot;**&quot;, tip_length = 0.02, color = &quot;black&quot;) + labs(x = &quot;Continents&quot;, y = &quot;Life Expectancy (years)&quot;, color = &quot;Year&quot;) + theme(text = element_text(size = 16)) # make text larger show(p) We can again offset the axes. offset_box &lt;- p + theme(axis.line = element_blank(), # hide the default axes axis.ticks.length = unit(7, &#39;pt&#39;)) + # Specify the breaks of y-axis scale_y_continuous(breaks=seq(30,90,30), limits=c(25,95), expand=c(0,0)) + # Specify location of x-axis geom_segment(y=30, yend=90, x=0.4, xend=0.4, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) + # Specify location of y-axis geom_segment(y=25, yend=25, x=1, xend=5, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) show(offset_box) Now we can save this publication-ready figure. ggsave(&#39;data/10_pubvis/life_year_jitter.png&#39;, plot = offset_box) ## Saving 7 x 5 in image 10.4 Histogram and density plot Since these have been covered extensively in previous labs, I‚Äôm going to go through this section rather quickly. The histogram and density plot are great tools for looking at distributions of a single variable: ggplot(data = subset(gapminder, year == 2007), aes(x = lifeExp)) + geom_histogram(fill = &#39;#69B3A2&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(data = subset(gapminder, year == 2007), aes(x = lifeExp)) + geom_density(fill = &#39;#69B3A2&#39;) We can also use these plots to compare distributions. ggplot() + # 2007 data and label geom_density(data = subset(gapminder, year == 2007), aes(x = lifeExp, y = ..density..), fill = &#39;#69B3A2&#39;) + geom_label(aes(x=40, y=0.03, label=&#39;2007&#39;), colour = &#39;#69B3A2&#39;) + # 1977 data and label geom_density(data = subset(gapminder, year == 1977), aes(x = lifeExp, y = -..density..), fill = &#39;#404080&#39;) + geom_label(aes(x=40, y=-0.03, label=&#39;1977&#39;), colour = &#39;#404080&#39;) + theme_minimal() Perhaps a histogram would be better for our purposes. hist.07.77 &lt;- ggplot() + # 2007 data and label geom_histogram(data = subset(gapminder, year == 2007), aes(x = lifeExp, y = ..density..), fill = &#39;#69B3A2&#39;) + geom_label(aes(x=40, y=0.03, label=&#39;2007&#39;), colour = &#39;#69B3A2&#39;) + # 1977 data and label geom_histogram(data = subset(gapminder, year == 1977), aes(x = lifeExp, y = -..density..), fill = &#39;#404080&#39;) + geom_label(aes(x=40, y=-0.03, label=&#39;1977&#39;), colour = &#39;#404080&#39;) + theme_minimal() hist.07.77 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 10.5 Assembly of multiple figures (OPTIONAL) Many figures in academic journals include multiple subfigures within a figure. To assemble many graphs into one figure, usually we use a graphical design software such as Illustrator, Inkscape or even PowerPoint. If you are able to generate all subfigures within one single R script (so that all the ggplot2 objects are present together), you could go on to use R package patchwork to assemble them into a big figure. (Please note that this could be a very rare scenario when conducting serious research - Each subfigures may require intense computation and modelling work that are performed with several scripts. They may even come from different people - your teammates and collaborators. You don‚Äôt always have access to all te subfigures within one workspace. Most of the time you would still find yourself using graphical design softwares to assemble the figures.) # install.packages(&#39;patchwork&#39;) library(patchwork) ## Warning: package &#39;patchwork&#39; was built under R version 3.6.2 Here is a super simple example: just add the plots together! Recall that these variables were saved throughout our lab. hist.07.77 + lineplot ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can also do something a bit more complicated: (hist.07.77 + lineplot) / offset_box ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can also add spacing between plots: patch &lt;- (hist.07.77 + plot_spacer() + lineplot) / (meanLE.77.07 + theme(text=element_text(size=12))) patch ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Most figures in papers are annotated. In most cases, plots are combined using a photoshop tool. Labels are usually also added with a photoshop tool. For whatever reason you wish to programmatically add labels, here‚Äôs howyou do it: (hist.07.77 + lineplot) / (meanLE.77.07 + theme(text=element_text(size=12))) + plot_annotation(tag_levels = &#39;A&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Note that tag_levels takes: ‚Äò1‚Äô for Arabic numerals, ‚ÄòA‚Äô for uppercase Latin letters, ‚Äòa‚Äô for lowercase Latin letters, ‚ÄòI‚Äô for uppercase Roman numerals, and ‚Äòi‚Äô for lowercase Roman numerals. More often than not, we want all of our figures to have the same dimensions. patchwork makes it easy for us to do so: aligned_plots &lt;- align_patches(lineplot, scatter_trend, meanLE.77.07, offset_box) for (p in aligned_plots) { plot(p) } That‚Äôs all there is to it! If you want more customization options, read the official documentation: https://patchwork.data-imaginist.com/articles/patchwork.html. 10.6 Extras (OPTIONAL) 10.6.1 Choropleth ‚ÄúThe greatest value of a picture is when it forces us to notice what we never expected to see.‚Äù ‚ÄìJohn Tukey The choropleth is used to display differences in geographical regions using different colours/shades/patterns. To use map data, we need to install maps package. install.packages(&quot;maps&quot;) First, let‚Äôs load maps and retrieve all of the data from the year 2007. library(maps) dat2007 &lt;- gapminder %&gt;% filter(year == 2007) dat2007 &lt;- dat2007 %&gt;% rename(&#39;region&#39; = &#39;country&#39;) Now let‚Äôs get the world map data. This is necessary because it contains the longitude and latitudes we need to draw the map world_map &lt;- map_data(&quot;world&quot;) head(world_map) ## long lat group order region subregion ## 1 -69.89912 12.45200 1 1 Aruba &lt;NA&gt; ## 2 -69.89571 12.42300 1 2 Aruba &lt;NA&gt; ## 3 -69.94219 12.43853 1 3 Aruba &lt;NA&gt; ## 4 -70.00415 12.50049 1 4 Aruba &lt;NA&gt; ## 5 -70.06612 12.54697 1 5 Aruba &lt;NA&gt; ## 6 -70.05088 12.59707 1 6 Aruba &lt;NA&gt; Whereas gapminder names USA and UK ‚ÄòUnited States‚Äô and ‚ÄòUnited Kingdom‚Äô respectively, world_map names them by their abbreviations. Let‚Äôs rename the gapminder data. dat2007 &lt;- dat2007 %&gt;% mutate(region = fct_recode(region, &#39;USA&#39; = &#39;United States&#39;, &#39;UK&#39; = &#39;United Kingdom&#39;)) We‚Äôre almost there! Now, we need to merge the data. life.exp.map &lt;- left_join(world_map, dat2007, by = &quot;region&quot;) head(life.exp.map) ## long lat group order region subregion continent year lifeExp pop ## 1 -69.89912 12.45200 1 1 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 2 -69.89571 12.42300 1 2 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 3 -69.94219 12.43853 1 3 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 4 -70.00415 12.50049 1 4 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 5 -70.06612 12.54697 1 5 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 6 -70.05088 12.59707 1 6 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## gdpPercap ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 NA Finally, we can plot the data. We specify group to draw each country individually. # grey means no gapminder data ggplot(life.exp.map, aes(x=long, y=lat, group = group)) + geom_polygon(aes(fill = lifeExp), color = &quot;white&quot;) + scale_fill_viridis_b(option=&#39;D&#39;) + theme_bw() 10.6.2 Animations While we covered the most relevant data visualizations for your project, we‚Äôve barely scratched the surface of what R can do. For example, you can create animations. Let‚Äôs install the gganimate package. install.packages(&#39;devtools&#39;) devtools::install_github(&#39;thomasp85/gganimate&#39;) Next, load the package: library(gganimate) ## Warning: package &#39;gganimate&#39; was built under R version 3.6.2 Finally, we can create the plot! This might take a while, but I promise it will be worth the wait! ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, col = country)) + geom_point(alpha = 0.7, show.legend = F) + scale_colour_manual(values = country_colors) + scale_x_log10() + scale_size(range = c(2, 12)) + facet_wrap(~continent) + theme_bw() + theme(panel.grid = element_blank()) + # here is the animation code labs(title = &#39;Year: {frame_time}&#39;, x = &#39;GDP per capita&#39;, y = &#39;Life expectancy (years)&#39;) + transition_time(year) + ease_aes(&#39;linear&#39;) 10.7 Additional resources I highly recommend you read through these websites: From Data to Viz: https://www.data-to-viz.com Includes the visualization and the type of data it corresponds to. Patchwork: https://github.com/thomasp85/patchwork For creating multi-plot figures. Caveats: https://www.data-to-viz.com/caveats.html Pitfalls to avooid when creating figures. The Python Graph Gallery: https://python-graph-gallery.com If you‚Äôre more comfortable with Python. Includes the visualization and code to creat it. Data visualizations for various data types. Check out https://www.data-to-viz.com/ for an interactive version of this chart! "]]
